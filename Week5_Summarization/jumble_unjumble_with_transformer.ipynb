{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_files():\n",
    "    if os.path.exists(\"DataSets/Jumble/jumbled.txt\"): os.remove(\"DataSets/Jumble/jumbled.txt\")\n",
    "    if os.path.exists(\"DataSets/Jumble/unjumbled.txt\"): os.remove(\"DataSets/Jumble/unjumbled.txt\")   \n",
    "    f1 = open(\"DataSets/Jumble/jumbled.txt\",\"w\")\n",
    "    f2 = open(\"DataSets/Jumble/unjumbled.txt\",\"w\")\n",
    "    with open(\"DataSets/Jumble/source.txt\",\"r\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.lower().strip().split()\n",
    "            sentence = split_line[1:]\n",
    "            f2.write(' '.join(sentence)+\"\\n\")\n",
    "            random.shuffle(sentence)\n",
    "            f1.write(\" \".join(sentence)+\"\\n\")\n",
    "    f1.close()\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_of_words():\n",
    "    word_count_dict = {}\n",
    "    with open(\"DataSets/Jumble/jumbled.txt\",\"r\") as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                if word in word_count_dict: word_count_dict[word] += 1\n",
    "                else: word_count_dict[word] = 1\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_dict = get_count_of_words()\n",
    "print(len(word_count_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_count = 4\n",
    "count = 0\n",
    "for k,v in word_count_dict.items():\n",
    "    if v > min_word_count: count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_less_frequent_words(filename,word_count_dict,min_word_count,replace_token=\"<unk>\"):\n",
    "    with open(filename,\"r\") as f:\n",
    "        with open(\"DataSets/Jumble/processed_\"+filename.split(\"/\")[-1],\"w\") as f1:\n",
    "            for line in f:\n",
    "                words = line.strip().split()\n",
    "                sentence_to_write = []\n",
    "                for word in words:\n",
    "                    if word_count_dict[word] > min_word_count: sentence_to_write.append(word)\n",
    "                    else: sentence_to_write.append(replace_token)\n",
    "                sentence_to_write.append(\"\\n\")\n",
    "                f1.write(\" \".join(sentence_to_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_less_frequent_words(\n",
    "    \"DataSets/Jumble/jumbled.txt\",word_count_dict,min_word_count=min_word_count,replace_token=\"<unk>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_less_frequent_words(\n",
    "    \"DataSets/Jumble/unjumbled.txt\",word_count_dict,min_word_count=min_word_count,replace_token=\"<unk>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabBuilder:\n",
    "    def __init__(self,text_corpus,unknown_token=None,pad_token=None,sos_token=None,eos_token=None):\n",
    "        '''\n",
    "        text_corpus = [\n",
    "            sentence_1,  # sentence_1 = \"a yellow car ...\"\n",
    "            sentence_2\n",
    "            ...\n",
    "        ]\n",
    "        '''\n",
    "        self.text_corpus = text_corpus\n",
    "        self.unknown_token = unknown_token or \"<unk>\"\n",
    "        self.pad_token = pad_token or \"<pad>\"\n",
    "        self.sos_token = sos_token or \"<sos>\"\n",
    "        self.eos_token = eos_token or \"<eos>\"\n",
    "        self.word_to_index, self.index_to_word = self.get_vocabs()\n",
    "                        \n",
    "    def get_vocabs(self):\n",
    "        word_to_index = {}\n",
    "        index_count = 0\n",
    "        for sentence in self.text_corpus:\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                if word not in word_to_index:\n",
    "                    word_to_index[word] = index_count\n",
    "                    index_count += 1\n",
    "        if not self.unknown_token in word_to_index: \n",
    "            word_to_index[self.unknown_token] = index_count\n",
    "            index_count += 1\n",
    "        if not self.pad_token in word_to_index: \n",
    "            word_to_index[self.pad_token] = index_count\n",
    "            index_count += 1\n",
    "        if not self.sos_token in word_to_index: \n",
    "            word_to_index[self.sos_token] = index_count\n",
    "            index_count += 1\n",
    "        if not self.eos_token in word_to_index: \n",
    "            word_to_index[self.eos_token] = index_count\n",
    "            index_count += 1\n",
    "        index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "        return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeDecode:\n",
    "    def __init__(self,word_to_index,index_to_word,pad_token,unknown_token,smallcase=True):\n",
    "        self.smallcase = smallcase\n",
    "        self.word_to_index = word_to_index\n",
    "        self.index_to_word = index_to_word\n",
    "        self.pad_token = pad_token\n",
    "        self.unknown_token = unknown_token\n",
    "    \n",
    "    def get_encoding(self,sentence):\n",
    "        '''\n",
    "        sentence can be a string, or a list of words\n",
    "        '''\n",
    "        if isinstance(sentence,str): sentence = sentence.split(\" \")\n",
    "        if self.smallcase: sentence =  [word.lower() for word in sentence]\n",
    "        encoded_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in self.word_to_index: encoded_sentence.append(self.word_to_index[word])\n",
    "            else: encoded_sentence.append(self.word_to_index[self.unknown_token])\n",
    "        return encoded_sentence\n",
    "    \n",
    "    def get_decoding(self,encoded_sentence):\n",
    "        '''\n",
    "        encoded_sentence must be a list of vocab indices.\n",
    "        Ex: encoded_sentence = [24,21,4,1,..] \n",
    "        '''\n",
    "        sentence = [self.index_to_word[index] for index in encoded_sentence]\n",
    "        return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnjumbleEncoderModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,num_encoder_layers,debug):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.debug: \n",
    "            print(\"_______________________________\")\n",
    "            print(\"\\t\\tEncoder\\t\\t\")\n",
    "            print(\"_______________________________\")\n",
    "        if self.debug: print(\"Before starting: x Shape:\",x.shape)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.relu(x)\n",
    "        if self.debug: print(\"Embedding, x Shape:\",x.shape)\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        if self.debug: print(\"Reshaping for Transformer, x Shape:\",x.shape)\n",
    "            \n",
    "        op = self.transformer_encoder(x)\n",
    "        if self.debug: print(\"Transformer Encoder, op Shape:\",op.shape)\n",
    "            \n",
    "        return op\n",
    "    \n",
    "class UnjumbleDecoderModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,num_decoder_layers,debug):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=4)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self,x,op_from_encoder):\n",
    "        if self.debug: \n",
    "            print(\"_______________________________\")\n",
    "            print(\"\\t\\tDecoder\\t\\t\")\n",
    "            print(\"_______________________________\")\n",
    "        if self.debug: print(\"Before starting: x Shape:\",x.shape,\" op_from_encoder Shape:\",op_from_encoder.shape)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.relu(x)\n",
    "        if self.debug: print(\"Embedding, x Shape:\",x.shape)\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        if self.debug: print(\"Reshaping for Transformer, x Shape:\",x.shape)\n",
    "        \n",
    "        op = self.transformer_decoder(x,op_from_encoder)\n",
    "        if self.debug: print(\"Transformer Decoder, op Shape:\",op.shape)\n",
    "            \n",
    "        # Resizing for Linear Layer\n",
    "        op = op.reshape(-1,op.shape[2])\n",
    "        if self.debug: print(\"Reshaping op Shape:\",op.shape)\n",
    "        \n",
    "        linear_op = self.linear(op)\n",
    "        if self.debug: print(\"Linear linear_op Shape:\",linear_op.shape)\n",
    "        \n",
    "        op = self.log_softmax(linear_op)\n",
    "        if self.debug: print(\"log_softmax op Shape:\",op.shape)\n",
    "            \n",
    "        if self.debug:print(\"_______________________________\\n\\n\")\n",
    "            \n",
    "        return op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40460 40460 40460\n"
     ]
    }
   ],
   "source": [
    "unknown_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "Xe,Xd,Y = [],[],[]\n",
    "with open(\"DataSets/Jumble/processed_jumbled.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        Xe.append(line.strip()+\" \" +eos_token)\n",
    "with open(\"DataSets/Jumble/processed_unjumbled.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        Xd.append(sos_token+\" \"+line.strip())\n",
    "        Y.append(line.strip()+\" \" +eos_token)\n",
    "print(len(Xe),len(Xd),len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36414 4046 36414 4046 36414 4046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3005, 3005, 3005, 3005)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr_e, Xval_e, Xtr_d, Xval_d, Ytr, Yval = train_test_split(Xe,Xd,Y,test_size=0.1,random_state=20)\n",
    "print(len(Xtr_e), len(Xval_e), len(Xtr_d), len(Xval_d), len(Ytr), len(Yval))\n",
    "encoder_vocab_builder = VocabBuilder(Xtr_e,unknown_token=unknown_token,pad_token=pad_token,sos_token=sos_token,eos_token=eos_token)\n",
    "decoder_vocab_builder = VocabBuilder(Xtr_d,unknown_token=unknown_token,pad_token=pad_token,sos_token=sos_token,eos_token=eos_token)\n",
    "encoder_wtoi,encoder_itow = encoder_vocab_builder.word_to_index, encoder_vocab_builder.index_to_word\n",
    "decoder_wtoi,decoder_itow = decoder_vocab_builder.word_to_index, decoder_vocab_builder.index_to_word\n",
    "len(encoder_itow),len(encoder_wtoi),len(decoder_wtoi),len(decoder_itow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "num_encoder_layers = 3\n",
    "model_encoder = UnjumbleEncoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=300,num_encoder_layers=num_encoder_layers,debug=True\n",
    ").to(device)\n",
    "num_decoder_layers = num_encoder_layers\n",
    "model_decoder = UnjumbleDecoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=300,num_decoder_layers=num_decoder_layers,debug=True\n",
    ").to(device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer_encoder = torch.optim.Adam(model_encoder.parameters(),lr=0.003)\n",
    "optimizer_decoder = torch.optim.Adam(model_decoder.parameters(),lr=0.003) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a descends . snowy man mountain a <eos> [13, 47, 8, 48, 49, 50, 13, 9]\n",
      "<sos> a man descends a snowy mountain . [0, 10, 47, 48, 10, 49, 50, 9]\n",
      "a man descends a snowy mountain . <eos> [10, 47, 48, 10, 49, 50, 9, 3004]\n",
      "torch.Size([1, 8]) torch.Size([1, 8]) torch.Size([1, 8])\n",
      "_______________________________\n",
      "\t\tEncoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 8])\n",
      "Embedding, x Shape: torch.Size([1, 8, 300])\n",
      "Reshaping for Transformer, x Shape: torch.Size([8, 1, 300])\n",
      "Transformer Encoder, op Shape: torch.Size([8, 1, 300])\n",
      "_______________________________\n",
      "\t\tDecoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 8])  op_from_encoder Shape: torch.Size([8, 1, 300])\n",
      "Embedding, x Shape: torch.Size([1, 8, 300])\n",
      "Reshaping for Transformer, x Shape: torch.Size([8, 1, 300])\n",
      "Transformer Decoder, op Shape: torch.Size([8, 1, 300])\n",
      "Reshaping op Shape: torch.Size([8, 300])\n",
      "Linear linear_op Shape: torch.Size([8, 3005])\n",
      "log_softmax op Shape: torch.Size([8, 3005])\n",
      "_______________________________\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "torch.Size([1, 9]) torch.Size([1, 9]) torch.Size([1, 9])\n",
      "_______________________________\n",
      "\t\tEncoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 9])\n",
      "Embedding, x Shape: torch.Size([1, 9, 300])\n",
      "Reshaping for Transformer, x Shape: torch.Size([9, 1, 300])\n",
      "Transformer Encoder, op Shape: torch.Size([9, 1, 300])\n",
      "_______________________________\n",
      "\t\tDecoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 9])  op_from_encoder Shape: torch.Size([9, 1, 300])\n",
      "Embedding, x Shape: torch.Size([1, 9, 300])\n",
      "Reshaping for Transformer, x Shape: torch.Size([9, 1, 300])\n",
      "Transformer Decoder, op Shape: torch.Size([9, 1, 300])\n",
      "Reshaping op Shape: torch.Size([9, 300])\n",
      "Linear linear_op Shape: torch.Size([9, 3005])\n",
      "log_softmax op Shape: torch.Size([9, 3005])\n",
      "_______________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_index = 6\n",
    "encoder_encode_decode = EncodeDecode(encoder_wtoi,encoder_itow,pad_token,unknown_token)\n",
    "decoder_encode_decode = EncodeDecode(decoder_wtoi,decoder_itow,pad_token,unknown_token)\n",
    "print(Xtr_e[data_index],encoder_encode_decode.get_encoding(Xtr_e[data_index]))\n",
    "print(Xtr_d[data_index],decoder_encode_decode.get_encoding(Xtr_d[data_index]))\n",
    "print(Ytr[data_index],decoder_encode_decode.get_encoding(Ytr[data_index]))\n",
    "\n",
    "model_encoder.train()\n",
    "model_decoder.train()\n",
    "\n",
    "optimizer_encoder.zero_grad()\n",
    "optimizer_decoder.zero_grad()\n",
    "Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtr_e[data_index])]).to(device)\n",
    "Xd_b = torch.tensor([decoder_encode_decode.get_encoding(Xtr_d[data_index])]).to(device)\n",
    "Y_b = torch.tensor([decoder_encode_decode.get_encoding(Ytr[data_index])]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "op_from_encoder = model_encoder(Xe_b)\n",
    "op = model_decoder(Xd_b,op_from_encoder)\n",
    "\n",
    "loss = loss_fn(op,Y_b.reshape(-1))\n",
    "loss.backward()\n",
    "optimizer_encoder.step()\n",
    "optimizer_decoder.step()\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "optimizer_encoder.zero_grad()\n",
    "optimizer_decoder.zero_grad()\n",
    "Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtr_e[data_index+1])]).to(device)\n",
    "Xd_b = torch.tensor([decoder_encode_decode.get_encoding(Xtr_d[data_index+1])]).to(device)\n",
    "Y_b = torch.tensor([decoder_encode_decode.get_encoding(Ytr[data_index+1])]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "op_from_encoder = model_encoder(Xe_b)\n",
    "op = model_decoder(Xd_b,op_from_encoder)\n",
    "loss = loss_fn(op,Y_b.reshape(-1))\n",
    "loss.backward()\n",
    "optimizer_encoder.step()\n",
    "optimizer_decoder.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi):\n",
    "    data_index = random.randint(0,100)\n",
    "    Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xval_e[data_index])]).to(device)\n",
    "    print(Xval_e[data_index],Xe_b)\n",
    "    print(Xval_d[data_index])\n",
    "    \n",
    "    model_encoder.eval()\n",
    "    model_decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        softmax_ops = []\n",
    "        op_from_encoder = model_encoder(Xe_b)\n",
    "        sos_word = torch.tensor([[decoder_wtoi[\"<sos>\"]]]).to(device)\n",
    "        op = model_decoder(sos_word,op_from_encoder)\n",
    "        unjumbled_sentence = []\n",
    "        for i in range(25):\n",
    "            predicted_word = torch.argmax(op,axis=1).tolist()\n",
    "#             print(\"Predicted .....................\",predicted_word)\n",
    "            unjumbled_sentence.append(decoder_itow[predicted_word[0]])\n",
    "            if predicted_word[0] == decoder_wtoi[\"<eos>\"]: break\n",
    "            op = model_decoder(torch.tensor([predicted_word]).to(device),op_from_encoder)\n",
    "        print(\"_______________________________________\")\n",
    "        print(unjumbled_sentence)\n",
    "        print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people . sit benches on <eos> tensor([[203,   8, 401, 512,  53,   9]], device='cuda:0')\n",
      "<sos> people sit on benches .\n",
      "_______________________________________\n",
      "['<eos>']\n",
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "predict(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "num_encoder_layers = 3\n",
    "model_encoder = UnjumbleEncoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=400,num_encoder_layers=num_encoder_layers,debug=False\n",
    ").to(device)\n",
    "num_decoder_layers = num_encoder_layers\n",
    "model_decoder = UnjumbleDecoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=400,num_decoder_layers=num_decoder_layers,debug=False\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer_encoder = torch.optim.Adam(model_encoder.parameters(),lr=0.0003)\n",
    "optimizer_decoder = torch.optim.Adam(model_decoder.parameters(),lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 Loss: 8.253918647766113\n",
      "Epoch: 0 Batch: 2000 Loss: 4.433011531829834\n",
      "Epoch: 0 Batch: 4000 Loss: 1.855195164680481\n",
      "Epoch: 0 Batch: 6000 Loss: 4.546597003936768\n",
      "Epoch: 0 Batch: 8000 Loss: 4.006786346435547\n",
      "Epoch: 0 Batch: 10000 Loss: 5.370200157165527\n",
      "Epoch: 0 Batch: 12000 Loss: 3.9275732040405273\n",
      "Epoch: 0 Batch: 14000 Loss: 3.794468879699707\n",
      "Epoch: 0 Batch: 16000 Loss: 3.8105435371398926\n",
      "Epoch: 0 Batch: 18000 Loss: 3.3745758533477783\n",
      "Epoch: 0 Batch: 20000 Loss: 4.577354431152344\n",
      "Epoch: 0 Batch: 22000 Loss: 4.788670539855957\n",
      "Epoch: 0 Batch: 24000 Loss: 3.61185884475708\n",
      "Epoch: 0 Batch: 26000 Loss: 3.238779067993164\n",
      "Epoch: 0 Batch: 28000 Loss: 4.005866050720215\n",
      "Epoch: 0 Batch: 30000 Loss: 2.8864517211914062\n",
      "Epoch: 0 Batch: 32000 Loss: 5.089885711669922\n",
      "Epoch: 0 Batch: 34000 Loss: 4.502218246459961\n",
      "Epoch: 0 Batch: 36000 Loss: 3.627053737640381\n",
      "______________________________________\n",
      "Epoch Loss: 146783.75069224834\n",
      "handle older car in bag . yellow bicycle with a <unk> woman the on riding is shopping the wearing , a background <eos> tensor([[2291,  675,  128,   15,  667,    8,   86,  149,   24,   13,    0,   30,\n",
      "            1,   53,  270,   32,  360,    1,  108,   96,   13,  144,    9]],\n",
      "       device='cuda:0')\n",
      "<sos> older woman wearing <unk> riding a bicycle with a shopping bag on the handle , yellow car is in the background .\n",
      "_______________________________________\n",
      "['<eos>']\n",
      "_______________________________________\n",
      "_______________________________________\n",
      "Epoch: 1 Batch: 0 Loss: 3.971458911895752\n",
      "Epoch: 1 Batch: 2000 Loss: 4.561129093170166\n",
      "Epoch: 1 Batch: 4000 Loss: 2.6241462230682373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-fbe126714f31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m2000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Batch:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Loss:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    model_encoder.train()\n",
    "    model_decoder.train()\n",
    "    epoch_loss = 0\n",
    "    for j in range(len(Xtr_e)):\n",
    "        optimizer_encoder.zero_grad()\n",
    "        optimizer_decoder.zero_grad()\n",
    "        Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtr_e[j])]).to(device)\n",
    "        Xd_b = torch.tensor([decoder_encode_decode.get_encoding(Xtr_d[j])]).to(device)\n",
    "        Y_b = torch.tensor([decoder_encode_decode.get_encoding(Ytr[j])]).to(device)\n",
    "        op_from_encoder = model_encoder(Xe_b)\n",
    "        op = model_decoder(Xd_b,op_from_encoder)\n",
    "        loss = loss_fn(op,Y_b.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer_encoder.step()\n",
    "        optimizer_decoder.step()\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        if j%2000 == 0: print(\"Epoch:\",i,\"Batch:\",j,\"Loss:\",batch_loss)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Epoch Loss:\",epoch_loss)\n",
    "    predict(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "    print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_whole_val(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi):\n",
    "    model_encoder.eval()\n",
    "    model_decoder.eval()\n",
    "    accuracy_tuple_list = []  #[(jumbed_sent,unjumbled_sent,predicted_sent,hard,soft,word_count),...,]\n",
    "    with torch.no_grad():\n",
    "        for data_index in range(len(Xval_e)):\n",
    "            if data_index % 50 == 0: print(data_index,end = ' ')\n",
    "            Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xval_e[data_index])]).to(device)\n",
    "\n",
    "            init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "            op_from_encoder,ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "            sos_word = torch.tensor([[decoder_wtoi[\"<sos>\"]]]).to(device)\n",
    "            op,ht,softmax_op = model_decoder(sos_word,ht_for_decoder,op_from_encoder)\n",
    "            unjumbled_sentence = []\n",
    "            for i in range(25):\n",
    "                predicted_word = torch.argmax(op,axis=1).tolist()\n",
    "                unjumbled_sentence.append(decoder_itow[predicted_word[0]])\n",
    "                if predicted_word[0] == decoder_wtoi[\"<eos>\"]: break\n",
    "                op,ht,softmax_op = model_decoder(torch.tensor([predicted_word]).to(device),ht,op_from_encoder)\n",
    "                \n",
    "            hard_accuracy = 1 if \" \".join(unjumbled_sentence) == Yval[data_index] else 0\n",
    "            word_count = len(set(Yval[data_index].split()))\n",
    "            soft_accuracy = len(set(unjumbled_sentence).intersection(set(Yval[data_index].split())))/word_count\n",
    "            accuracy_tuple_list.append(\n",
    "                (Xval_e[data_index],Yval[data_index],\" \".join(unjumbled_sentence),hard_accuracy,soft_accuracy,word_count)\n",
    "            )\n",
    "    return accuracy_tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tuple_list = predict_on_whole_val(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "df = pd.DataFrame(accuracy_tuple_list)\n",
    "df.columns = [\"jumbled_sent\",\"unjumbled_sent\",\"prediction\",\"hard_accuracy\",\"soft_accuracy\",\"word_count\"]\n",
    "print(df.shape,df['hard_accuracy'].sum())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum((df['soft_accuracy']*df['word_count']).tolist())\n",
    "b = df['word_count'].sum()\n",
    "a,b,a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['soft_accuracy']==1].shape, df[df['soft_accuracy']==1].shape[0]/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(model_encoder_new,model_decoder_new,encoder_encode_decode,decoder_itow,decoder_wtoi):\n",
    "    model_encoder_new.eval()\n",
    "    model_decoder_new.eval()\n",
    "    accuracy_tuple_list = []  #[(jumbed_sent,unjumbled_sent,predicted_sent,hard,soft,word_count),...,]\n",
    "    with torch.no_grad():\n",
    "        for data_index in range(len(Xtest_e)):\n",
    "            if data_index % 50 == 0: print(data_index,end = ' ')\n",
    "            Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtest_e[data_index])]).to(device)\n",
    "            init_ht_for_encoder = model_encoder_new.init_hidden().to(device)\n",
    "            op_from_encoder,ht,ht_for_decoder = model_encoder_new(Xe_b,init_ht_for_encoder)\n",
    "            sos_word = torch.tensor([[decoder_wtoi[\"<sos>\"]]]).to(device)\n",
    "            op,ht,softmax_op = model_decoder_new(sos_word,ht_for_decoder,op_from_encoder)\n",
    "            unjumbled_sentence = []\n",
    "            for i in range(25):\n",
    "                predicted_word = torch.argmax(op,axis=1).tolist()\n",
    "                unjumbled_sentence.append(decoder_itow[predicted_word[0]])\n",
    "                if predicted_word[0] == decoder_wtoi[\"<eos>\"]: break\n",
    "                op,ht,softmax_op = model_decoder_new(torch.tensor([predicted_word]).to(device),ht,op_from_encoder)\n",
    "            hard_accuracy = 1 if \" \".join(unjumbled_sentence) == Ytest[data_index] else 0\n",
    "            word_count = len(set(Ytest[data_index].split()))\n",
    "            soft_accuracy = len(set(unjumbled_sentence).intersection(set(Ytest[data_index].split())))/word_count\n",
    "            accuracy_tuple_list.append(\n",
    "                (Xtest_e[data_index],Ytest[data_index],\" \".join(unjumbled_sentence),hard_accuracy,soft_accuracy,word_count)\n",
    "            )\n",
    "    return accuracy_tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_e = [\n",
    "    \"is eating . Nitish apple <eos>\",\n",
    "    \"is city my favorite New York. <eos>\",\n",
    "    \"a a and dog are man woods walking through the . <eos>\"\n",
    "]\n",
    "Ytest = [\n",
    "    \"Nitish is eating apple . <eos>\",\n",
    "    \"New York is my faorite city. <eos>\",\n",
    "    \"a man and a dog are walking through the woods . <eos>\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tuple_list_test = predict_on_test(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "df_test = pd.DataFrame(accuracy_tuple_list_test)\n",
    "df_test.columns = [\"jumbled_sent\",\"unjumbled_sent\",\"prediction\",\"hard_accuracy\",\"soft_accuracy\",\"word_count\"]\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Model was not required to be saved explicitly as it is a part of the decoder model only.\n",
    "#### Additionally the word_to_index and index_to_word dictionary and get_encoding funtion will be required for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_encoder.state_dict(), \"SavedModels/Jumble/encoder_model.pt\")\n",
    "torch.save(model_attention.state_dict(), \"SavedModels/Jumble/attention_model.pt\")\n",
    "torch.save(model_decoder.state_dict(), \"SavedModels/Jumble/decoder_model.pt\")\n",
    "\n",
    "# torch.save(model_encoder,\"SavedModels/Jumble/encoder_model.pt\")\n",
    "# torch.save(model_attention,\"SavedModels/Jumble/attention_model.pt\")\n",
    "# torch.save(model_decoder,\"SavedModels/Jumble/decoder_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "hidden_size_encoder = 400\n",
    "encoder_encode_decode = EncodeDecode(encoder_wtoi,encoder_itow,pad_token,unknown_token)\n",
    "decoder_encode_decode = EncodeDecode(decoder_wtoi,decoder_itow,pad_token,unknown_token)\n",
    "loaded_encoder_model = UnjumbleEncoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=300,num_lstm_layers=2,\n",
    "    hidden_size=hidden_size_encoder,make_bidirectional=True,debug=False\n",
    ").to(device)\n",
    "loaded_encoder_model.load_state_dict(torch.load(\"SavedModels/Jumble/encoder_model.pt\"))\n",
    "loaded_encoder_model.eval()\n",
    "\n",
    "if loaded_encoder_model.bidirectional: hidden_size_decoder = 2*hidden_size_encoder\n",
    "loaded_attention_model = UnjumbleBahadnauAttention(hidden_size_decoder,debug=False).to(device)\n",
    "loaded_attention_model.load_state_dict(torch.load(\"SavedModels/Jumble/attention_model.pt\"))\n",
    "loaded_attention_model.eval()\n",
    "\n",
    "loaded_decoder_model = UnjumbleDecoderModel(model_attention = loaded_attention_model,\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=300,num_lstm_layers=1,\n",
    "    hidden_size=hidden_size_decoder,make_bidirectional=False,debug=False\n",
    ").to(device)\n",
    "loaded_decoder_model.load_state_dict(torch.load(\"SavedModels/Jumble/decoder_model.pt\"))\n",
    "loaded_decoder_model.eval()\n",
    "\n",
    "\n",
    "# loaded_encoder_model = torch.load(\"SavedModels/Jumble/encoder_model.pt\")\n",
    "# loaded_decoder_model = torch.load(\"SavedModels/Jumble/decoder_model.pt\")\n",
    "# model_attention = torch.load(\"SavedModels/Jumble/attention_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tuple_list_test = predict_on_test(loaded_encoder_model,loaded_decoder_model,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "df_test = pd.DataFrame(accuracy_tuple_list_test)\n",
    "df_test.columns = [\"jumbled_sent\",\"unjumbled_sent\",\"prediction\",\"hard_accuracy\",\"soft_accuracy\",\"word_count\"]\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitf0b0a3d2859f4904a6dd3c0263fd37ec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
