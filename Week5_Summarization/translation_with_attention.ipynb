{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_files():\n",
    "    if os.path.exists(\"DataSets/Jumble/jumbled.txt\"): os.remove(\"DataSets/Jumble/jumbled.txt\")\n",
    "    if os.path.exists(\"DataSets/Jumble/unjumbled.txt\"): os.remove(\"DataSets/Jumble/unjumbled.txt\")   \n",
    "    f1 = open(\"DataSets/Jumble/jumbled.txt\",\"w\")\n",
    "    f2 = open(\"DataSets/Jumble/unjumbled.txt\",\"w\")\n",
    "    with open(\"DataSets/Jumble/source.txt\",\"r\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.lower().strip().split()\n",
    "            sentence = split_line[1:]\n",
    "            f2.write(' '.join(sentence)+\"\\n\")\n",
    "            random.shuffle(sentence)\n",
    "            f1.write(\" \".join(sentence)+\"\\n\")\n",
    "    f1.close()\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_of_words():\n",
    "    word_count_dict = {}\n",
    "    with open(\"DataSets/Jumble/jumbled.txt\",\"r\") as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                if word in word_count_dict: word_count_dict[word] += 1\n",
    "                else: word_count_dict[word] = 1\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8918\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = get_count_of_words()\n",
    "print(len(word_count_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001\n"
     ]
    }
   ],
   "source": [
    "min_word_count = 4\n",
    "count = 0\n",
    "for k,v in word_count_dict.items():\n",
    "    if v > min_word_count: count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_less_frequent_words(filename,word_count_dict,min_word_count,replace_token=\"<unk>\"):\n",
    "    with open(filename,\"r\") as f:\n",
    "        with open(\"DataSets/Jumble/processed_\"+filename.split(\"/\")[-1],\"w\") as f1:\n",
    "            for line in f:\n",
    "                words = line.strip().split()\n",
    "                sentence_to_write = []\n",
    "                for word in words:\n",
    "                    if word_count_dict[word] > min_word_count: sentence_to_write.append(word)\n",
    "                    else: sentence_to_write.append(replace_token)\n",
    "                sentence_to_write.append(\"\\n\")\n",
    "                f1.write(\" \".join(sentence_to_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_less_frequent_words(\n",
    "    \"DataSets/Jumble/jumbled.txt\",word_count_dict,min_word_count=min_word_count,replace_token=\"<unk>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_less_frequent_words(\n",
    "    \"DataSets/Jumble/unjumbled.txt\",word_count_dict,min_word_count=min_word_count,replace_token=\"<unk>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabBuilder:\n",
    "    def __init__(self,text_corpus,unknown_token=None,pad_token=None,sos_token=None,eos_token=None):\n",
    "        '''\n",
    "        text_corpus = [\n",
    "            sentence_1,  # sentence_1 = \"a yellow car ...\"\n",
    "            sentence_2\n",
    "            ...\n",
    "        ]\n",
    "        '''\n",
    "        self.text_corpus = text_corpus\n",
    "        self.unknown_token = unknown_token or \"<unk>\"\n",
    "        self.pad_token = pad_token or \"<pad>\"\n",
    "        self.sos_token = sos_token or \"<sos>\"\n",
    "        self.eos_token = eos_token or \"<eos>\"\n",
    "        self.word_to_index, self.index_to_word = self.get_vocabs()\n",
    "                        \n",
    "    def get_vocabs(self):\n",
    "        word_to_index = {}\n",
    "        index_count = 0\n",
    "        for sentence in self.text_corpus:\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                if word not in word_to_index:\n",
    "                    word_to_index[word] = index_count\n",
    "                    index_count += 1\n",
    "        if not self.unknown_token in word_to_index: \n",
    "            word_to_index[self.unknown_token] = index_count\n",
    "            index_count += 1\n",
    "        if not self.pad_token in word_to_index: \n",
    "            word_to_index[self.pad_token] = index_count\n",
    "            index_count += 1\n",
    "        if not self.sos_token in word_to_index: \n",
    "            word_to_index[self.sos_token] = index_count\n",
    "            index_count += 1\n",
    "        if not self.eos_token in word_to_index: \n",
    "            word_to_index[self.eos_token] = index_count\n",
    "            index_count += 1\n",
    "        index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "        return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeDecode:\n",
    "    def __init__(self,word_to_index,index_to_word,pad_token,unknown_token,smallcase=True):\n",
    "        self.smallcase = smallcase\n",
    "        self.word_to_index = word_to_index\n",
    "        self.index_to_word = index_to_word\n",
    "        self.pad_token = pad_token\n",
    "        self.unknown_token = unknown_token\n",
    "    \n",
    "    def get_encoding(self,sentence):\n",
    "        '''\n",
    "        sentence can be a string, or a list of words\n",
    "        '''\n",
    "        if isinstance(sentence,str): sentence = sentence.split(\" \")\n",
    "        if self.smallcase: sentence =  [word.lower() for word in sentence]\n",
    "        encoded_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in self.word_to_index: encoded_sentence.append(self.word_to_index[word])\n",
    "            else: encoded_sentence.append(self.word_to_index[self.unknown_token])\n",
    "        return encoded_sentence\n",
    "    \n",
    "    def get_decoding(self,encoded_sentence):\n",
    "        '''\n",
    "        encoded_sentence must be a list of vocab indices.\n",
    "        Ex: encoded_sentence = [24,21,4,1,..] \n",
    "        '''\n",
    "        sentence = [self.index_to_word[index] for index in encoded_sentence]\n",
    "        return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnjumbleEncoderModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,num_lstm_layers,hidden_size,make_bidirectional,debug):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.bidirectional = make_bidirectional\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gru = nn.GRU(input_size=embedding_dim,hidden_size=hidden_size,dropout=0.5,\n",
    "                            num_layers=num_lstm_layers,bidirectional=make_bidirectional,batch_first=True)\n",
    "        \n",
    "    def forward(self,x,h):\n",
    "        if self.debug: \n",
    "            print(\"_______________________________\")\n",
    "            print(\"\\t\\tEncoder\\t\\t\")\n",
    "            print(\"_______________________________\")\n",
    "        if self.debug: print(\"Before starting: x Shape:\",x.shape,\"Prev State Shape\",h.shape)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.relu(x)\n",
    "        if self.debug: print(\"Embedding, x Shape:\",x.shape)\n",
    "        \n",
    "        op,ht = self.gru(x,h)\n",
    "        if self.debug: print(\"GRU, op Shape:\",op.shape,\"ht shape\",ht.shape)\n",
    "        \n",
    "        if self.bidirectional: \n",
    "            ht_for_decoder = torch.cat((ht[-1],ht[-2]),axis=1)\n",
    "            ht_for_decoder = ht_for_decoder.unsqueeze(0)\n",
    "        else: ht_for_decoder = ht[-1].unsqueeze(0)\n",
    "        if self.debug: print(\"ht for decoder shape\",ht_for_decoder.shape)\n",
    "            \n",
    "        return op,ht,ht_for_decoder\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        first_param = self.num_lstm_layers\n",
    "        if self.bidirectional: first_param *= 2\n",
    "        return torch.zeros(first_param, 1, self.hidden_size)\n",
    "\n",
    "class UnjumbleBahadnauAttention(nn.Module):\n",
    "    def __init__(self,attention_neurons,debug):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.rnn = nn.RNNCell(input_size=attention_neurons,hidden_size=attention_neurons,bias=False)\n",
    "        self.linear = nn.Linear(in_features = attention_neurons, out_features = 1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "    \n",
    "    def forward(self,op_from_enoder,st_minus_one_from_decoder):\n",
    "        \n",
    "        # Reshape the op_from_enoder from (batch_size,seq_length,lstm_neurons) to (batch_size*seq_length,lstm_neurons)\n",
    "        # And reshape st_minus_one_from_decoder from (1,batch_size,lstm_neurons) to (batch_size,lstm_neurons)\n",
    "        # And repeat st_minus_one_from_decoder to seq_length times to get (batch_size*seq_length,lstm_neurons)\n",
    "        if self.debug: \n",
    "            print(\"_______________________________\")\n",
    "            print(\"\\t\\tAttention\\t\\t\")\n",
    "            print(\"_______________________________\")\n",
    "        seq_length = op_from_enoder.shape[1]\n",
    "        op_from_enoder = op_from_enoder.reshape(-1,op_from_enoder.shape[2])\n",
    "        st_minus_one_from_decoder = st_minus_one_from_decoder[-1]\n",
    "        st_minus_one_from_decoder = st_minus_one_from_decoder.repeat(seq_length,1)\n",
    "        if self.debug: print(\"Shape of op_from_encoder:\",op_from_enoder.shape,\n",
    "                             \"Shape of st_minus_one_from_decoder:\",st_minus_one_from_decoder.shape)\n",
    "            \n",
    "        rnn_op = self.rnn(op_from_enoder,st_minus_one_from_decoder)\n",
    "        if self.debug: print(\"RNN Cell Op:\",rnn_op.shape)\n",
    "            \n",
    "        linear_op = self.linear(rnn_op)\n",
    "        if self.debug: print(\"Linear Op:\",linear_op.shape)\n",
    "            \n",
    "        softmax_op = self.softmax(linear_op)\n",
    "        if self.debug: print(\"Softmax Op:\",softmax_op.shape)\n",
    "            \n",
    "        ct = torch.sum(torch.mul(op_from_enoder,softmax_op),dim=0).unsqueeze(0)\n",
    "        if self.debug: print(\"Weighted Averaged h vectors:\",ct.shape)\n",
    "        \n",
    "        return ct,softmax_op\n",
    "        \n",
    "\n",
    "class UnjumbleDecoderModel(nn.Module):\n",
    "    def __init__(self,model_attention,vocab_size,embedding_dim,num_lstm_layers,\n",
    "                 hidden_size,make_bidirectional,debug):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.model_attention = model_attention\n",
    "        self.bidirectional = make_bidirectional\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gru_input_size = embedding_dim  + hidden_size\n",
    "        self.gru = nn.GRU(input_size=self.gru_input_size,hidden_size=hidden_size,\n",
    "                            num_layers=num_lstm_layers,bidirectional=make_bidirectional,batch_first=True)\n",
    "        self.in_features = hidden_size*2 if make_bidirectional else hidden_size\n",
    "        self.linear = nn.Linear(in_features=self.in_features, out_features=vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self,x,s0_from_encoder,op_from_encoder_for_attn):\n",
    "        if self.debug: \n",
    "            print(\"_______________________________\")\n",
    "            print(\"\\t\\tDecoder\\t\\t\")\n",
    "            print(\"_______________________________\")\n",
    "        if self.debug: print(\"Before starting: x Shape:\",x.shape,\" s0_from_encoder Shape:\",s0_from_encoder.shape)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.relu(x)\n",
    "        if self.debug: print(\"Embedding, x Shape:\",x.shape)\n",
    "        \n",
    "        \n",
    "        seq_length = x.shape[1]\n",
    "        if self.debug: print(\"Sequence Length:\",seq_length)\n",
    "        \n",
    "        all_timestep_op = []\n",
    "        for i in range(seq_length):\n",
    "            if i == 0: \n",
    "                ct,softmax_op = self.model_attention(op_from_encoder_for_attn,s0_from_encoder)\n",
    "                concatenated_x = torch.cat((x[0][i].unsqueeze(0),ct),axis=1)\n",
    "                if self.debug: print(\"concatenated_x shape:\",concatenated_x.shape)\n",
    "                gru_op,ht = self.gru(concatenated_x.unsqueeze(0),s0_from_encoder)\n",
    "                \n",
    "            else: \n",
    "                ct,softmax_op = self.model_attention(op_from_encoder_for_attn,ht)\n",
    "                concatenated_x = torch.cat((x[0][i].unsqueeze(0),ct),axis=1)\n",
    "                if self.debug: print(\"concatenated_x shape:\",concatenated_x.shape)\n",
    "                gru_op,ht = self.gru(concatenated_x.unsqueeze(0),ht)\n",
    "            \n",
    "            all_timestep_op.append(gru_op)\n",
    "            if self.debug:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"GRU_OP:\",gru_op.shape,\"Ht:\",ht.shape)\n",
    "                print(\"---------------------------------\")\n",
    "        \n",
    "        gru_final_op = torch.cat(all_timestep_op,axis=1)\n",
    "        if self.debug: print(\"GRU, Final Shape:\",gru_final_op.shape,\"ht shape\",ht.shape)\n",
    "            \n",
    "        # Resizing caption for Linear Layer\n",
    "        gru_final_op = gru_final_op.reshape(-1,gru_final_op.shape[2])\n",
    "        if self.debug: print(\"Reshaping gru_final_op Shape:\",gru_final_op.shape)\n",
    "        \n",
    "        linear_op = self.linear(gru_final_op)\n",
    "        if self.debug: print(\"Linear linear_op Shape:\",linear_op.shape)\n",
    "        \n",
    "        op = self.log_softmax(linear_op)\n",
    "        if self.debug: print(\"log_softmax op Shape:\",op.shape)\n",
    "            \n",
    "        if self.debug:print(\"_______________________________\\n\\n\")\n",
    "            \n",
    "        return op,ht,softmax_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40460 40460 40460\n"
     ]
    }
   ],
   "source": [
    "unknown_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "Xe,Xd,Y = [],[],[]\n",
    "with open(\"DataSets/Jumble/processed_jumbled.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        Xe.append(line.strip()+\" \" +eos_token)\n",
    "with open(\"DataSets/Jumble/processed_unjumbled.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        Xd.append(sos_token+\" \"+line.strip())\n",
    "        Y.append(line.strip()+\" \" +eos_token)\n",
    "print(len(Xe),len(Xd),len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36414 4046 36414 4046 36414 4046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3005, 3005, 3005, 3005)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr_e, Xval_e, Xtr_d, Xval_d, Ytr, Yval = train_test_split(Xe,Xd,Y,test_size=0.1,random_state=20)\n",
    "print(len(Xtr_e), len(Xval_e), len(Xtr_d), len(Xval_d), len(Ytr), len(Yval))\n",
    "encoder_vocab_builder = VocabBuilder(Xtr_e,unknown_token=unknown_token,pad_token=pad_token,sos_token=sos_token,eos_token=eos_token)\n",
    "decoder_vocab_builder = VocabBuilder(Xtr_d,unknown_token=unknown_token,pad_token=pad_token,sos_token=sos_token,eos_token=eos_token)\n",
    "encoder_wtoi,encoder_itow = encoder_vocab_builder.word_to_index, encoder_vocab_builder.index_to_word\n",
    "decoder_wtoi,decoder_itow = decoder_vocab_builder.word_to_index, decoder_vocab_builder.index_to_word\n",
    "len(encoder_itow),len(encoder_wtoi),len(decoder_wtoi),len(decoder_itow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "hidden_size_encoder = 400\n",
    "hidden_size_decoder = hidden_size_encoder\n",
    "model_encoder = UnjumbleEncoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=300,num_lstm_layers=2,\n",
    "    hidden_size=hidden_size_encoder,make_bidirectional=True,debug=True\n",
    ").to(device)\n",
    "if model_encoder.bidirectional: hidden_size_decoder = 2*hidden_size_encoder\n",
    "model_attention = UnjumbleBahadnauAttention(hidden_size_decoder,debug=True).to(device)\n",
    "model_decoder = UnjumbleDecoderModel(\n",
    "    model_attention = model_attention,\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=300,num_lstm_layers=1,\n",
    "    hidden_size=hidden_size_decoder,make_bidirectional=False,debug=True\n",
    ").to(device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer_encoder = torch.optim.Adam(model_encoder.parameters(),lr=0.003)\n",
    "optimizer_decoder = torch.optim.Adam(model_decoder.parameters(),lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". man snowy a mountain descends a <eos> [7, 47, 48, 13, 49, 50, 13, 9]\n",
      "<sos> a man descends a snowy mountain . [0, 10, 47, 48, 10, 49, 50, 9]\n",
      "a man descends a snowy mountain . <eos> [10, 47, 48, 10, 49, 50, 9, 3004]\n",
      "torch.Size([1, 8]) torch.Size([1, 8]) torch.Size([1, 8])\n",
      "_______________________________\n",
      "\t\tEncoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 8]) Prev State Shape torch.Size([4, 1, 400])\n",
      "Embedding, x Shape: torch.Size([1, 8, 300])\n",
      "GRU, op Shape: torch.Size([1, 8, 800]) ht shape torch.Size([4, 1, 400])\n",
      "ht for decoder shape torch.Size([1, 1, 800])\n",
      "_______________________________\n",
      "\t\tDecoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 8])  s0_from_encoder Shape: torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 8, 300])\n",
      "Sequence Length: 8\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([8, 800]) Shape of st_minus_one_from_decoder: torch.Size([8, 800])\n",
      "RNN Cell Op: torch.Size([8, 800])\n",
      "Linear Op: torch.Size([8, 1])\n",
      "Softmax Op: torch.Size([8, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([8, 800]) Shape of st_minus_one_from_decoder: torch.Size([8, 800])\n",
      "RNN Cell Op: torch.Size([8, 800])\n",
      "Linear Op: torch.Size([8, 1])\n",
      "Softmax Op: torch.Size([8, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([8, 800]) Shape of st_minus_one_from_decoder: torch.Size([8, 800])\n",
      "RNN Cell Op: torch.Size([8, 800])\n",
      "Linear Op: torch.Size([8, 1])\n",
      "Softmax Op: torch.Size([8, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([8, 800]) Shape of st_minus_one_from_decoder: torch.Size([8, 800])\n",
      "RNN Cell Op: torch.Size([8, 800])\n",
      "Linear Op: torch.Size([8, 1])\n",
      "Softmax Op: torch.Size([8, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([8, 800]) Shape of st_minus_one_from_decoder: torch.Size([8, 800])\n",
      "RNN Cell Op: torch.Size([8, 800])\n",
      "Linear Op: torch.Size([8, 1])\n",
      "Softmax Op: torch.Size([8, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([8, 800]) Shape of st_minus_one_from_decoder: torch.Size([8, 800])\n",
      "RNN Cell Op: torch.Size([8, 800])\n",
      "Linear Op: torch.Size([8, 1])\n",
      "Softmax Op: torch.Size([8, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([8, 800]) Shape of st_minus_one_from_decoder: torch.Size([8, 800])\n",
      "RNN Cell Op: torch.Size([8, 800])\n",
      "Linear Op: torch.Size([8, 1])\n",
      "Softmax Op: torch.Size([8, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([8, 800]) Shape of st_minus_one_from_decoder: torch.Size([8, 800])\n",
      "RNN Cell Op: torch.Size([8, 800])\n",
      "Linear Op: torch.Size([8, 1])\n",
      "Softmax Op: torch.Size([8, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "GRU, Final Shape: torch.Size([1, 8, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping gru_final_op Shape: torch.Size([8, 800])\n",
      "Linear linear_op Shape: torch.Size([8, 3005])\n",
      "log_softmax op Shape: torch.Size([8, 3005])\n",
      "_______________________________\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "torch.Size([1, 9]) torch.Size([1, 9]) torch.Size([1, 9])\n",
      "_______________________________\n",
      "\t\tEncoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 9]) Prev State Shape torch.Size([4, 1, 400])\n",
      "Embedding, x Shape: torch.Size([1, 9, 300])\n",
      "GRU, op Shape: torch.Size([1, 9, 800]) ht shape torch.Size([4, 1, 400])\n",
      "ht for decoder shape torch.Size([1, 1, 800])\n",
      "_______________________________\n",
      "\t\tDecoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 9])  s0_from_encoder Shape: torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 9, 300])\n",
      "Sequence Length: 9\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([9, 800]) Shape of st_minus_one_from_decoder: torch.Size([9, 800])\n",
      "RNN Cell Op: torch.Size([9, 800])\n",
      "Linear Op: torch.Size([9, 1])\n",
      "Softmax Op: torch.Size([9, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([9, 800]) Shape of st_minus_one_from_decoder: torch.Size([9, 800])\n",
      "RNN Cell Op: torch.Size([9, 800])\n",
      "Linear Op: torch.Size([9, 1])\n",
      "Softmax Op: torch.Size([9, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([9, 800]) Shape of st_minus_one_from_decoder: torch.Size([9, 800])\n",
      "RNN Cell Op: torch.Size([9, 800])\n",
      "Linear Op: torch.Size([9, 1])\n",
      "Softmax Op: torch.Size([9, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([9, 800]) Shape of st_minus_one_from_decoder: torch.Size([9, 800])\n",
      "RNN Cell Op: torch.Size([9, 800])\n",
      "Linear Op: torch.Size([9, 1])\n",
      "Softmax Op: torch.Size([9, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([9, 800]) Shape of st_minus_one_from_decoder: torch.Size([9, 800])\n",
      "RNN Cell Op: torch.Size([9, 800])\n",
      "Linear Op: torch.Size([9, 1])\n",
      "Softmax Op: torch.Size([9, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([9, 800]) Shape of st_minus_one_from_decoder: torch.Size([9, 800])\n",
      "RNN Cell Op: torch.Size([9, 800])\n",
      "Linear Op: torch.Size([9, 1])\n",
      "Softmax Op: torch.Size([9, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([9, 800]) Shape of st_minus_one_from_decoder: torch.Size([9, 800])\n",
      "RNN Cell Op: torch.Size([9, 800])\n",
      "Linear Op: torch.Size([9, 1])\n",
      "Softmax Op: torch.Size([9, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([9, 800]) Shape of st_minus_one_from_decoder: torch.Size([9, 800])\n",
      "RNN Cell Op: torch.Size([9, 800])\n",
      "Linear Op: torch.Size([9, 1])\n",
      "Softmax Op: torch.Size([9, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([9, 800]) Shape of st_minus_one_from_decoder: torch.Size([9, 800])\n",
      "RNN Cell Op: torch.Size([9, 800])\n",
      "Linear Op: torch.Size([9, 1])\n",
      "Softmax Op: torch.Size([9, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "GRU, Final Shape: torch.Size([1, 9, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping gru_final_op Shape: torch.Size([9, 800])\n",
      "Linear linear_op Shape: torch.Size([9, 3005])\n",
      "log_softmax op Shape: torch.Size([9, 3005])\n",
      "_______________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_index = 6\n",
    "encoder_encode_decode = EncodeDecode(encoder_wtoi,encoder_itow,pad_token,unknown_token)\n",
    "decoder_encode_decode = EncodeDecode(decoder_wtoi,decoder_itow,pad_token,unknown_token)\n",
    "print(Xtr_e[data_index],encoder_encode_decode.get_encoding(Xtr_e[data_index]))\n",
    "print(Xtr_d[data_index],decoder_encode_decode.get_encoding(Xtr_d[data_index]))\n",
    "print(Ytr[data_index],decoder_encode_decode.get_encoding(Ytr[data_index]))\n",
    "\n",
    "init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "model_encoder.train()\n",
    "model_decoder.train()\n",
    "\n",
    "optimizer_encoder.zero_grad()\n",
    "optimizer_decoder.zero_grad()\n",
    "Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtr_e[data_index])]).to(device)\n",
    "Xd_b = torch.tensor([decoder_encode_decode.get_encoding(Xtr_d[data_index])]).to(device)\n",
    "Y_b = torch.tensor([decoder_encode_decode.get_encoding(Ytr[data_index])]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "op_from_encoder,ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "op,_,_ = model_decoder(Xd_b,ht_for_decoder,op_from_encoder)\n",
    "ht = ht.detach()\n",
    "loss = loss_fn(op,Y_b.reshape(-1))\n",
    "loss.backward()\n",
    "optimizer_encoder.step()\n",
    "optimizer_decoder.step()\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "optimizer_encoder.zero_grad()\n",
    "optimizer_decoder.zero_grad()\n",
    "Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtr_e[data_index+1])]).to(device)\n",
    "Xd_b = torch.tensor([decoder_encode_decode.get_encoding(Xtr_d[data_index+1])]).to(device)\n",
    "Y_b = torch.tensor([decoder_encode_decode.get_encoding(Ytr[data_index+1])]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "op_from_encoder,ht,ht_for_decoder = model_encoder(Xe_b,ht)\n",
    "op,_,_ = model_decoder(Xd_b,ht_for_decoder,op_from_encoder)\n",
    "ht = ht.detach()\n",
    "loss = loss_fn(op,Y_b.reshape(-1))\n",
    "loss.backward()\n",
    "optimizer_encoder.step()\n",
    "optimizer_decoder.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi):\n",
    "    data_index = random.randint(0,100)\n",
    "    Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xval_e[data_index])]).to(device)\n",
    "    print(Xval_e[data_index],Xe_b)\n",
    "    print(Xval_d[data_index])\n",
    "    \n",
    "    model_encoder.eval()\n",
    "    model_decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        softmax_ops = []\n",
    "        init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "        op_from_encoder,ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "        sos_word = torch.tensor([[decoder_wtoi[\"<sos>\"]]]).to(device)\n",
    "        op,ht,softmax_op = model_decoder(sos_word,ht_for_decoder,op_from_encoder)\n",
    "        softmax_ops.append([round(float(el),3) for el in softmax_op.cpu()])\n",
    "        unjumbled_sentence = []\n",
    "        for i in range(25):\n",
    "            predicted_word = torch.argmax(op,axis=1).tolist()\n",
    "#             print(\"Predicted .....................\",predicted_word)\n",
    "            unjumbled_sentence.append(decoder_itow[predicted_word[0]])\n",
    "            if predicted_word[0] == decoder_wtoi[\"<eos>\"]: break\n",
    "            op,ht,softmax_op = model_decoder(torch.tensor([predicted_word]).to(device),ht,op_from_encoder)\n",
    "            softmax_ops.append([round(float(el),3) for el in softmax_op.cpu()])\n",
    "        print(\"_______________________________________\")\n",
    "        print(unjumbled_sentence)\n",
    "        print(\"attention weights\")\n",
    "        df = pd.DataFrame(softmax_ops)\n",
    "        if df.shape[0] == len(unjumbled_sentence):\n",
    "            jumbled_words = Xval_e[data_index].split()\n",
    "            df.columns = jumbled_words\n",
    "            df.index = unjumbled_sentence\n",
    "            plt.figure(figsize=(16,4))\n",
    "            sns.heatmap(df, annot=True)\n",
    "            plt.show()\n",
    "        print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while in the jacket three wears dogs dog . a playing one snow <eos> tensor([[137,  11,   4, 764,  97, 821,  73,  52,   7,  13, 155,  95, 122,   9]],\n",
      "       device='cuda:0')\n",
      "<sos> three dogs playing in the snow while one dog wears a jacket .\n",
      "_______________________________\n",
      "\t\tEncoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 14]) Prev State Shape torch.Size([4, 1, 400])\n",
      "Embedding, x Shape: torch.Size([1, 14, 300])\n",
      "GRU, op Shape: torch.Size([1, 14, 800]) ht shape torch.Size([4, 1, 400])\n",
      "ht for decoder shape torch.Size([1, 1, 800])\n",
      "_______________________________\n",
      "\t\tDecoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 1])  s0_from_encoder Shape: torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 1, 300])\n",
      "Sequence Length: 1\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([14, 800]) Shape of st_minus_one_from_decoder: torch.Size([14, 800])\n",
      "RNN Cell Op: torch.Size([14, 800])\n",
      "Linear Op: torch.Size([14, 1])\n",
      "Softmax Op: torch.Size([14, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "GRU, Final Shape: torch.Size([1, 1, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping gru_final_op Shape: torch.Size([1, 800])\n",
      "Linear linear_op Shape: torch.Size([1, 3005])\n",
      "log_softmax op Shape: torch.Size([1, 3005])\n",
      "_______________________________\n",
      "\n",
      "\n",
      "_______________________________\n",
      "\t\tDecoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 1])  s0_from_encoder Shape: torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 1, 300])\n",
      "Sequence Length: 1\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([14, 800]) Shape of st_minus_one_from_decoder: torch.Size([14, 800])\n",
      "RNN Cell Op: torch.Size([14, 800])\n",
      "Linear Op: torch.Size([14, 1])\n",
      "Softmax Op: torch.Size([14, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "GRU, Final Shape: torch.Size([1, 1, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping gru_final_op Shape: torch.Size([1, 800])\n",
      "Linear linear_op Shape: torch.Size([1, 3005])\n",
      "log_softmax op Shape: torch.Size([1, 3005])\n",
      "_______________________________\n",
      "\n",
      "\n",
      "_______________________________\n",
      "\t\tDecoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 1])  s0_from_encoder Shape: torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 1, 300])\n",
      "Sequence Length: 1\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([14, 800]) Shape of st_minus_one_from_decoder: torch.Size([14, 800])\n",
      "RNN Cell Op: torch.Size([14, 800])\n",
      "Linear Op: torch.Size([14, 1])\n",
      "Softmax Op: torch.Size([14, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "GRU, Final Shape: torch.Size([1, 1, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping gru_final_op Shape: torch.Size([1, 800])\n",
      "Linear linear_op Shape: torch.Size([1, 3005])\n",
      "log_softmax op Shape: torch.Size([1, 3005])\n",
      "_______________________________\n",
      "\n",
      "\n",
      "_______________________________________\n",
      "['the', '.', '<eos>']\n",
      "attention weights\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "predict(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "hidden_size_encoder = 400\n",
    "hidden_size_decoder = hidden_size_encoder\n",
    "model_encoder = UnjumbleEncoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=300,num_lstm_layers=2,\n",
    "    hidden_size=hidden_size_encoder,make_bidirectional=True,debug=False\n",
    ").to(device)\n",
    "if model_encoder.bidirectional: hidden_size_decoder = 2*hidden_size_encoder\n",
    "model_attention = UnjumbleBahadnauAttention(hidden_size_decoder,debug=False).to(device)\n",
    "model_decoder = UnjumbleDecoderModel(\n",
    "    model_attention = model_attention,\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=300,num_lstm_layers=1,\n",
    "    hidden_size=hidden_size_decoder,make_bidirectional=False,debug=False\n",
    ").to(device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer_encoder = torch.optim.Adam(model_encoder.parameters(),lr=0.0003)\n",
    "optimizer_decoder = torch.optim.Adam(model_decoder.parameters(),lr=0.0003)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "    model_encoder.train()\n",
    "    model_decoder.train()\n",
    "    epoch_loss = 0\n",
    "    for j in range(len(Xtr_e)):\n",
    "        optimizer_encoder.zero_grad()\n",
    "        optimizer_decoder.zero_grad()\n",
    "        Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtr_e[j])]).to(device)\n",
    "        Xd_b = torch.tensor([decoder_encode_decode.get_encoding(Xtr_d[j])]).to(device)\n",
    "        Y_b = torch.tensor([decoder_encode_decode.get_encoding(Ytr[j])]).to(device)\n",
    "        op_from_encoder,ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "        op,_,_ = model_decoder(Xd_b,ht_for_decoder,op_from_encoder)\n",
    "#         ht = ht.detach()\n",
    "#         init_ht_for_encoder = ht\n",
    "        loss = loss_fn(op,Y_b.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer_encoder.step()\n",
    "        optimizer_decoder.step()\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        if j%2000 == 0: print(\"Epoch:\",i,\"Batch:\",j,\"Loss:\",batch_loss)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Epoch Loss:\",epoch_loss)\n",
    "    predict(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "    print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_whole_val(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi):\n",
    "    model_encoder.eval()\n",
    "    model_decoder.eval()\n",
    "    accuracy_tuple_list = []  #[(jumbed_sent,unjumbled_sent,predicted_sent,hard,soft,word_count),...,]\n",
    "    with torch.no_grad():\n",
    "        for data_index in range(len(Xval_e)):\n",
    "            if data_index % 50 == 0: print(data_index,end = ' ')\n",
    "            Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xval_e[data_index])]).to(device)\n",
    "\n",
    "            init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "            op_from_encoder,ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "            sos_word = torch.tensor([[decoder_wtoi[\"<sos>\"]]]).to(device)\n",
    "            op,ht,softmax_op = model_decoder(sos_word,ht_for_decoder,op_from_encoder)\n",
    "            unjumbled_sentence = []\n",
    "            for i in range(25):\n",
    "                predicted_word = torch.argmax(op,axis=1).tolist()\n",
    "                unjumbled_sentence.append(decoder_itow[predicted_word[0]])\n",
    "                if predicted_word[0] == decoder_wtoi[\"<eos>\"]: break\n",
    "                op,ht,softmax_op = model_decoder(torch.tensor([predicted_word]).to(device),ht,op_from_encoder)\n",
    "                \n",
    "            hard_accuracy = 1 if \" \".join(unjumbled_sentence) == Yval[data_index] else 0\n",
    "            word_count = len(set(Yval[data_index].split()))\n",
    "            soft_accuracy = len(set(unjumbled_sentence).intersection(set(Yval[data_index].split())))/word_count\n",
    "            accuracy_tuple_list.append(\n",
    "                (Xval_e[data_index],Yval[data_index],\" \".join(unjumbled_sentence),hard_accuracy,soft_accuracy,word_count)\n",
    "            )\n",
    "    return accuracy_tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tuple_list = predict_on_whole_val(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "df = pd.DataFrame(accuracy_tuple_list)\n",
    "df.columns = [\"jumbled_sent\",\"unjumbled_sent\",\"prediction\",\"hard_accuracy\",\"soft_accuracy\",\"word_count\"]\n",
    "print(df.shape,df['hard_accuracy'].sum())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum((df['soft_accuracy']*df['word_count']).tolist())\n",
    "b = df['word_count'].sum()\n",
    "a,b,a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(model_encoder_new,model_decoder_new,encoder_encode_decode,decoder_itow,decoder_wtoi):\n",
    "    model_encoder.eval()\n",
    "    model_decoder.eval()\n",
    "    accuracy_tuple_list = []  #[(jumbed_sent,unjumbled_sent,predicted_sent,hard,soft,word_count),...,]\n",
    "    with torch.no_grad():\n",
    "        for data_index in range(len(Xtest_e)):\n",
    "            if data_index % 50 == 0: print(data_index,end = ' ')\n",
    "            Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtest_e[data_index])]).to(device)\n",
    "\n",
    "            init_ht_for_encoder = model_encoder_new.init_hidden().to(device)\n",
    "            op_from_encoder,ht,ht_for_decoder = model_encoder_new(Xe_b,init_ht_for_encoder)\n",
    "            sos_word = torch.tensor([[decoder_wtoi[\"<sos>\"]]]).to(device)\n",
    "            op,ht,softmax_op = model_decoder_new(sos_word,ht_for_decoder,op_from_encoder)\n",
    "            unjumbled_sentence = []\n",
    "            for i in range(25):\n",
    "                predicted_word = torch.argmax(op,axis=1).tolist()\n",
    "                unjumbled_sentence.append(decoder_itow[predicted_word[0]])\n",
    "                if predicted_word[0] == decoder_wtoi[\"<eos>\"]: break\n",
    "                op,ht,softmax_op = model_decoder_new(torch.tensor([predicted_word]).to(device),ht,op_from_encoder)\n",
    "            hard_accuracy = 1 if \" \".join(unjumbled_sentence) == Ytest[data_index] else 0\n",
    "            word_count = len(set(Ytest[data_index].split()))\n",
    "            soft_accuracy = len(set(unjumbled_sentence).intersection(set(Ytest[data_index].split())))/word_count\n",
    "            accuracy_tuple_list.append(\n",
    "                (Xtest_e[data_index],Ytest[data_index],\" \".join(unjumbled_sentence),hard_accuracy,soft_accuracy,word_count)\n",
    "            )\n",
    "    return accuracy_tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_e = [\n",
    "    \"is eating . Nitish apple <eos>\",\n",
    "    \"is city my favorite New York. <eos>\",\n",
    "    \"a a and dog are man woods walking through the . <eos>\"\n",
    "]\n",
    "Ytest = [\n",
    "    \"Nitish is eating apple . <eos>\",\n",
    "    \"New York is my faorite city. <eos>\",\n",
    "    \"a man and a dog are walking through the woods . <eos>\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tuple_list_test = predict_on_test(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "df_test = pd.DataFrame(accuracy_tuple_list_test)\n",
    "df_test.columns = [\"jumbled_sent\",\"unjumbled_sent\",\"prediction\",\"hard_accuracy\",\"soft_accuracy\",\"word_count\"]\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Model was not required to be saved explicitly as it is a part of the decoder model only.\n",
    "#### Additionally the word_to_index and index_to_word dictionary and get_encoding funtion will be required for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_encoder,\"SavedModels/Jumble/encoder_model.pt\")\n",
    "torch.save(model_attention,\"SavedModels/Jumble/attention_model.pt\")\n",
    "torch.save(model_decoder,\"SavedModels/Jumble/decoder_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_encoder_model = torch.load(\"SavedModels/Jumble/encoder_model.pt\")\n",
    "loaded_decoder_model = torch.load(\"SavedModels/Jumble/decoder_model.pt\")\n",
    "model_attention = torch.load(\"SavedModels/Jumble/attention_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumbled_sent</th>\n",
       "      <th>unjumbled_sent</th>\n",
       "      <th>prediction</th>\n",
       "      <th>hard_accuracy</th>\n",
       "      <th>soft_accuracy</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is eating . Nitish apple &lt;eos&gt;</td>\n",
       "      <td>Nitish is eating apple . &lt;eos&gt;</td>\n",
       "      <td>the line &lt;unk&gt; fish her fish &lt;eos&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is city my favorite New York. &lt;eos&gt;</td>\n",
       "      <td>New York is my faorite city. &lt;eos&gt;</td>\n",
       "      <td>her &lt;unk&gt; &lt;unk&gt; city &lt;unk&gt; &lt;eos&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a a and dog are man woods walking through the ...</td>\n",
       "      <td>a man and a dog are walking through the woods ...</td>\n",
       "      <td>man walks through two soccer men sits &lt;eos&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        jumbled_sent  \\\n",
       "0                     is eating . Nitish apple <eos>   \n",
       "1                is city my favorite New York. <eos>   \n",
       "2  a a and dog are man woods walking through the ...   \n",
       "\n",
       "                                      unjumbled_sent  \\\n",
       "0                     Nitish is eating apple . <eos>   \n",
       "1                 New York is my faorite city. <eos>   \n",
       "2  a man and a dog are walking through the woods ...   \n",
       "\n",
       "                                    prediction  hard_accuracy  soft_accuracy  \\\n",
       "0           the line <unk> fish her fish <eos>              0       0.166667   \n",
       "1             her <unk> <unk> city <unk> <eos>              0       0.142857   \n",
       "2  man walks through two soccer men sits <eos>              0       0.272727   \n",
       "\n",
       "   word_count  \n",
       "0           6  \n",
       "1           7  \n",
       "2          11  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_tuple_list_test = predict_on_test(loaded_encoder_model,loaded_decoder_model,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "df_test = pd.DataFrame(accuracy_tuple_list_test)\n",
    "df_test.columns = [\"jumbled_sent\",\"unjumbled_sent\",\"prediction\",\"hard_accuracy\",\"soft_accuracy\",\"word_count\"]\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitf0b0a3d2859f4904a6dd3c0263fd37ec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
