{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_files():\n",
    "    if os.path.exists(\"jumbled.txt\"): os.remove(\"jumbled.txt\")\n",
    "    if os.path.exists(\"unjumbled.txt\"): os.remove(\"unjumbled.txt\")   \n",
    "    f1 = open(\"jumbled.txt\",\"w\")\n",
    "    f2 = open(\"unjumbled.txt\",\"w\")\n",
    "    with open(\"source.txt\",\"r\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.lower().strip().split()\n",
    "            sentence = split_line[1:]\n",
    "            f2.write(' '.join(sentence)+\"\\n\")\n",
    "            random.shuffle(sentence)\n",
    "            f1.write(\" \".join(sentence)+\"\\n\")\n",
    "    f1.close()\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_of_words():\n",
    "    word_count_dict = {}\n",
    "    with open(\"../Datasets/Jumble_Unjumble/jumbled.txt\",\"r\") as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                if word in word_count_dict: word_count_dict[word] += 1\n",
    "                else: word_count_dict[word] = 1\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8918\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = get_count_of_words()\n",
    "print(len(word_count_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5237\n"
     ]
    }
   ],
   "source": [
    "min_word_count = 1\n",
    "count = 0\n",
    "for k,v in word_count_dict.items():\n",
    "    if v > min_word_count: count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_less_frequent_words(filename,word_count_dict,min_word_count,replace_token=\"<unk>\"):\n",
    "    with open(filename,\"r\") as f:\n",
    "        with open(\"../Datasets/Jumble_Unjumble/processed_\"+filename.split(\"/\")[-1],\"w\") as f1:\n",
    "            for line in f:\n",
    "                words = line.strip().split()\n",
    "                sentence_to_write = []\n",
    "                for word in words:\n",
    "                    if word_count_dict[word] > min_word_count: sentence_to_write.append(word)\n",
    "                    else: sentence_to_write.append(replace_token)\n",
    "                sentence_to_write.append(\"\\n\")\n",
    "                f1.write(\" \".join(sentence_to_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_less_frequent_words(\"../Datasets/Jumble_Unjumble/jumbled.txt\",word_count_dict,min_word_count=min_word_count,replace_token=\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_less_frequent_words(\"../Datasets/Jumble_Unjumble/unjumbled.txt\",word_count_dict,min_word_count=min_word_count,replace_token=\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabBuilder:\n",
    "    def __init__(self,text_corpus,unknown_token=None,pad_token=None,sos_token=None,eos_token=None):\n",
    "        '''\n",
    "        text_corpus = [\n",
    "            sentence_1,  # sentence_1 = \"a yellow car ...\"\n",
    "            sentence_2\n",
    "            ...\n",
    "        ]\n",
    "        '''\n",
    "        self.text_corpus = text_corpus\n",
    "        self.unknown_token = unknown_token or \"<unk>\"\n",
    "        self.pad_token = pad_token or \"<pad>\"\n",
    "        self.sos_token = sos_token or \"<sos>\"\n",
    "        self.eos_token = eos_token or \"<eos>\"\n",
    "        self.word_to_index, self.index_to_word = self.get_vocabs()\n",
    "                        \n",
    "    def get_vocabs(self):\n",
    "        word_to_index = {}\n",
    "        index_count = 0\n",
    "        for sentence in self.text_corpus:\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                if word not in word_to_index:\n",
    "                    word_to_index[word] = index_count\n",
    "                    index_count += 1\n",
    "        if not self.unknown_token in word_to_index: \n",
    "            word_to_index[self.unknown_token] = index_count\n",
    "            index_count += 1\n",
    "        if not self.pad_token in word_to_index: \n",
    "            word_to_index[self.pad_token] = index_count\n",
    "            index_count += 1\n",
    "        if not self.sos_token in word_to_index: \n",
    "            word_to_index[self.sos_token] = index_count\n",
    "            index_count += 1\n",
    "        if not self.eos_token in word_to_index: \n",
    "            word_to_index[self.eos_token] = index_count\n",
    "            index_count += 1\n",
    "        index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "        return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeDecode:\n",
    "    def __init__(self,word_to_index,index_to_word,pad_token,unknown_token,smallcase=True):\n",
    "        self.smallcase = smallcase\n",
    "        self.word_to_index = word_to_index\n",
    "        self.index_to_word = index_to_word\n",
    "        self.pad_token = pad_token\n",
    "        self.unknown_token = unknown_token\n",
    "    \n",
    "    def get_encoding(self,sentence):\n",
    "        '''\n",
    "        sentence can be a string, or a list of words\n",
    "        '''\n",
    "        if isinstance(sentence,str): sentence = sentence.split(\" \")\n",
    "        if self.smallcase: sentence =  [word.lower() for word in sentence]\n",
    "        encoded_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in self.word_to_index: encoded_sentence.append(self.word_to_index[word])\n",
    "            else: encoded_sentence.append(self.word_to_index[self.unknown_token])\n",
    "        return encoded_sentence\n",
    "    \n",
    "    def get_decoding(self,encoded_sentence):\n",
    "        '''\n",
    "        encoded_sentence must be a list of vocab indices.\n",
    "        Ex: encoded_sentence = [24,21,4,1,..] \n",
    "        '''\n",
    "        sentence = [self.index_to_word[index] for index in encoded_sentence]\n",
    "        return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnjumbleEncoderModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,num_lstm_layers,hidden_size,make_bidirectional,debug):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.bidirectional = make_bidirectional\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gru = nn.GRU(input_size=embedding_dim,hidden_size=hidden_size,dropout=0.5,\n",
    "                            num_layers=num_lstm_layers,bidirectional=make_bidirectional,batch_first=True)\n",
    "        \n",
    "    def forward(self,x,h):\n",
    "        if self.debug: print(\"Before starting: x Shape:\",x.shape,\"Prev State Shape\",h.shape)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.relu(x)\n",
    "        if self.debug: print(\"Embedding, x Shape:\",x.shape)\n",
    "        \n",
    "        op,ht = self.gru(x,h)\n",
    "        if self.debug: print(\"GRU, op Shape:\",op.shape,\"ht shape\",ht.shape)\n",
    "        \n",
    "        if self.bidirectional: \n",
    "            ht_for_decoder = torch.cat((ht[-1],ht[-2]),axis=1)\n",
    "            ht_for_decoder = ht_for_decoder.unsqueeze(0)\n",
    "        else: ht_for_decoder = ht[-1].unsqueeze(0)\n",
    "        if self.debug: print(\"ht for decoder shape\",ht_for_decoder.shape)\n",
    "            \n",
    "        return ht,ht_for_decoder\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        first_param = self.num_lstm_layers\n",
    "        if self.bidirectional: first_param *= 2\n",
    "        return torch.zeros(first_param, 1, self.hidden_size)\n",
    "\n",
    "class UnjumbleDecoderModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,num_lstm_layers,hidden_size,make_bidirectional,debug):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.bidirectional = make_bidirectional\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gru = nn.GRU(input_size=embedding_dim,hidden_size=hidden_size,\n",
    "                            num_layers=num_lstm_layers,bidirectional=make_bidirectional,batch_first=True)\n",
    "        self.in_features = hidden_size*2 if make_bidirectional else hidden_size\n",
    "        self.linear = nn.Linear(in_features=self.in_features, out_features=vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self,x,prev_state):\n",
    "        if self.debug: print(\"Before starting: x Shape:\",x.shape,\"Prev State Shape\",prev_state.shape)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.relu(x)\n",
    "        if self.debug: print(\"Embedding, x Shape:\",x.shape)\n",
    "        \n",
    "        x,ht = self.gru(x,prev_state)\n",
    "        if self.debug: print(\"GRU, x Shape:\",x.shape,\"ht shape\",ht.shape)\n",
    "            \n",
    "        # Resizing caption for Linear Layer\n",
    "        x = x.reshape(-1,x.shape[2])\n",
    "        if self.debug: print(\"Reshaping x Shape:\",x.shape)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        if self.debug: print(\"Linear x Shape:\",x.shape)\n",
    "        \n",
    "        op = self.log_softmax(x)\n",
    "        if self.debug: print(\"log_softmax op Shape:\",op.shape)\n",
    "        \n",
    "        return op,ht\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40460 40460 40460\n"
     ]
    }
   ],
   "source": [
    "unknown_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "Xe,Xd,Y = [],[],[]\n",
    "with open(\"processed_jumbled.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        Xe.append(line.strip()+\" \" +eos_token)\n",
    "with open(\"processed_unjumbled.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        Xd.append(sos_token+\" \"+line.strip())\n",
    "        Y.append(line.strip()+\" \" +eos_token)\n",
    "print(len(Xe),len(Xd),len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36414 4046 36414 4046 36414 4046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3005, 3005, 3005, 3005)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr_e, Xval_e, Xtr_d, Xval_d, Ytr, Yval = train_test_split(Xe,Xd,Y,test_size=0.1,random_state=20)\n",
    "print(len(Xtr_e), len(Xval_e), len(Xtr_d), len(Xval_d), len(Ytr), len(Yval))\n",
    "encoder_vocab_builder = VocabBuilder(Xtr_e,unknown_token=unknown_token,pad_token=pad_token,sos_token=sos_token,eos_token=eos_token)\n",
    "decoder_vocab_builder = VocabBuilder(Xtr_d,unknown_token=unknown_token,pad_token=pad_token,sos_token=sos_token,eos_token=eos_token)\n",
    "encoder_wtoi,encoder_itow = encoder_vocab_builder.word_to_index, encoder_vocab_builder.index_to_word\n",
    "decoder_wtoi,decoder_itow = decoder_vocab_builder.word_to_index, decoder_vocab_builder.index_to_word\n",
    "len(encoder_itow),len(encoder_wtoi),len(decoder_wtoi),len(decoder_itow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "hidden_size_encoder = 400\n",
    "hidden_size_decoder = hidden_size_encoder\n",
    "model_encoder = UnjumbleEncoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=1000,num_lstm_layers=2,\n",
    "    hidden_size=hidden_size_encoder,make_bidirectional=True,debug=True\n",
    ").to(device)\n",
    "if model_encoder.bidirectional: hidden_size_decoder = 2*hidden_size_encoder\n",
    "model_decoder = UnjumbleDecoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=1000,num_lstm_layers=1,\n",
    "    hidden_size=hidden_size_decoder,make_bidirectional=False,debug=True\n",
    ").to(device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer_encoder = torch.optim.Adam(model_encoder.parameters(),lr=0.003)\n",
    "optimizer_decoder = torch.optim.Adam(model_decoder.parameters(),lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a snowy mountain . man descends a <eos> [11, 47, 48, 2, 49, 50, 11, 9]\n",
      "<sos> a man descends a snowy mountain . [0, 10, 47, 48, 10, 49, 50, 9]\n",
      "a man descends a snowy mountain . <eos> [10, 47, 48, 10, 49, 50, 9, 3004]\n",
      "torch.Size([1, 8]) torch.Size([1, 8]) torch.Size([1, 8])\n",
      "Before starting: x Shape: torch.Size([1, 8]) Prev State Shape torch.Size([4, 1, 400])\n",
      "Embedding, x Shape: torch.Size([1, 8, 1000])\n",
      "GRU, op Shape: torch.Size([1, 8, 800]) ht shape torch.Size([4, 1, 400])\n",
      "ht for decoder shape torch.Size([1, 1, 800])\n",
      "Before starting: x Shape: torch.Size([1, 8]) Prev State Shape torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 8, 1000])\n",
      "GRU, x Shape: torch.Size([1, 8, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping x Shape: torch.Size([8, 800])\n",
      "Linear x Shape: torch.Size([8, 3005])\n",
      "log_softmax op Shape: torch.Size([8, 3005])\n",
      "---------------------------------------------\n",
      "torch.Size([1, 9]) torch.Size([1, 9]) torch.Size([1, 9])\n",
      "Before starting: x Shape: torch.Size([1, 9]) Prev State Shape torch.Size([4, 1, 400])\n",
      "Embedding, x Shape: torch.Size([1, 9, 1000])\n",
      "GRU, op Shape: torch.Size([1, 9, 800]) ht shape torch.Size([4, 1, 400])\n",
      "ht for decoder shape torch.Size([1, 1, 800])\n",
      "Before starting: x Shape: torch.Size([1, 9]) Prev State Shape torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 9, 1000])\n",
      "GRU, x Shape: torch.Size([1, 9, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping x Shape: torch.Size([9, 800])\n",
      "Linear x Shape: torch.Size([9, 3005])\n",
      "log_softmax op Shape: torch.Size([9, 3005])\n"
     ]
    }
   ],
   "source": [
    "data_index = 6\n",
    "encoder_encode_decode = EncodeDecode(encoder_wtoi,encoder_itow,pad_token,unknown_token)\n",
    "decoder_encode_decode = EncodeDecode(decoder_wtoi,decoder_itow,pad_token,unknown_token)\n",
    "print(Xtr_e[data_index],encoder_encode_decode.get_encoding(Xtr_e[data_index]))\n",
    "print(Xtr_d[data_index],decoder_encode_decode.get_encoding(Xtr_d[data_index]))\n",
    "print(Ytr[data_index],decoder_encode_decode.get_encoding(Ytr[data_index]))\n",
    "\n",
    "init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "model_encoder.train()\n",
    "model_decoder.train()\n",
    "\n",
    "optimizer_encoder.zero_grad()\n",
    "optimizer_decoder.zero_grad()\n",
    "Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtr_e[data_index])]).to(device)\n",
    "Xd_b = torch.tensor([decoder_encode_decode.get_encoding(Xtr_d[data_index])]).to(device)\n",
    "Y_b = torch.tensor([decoder_encode_decode.get_encoding(Ytr[data_index])]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "op,_ = model_decoder(Xd_b,ht_for_decoder)\n",
    "ht = ht.detach()\n",
    "loss = loss_fn(op,Y_b.reshape(-1))\n",
    "loss.backward()\n",
    "optimizer_encoder.step()\n",
    "optimizer_decoder.step()\n",
    "print(\"---------------------------------------------\")\n",
    "optimizer_encoder.zero_grad()\n",
    "optimizer_decoder.zero_grad()\n",
    "Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtr_e[data_index+1])]).to(device)\n",
    "Xd_b = torch.tensor([decoder_encode_decode.get_encoding(Xtr_d[data_index+1])]).to(device)\n",
    "Y_b = torch.tensor([decoder_encode_decode.get_encoding(Ytr[data_index+1])]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "ht,ht_for_decoder = model_encoder(Xe_b,ht)\n",
    "op,_ = model_decoder(Xd_b,ht_for_decoder)\n",
    "ht = ht.detach()\n",
    "loss = loss_fn(op,Y_b.reshape(-1))\n",
    "loss.backward()\n",
    "optimizer_encoder.step()\n",
    "optimizer_decoder.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi):\n",
    "    data_index = random.randint(0,100)\n",
    "    Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xval_e[data_index])]).to(device)\n",
    "    print(Xval_e[data_index],Xe_b)\n",
    "    print(Xval_d[data_index])\n",
    "    \n",
    "    model_encoder.eval()\n",
    "    model_decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "        ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "        sos_word = torch.tensor([[decoder_wtoi[\"<sos>\"]]]).to(device)\n",
    "        op,ht = model_decoder(sos_word,ht_for_decoder)\n",
    "        unjumbled_sentence = []\n",
    "        for i in range(25):\n",
    "            predicted_word = torch.argmax(op,axis=1).tolist()\n",
    "#             print(\"Predicted .....................\",predicted_word)\n",
    "            if predicted_word[0] == decoder_wtoi[\"<eos>\"]: break\n",
    "            unjumbled_sentence.append(decoder_itow[predicted_word[0]])\n",
    "            op,ht = model_decoder(torch.tensor([predicted_word]).to(device),ht)\n",
    "        print(\"_______________________________________\")\n",
    "        print(unjumbled_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool spider-man boy a the wearing <unk> at of . edge young the <eos> tensor([[ 193, 1453,   12,   11,    3,  106,    5,    8,    4,    2,  740,   27,\n",
      "            3,    9]], device='cuda:0')\n",
      "<sos> a young boy wearing spider-man <unk> at the edge of the pool .\n",
      "Before starting: x Shape: torch.Size([1, 14]) Prev State Shape torch.Size([4, 1, 400])\n",
      "Embedding, x Shape: torch.Size([1, 14, 1000])\n",
      "GRU, op Shape: torch.Size([1, 14, 800]) ht shape torch.Size([4, 1, 400])\n",
      "ht for decoder shape torch.Size([1, 1, 800])\n",
      "Before starting: x Shape: torch.Size([1, 1]) Prev State Shape torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 1, 1000])\n",
      "GRU, x Shape: torch.Size([1, 1, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping x Shape: torch.Size([1, 800])\n",
      "Linear x Shape: torch.Size([1, 3005])\n",
      "log_softmax op Shape: torch.Size([1, 3005])\n",
      "Before starting: x Shape: torch.Size([1, 1]) Prev State Shape torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 1, 1000])\n",
      "GRU, x Shape: torch.Size([1, 1, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping x Shape: torch.Size([1, 800])\n",
      "Linear x Shape: torch.Size([1, 3005])\n",
      "log_softmax op Shape: torch.Size([1, 3005])\n",
      "Before starting: x Shape: torch.Size([1, 1]) Prev State Shape torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 1, 1000])\n",
      "GRU, x Shape: torch.Size([1, 1, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping x Shape: torch.Size([1, 800])\n",
      "Linear x Shape: torch.Size([1, 3005])\n",
      "log_softmax op Shape: torch.Size([1, 3005])\n",
      "Before starting: x Shape: torch.Size([1, 1]) Prev State Shape torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 1, 1000])\n",
      "GRU, x Shape: torch.Size([1, 1, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping x Shape: torch.Size([1, 800])\n",
      "Linear x Shape: torch.Size([1, 3005])\n",
      "log_softmax op Shape: torch.Size([1, 3005])\n",
      "_______________________________________\n",
      "['the', 'the', '.']\n"
     ]
    }
   ],
   "source": [
    "predict(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "hidden_size_encoder = 300\n",
    "hidden_size_decoder = hidden_size_encoder\n",
    "model_encoder = UnjumbleEncoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=1000,num_lstm_layers=2,\n",
    "    hidden_size=hidden_size_encoder,make_bidirectional=True,debug=False\n",
    ").to(device)\n",
    "if model_encoder.bidirectional: hidden_size_decoder = 2*hidden_size_encoder\n",
    "model_decoder = UnjumbleDecoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=1000,num_lstm_layers=1,\n",
    "    hidden_size=hidden_size_decoder,make_bidirectional=False,debug=False\n",
    ").to(device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer_encoder = torch.optim.Adam(model_encoder.parameters(),lr=0.001)\n",
    "optimizer_decoder = torch.optim.Adam(model_decoder.parameters(),lr=0.001)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 Loss: 2.549741268157959\n",
      "Epoch: 0 Batch: 2000 Loss: 2.512336254119873\n",
      "Epoch: 0 Batch: 4000 Loss: 0.63258957862854\n",
      "Epoch: 0 Batch: 6000 Loss: 2.558344841003418\n",
      "Epoch: 0 Batch: 8000 Loss: 2.824500799179077\n",
      "Epoch: 0 Batch: 10000 Loss: 2.8001487255096436\n",
      "Epoch: 0 Batch: 12000 Loss: 4.142967700958252\n",
      "Epoch: 0 Batch: 14000 Loss: 2.927480459213257\n",
      "Epoch: 0 Batch: 16000 Loss: 3.238931894302368\n",
      "Epoch: 0 Batch: 18000 Loss: 1.7174347639083862\n",
      "Epoch: 0 Batch: 20000 Loss: 2.857111930847168\n",
      "Epoch: 0 Batch: 22000 Loss: 3.4794559478759766\n",
      "Epoch: 0 Batch: 24000 Loss: 2.9830617904663086\n",
      "Epoch: 0 Batch: 26000 Loss: 0.9317401051521301\n",
      "Epoch: 0 Batch: 28000 Loss: 1.9379465579986572\n",
      "Epoch: 0 Batch: 30000 Loss: 1.0846409797668457\n",
      "Epoch: 0 Batch: 32000 Loss: 3.3337976932525635\n",
      "Epoch: 0 Batch: 34000 Loss: 3.112779378890991\n",
      "Epoch: 0 Batch: 36000 Loss: 3.6252198219299316\n",
      "______________________________________\n",
      "Epoch Loss: 91561.87906358391\n",
      "through of very a <unk> ears with brown floppy . grass a field runs dog <eos> tensor([[ 182,    4,  211,   11,    5, 1134,   33,   70, 1928,    2,   72,   11,\n",
      "           99,   81,   55,    9]], device='cuda:0')\n",
      "<sos> a brown dog with very floppy ears runs through a field of <unk> grass .\n",
      "_______________________________________\n",
      "['a', 'brown', 'dog', 'catches', 'a', '<unk>', 'with', 'a', 'blue', 'eyes', '.']\n",
      "_______________________________________\n",
      "Epoch: 1 Batch: 0 Loss: 2.5393526554107666\n",
      "Epoch: 1 Batch: 2000 Loss: 2.187692880630493\n",
      "Epoch: 1 Batch: 4000 Loss: 0.7367360591888428\n",
      "Epoch: 1 Batch: 6000 Loss: 2.403881311416626\n",
      "Epoch: 1 Batch: 8000 Loss: 2.6285202503204346\n",
      "Epoch: 1 Batch: 10000 Loss: 2.8792247772216797\n",
      "Epoch: 1 Batch: 12000 Loss: 3.6674773693084717\n",
      "Epoch: 1 Batch: 14000 Loss: 2.9669623374938965\n",
      "Epoch: 1 Batch: 16000 Loss: 2.893272876739502\n",
      "Epoch: 1 Batch: 18000 Loss: 1.7926586866378784\n",
      "Epoch: 1 Batch: 20000 Loss: 3.080216884613037\n",
      "Epoch: 1 Batch: 22000 Loss: 3.341970920562744\n",
      "Epoch: 1 Batch: 24000 Loss: 2.563547134399414\n",
      "Epoch: 1 Batch: 26000 Loss: 0.7076483368873596\n",
      "Epoch: 1 Batch: 28000 Loss: 2.057419538497925\n",
      "Epoch: 1 Batch: 30000 Loss: 1.0079188346862793\n",
      "Epoch: 1 Batch: 32000 Loss: 2.587061643600464\n",
      "Epoch: 1 Batch: 34000 Loss: 3.1134769916534424\n",
      "Epoch: 1 Batch: 36000 Loss: 3.7344553470611572\n",
      "______________________________________\n",
      "Epoch Loss: 88933.01897200942\n",
      ". snow the skiing is flip doing a in a and person <eos> tensor([[   2,  120,    3,  639,   29, 1611,  430,   11,   13,   11,   61,  124,\n",
      "            9]], device='cuda:0')\n",
      "<sos> a person is skiing and doing a flip in the snow .\n",
      "_______________________________________\n",
      "['a', 'person', 'in', 'the', 'red', 'and', 'blue', 'shorts', 'is', 'riding', 'a', 'vehicle', '.']\n",
      "_______________________________________\n",
      "Epoch: 2 Batch: 0 Loss: 2.537787437438965\n",
      "Epoch: 2 Batch: 2000 Loss: 2.6101934909820557\n",
      "Epoch: 2 Batch: 4000 Loss: 0.41041529178619385\n",
      "Epoch: 2 Batch: 6000 Loss: 2.2740015983581543\n",
      "Epoch: 2 Batch: 8000 Loss: 2.811751127243042\n",
      "Epoch: 2 Batch: 10000 Loss: 2.789557456970215\n",
      "Epoch: 2 Batch: 12000 Loss: 3.378154754638672\n",
      "Epoch: 2 Batch: 14000 Loss: 2.9015557765960693\n",
      "Epoch: 2 Batch: 16000 Loss: 3.122495174407959\n",
      "Epoch: 2 Batch: 18000 Loss: 1.9444395303726196\n",
      "Epoch: 2 Batch: 20000 Loss: 2.613499879837036\n",
      "Epoch: 2 Batch: 22000 Loss: 3.7433626651763916\n",
      "Epoch: 2 Batch: 24000 Loss: 2.3196403980255127\n",
      "Epoch: 2 Batch: 26000 Loss: 0.8370046615600586\n",
      "Epoch: 2 Batch: 28000 Loss: 1.6104367971420288\n",
      "Epoch: 2 Batch: 30000 Loss: 0.8882414102554321\n",
      "Epoch: 2 Batch: 32000 Loss: 3.3370649814605713\n",
      "Epoch: 2 Batch: 34000 Loss: 2.6676862239837646\n",
      "Epoch: 2 Batch: 36000 Loss: 3.7037253379821777\n",
      "______________________________________\n",
      "Epoch Loss: 86972.78068980575\n",
      "colorful is a girl . <unk> holding with clothes colorful <eos> tensor([[178,  29,  11,  24,   2,   5, 166,  33, 860, 178,   9]],\n",
      "       device='cuda:0')\n",
      "<sos> a girl with colorful clothes is holding colorful <unk> .\n",
      "_______________________________________\n",
      "['a', 'girl', 'with', 'no', 'shirt', 'hula', 'hoops', 'is', '<unk>', 'out', '.']\n",
      "_______________________________________\n",
      "Epoch: 3 Batch: 0 Loss: 2.5917532444000244\n",
      "Epoch: 3 Batch: 2000 Loss: 2.1438000202178955\n",
      "Epoch: 3 Batch: 4000 Loss: 0.2959703207015991\n",
      "Epoch: 3 Batch: 6000 Loss: 1.862334132194519\n",
      "Epoch: 3 Batch: 8000 Loss: 2.5157105922698975\n",
      "Epoch: 3 Batch: 10000 Loss: 2.6348366737365723\n",
      "Epoch: 3 Batch: 12000 Loss: 4.15364408493042\n",
      "Epoch: 3 Batch: 14000 Loss: 3.0086441040039062\n",
      "Epoch: 3 Batch: 16000 Loss: 2.8887808322906494\n",
      "Epoch: 3 Batch: 18000 Loss: 1.7298698425292969\n",
      "Epoch: 3 Batch: 20000 Loss: 2.7922465801239014\n",
      "Epoch: 3 Batch: 22000 Loss: 3.7426819801330566\n",
      "Epoch: 3 Batch: 24000 Loss: 2.060817003250122\n",
      "Epoch: 3 Batch: 26000 Loss: 0.6487711071968079\n",
      "Epoch: 3 Batch: 28000 Loss: 1.8387725353240967\n",
      "Epoch: 3 Batch: 30000 Loss: 0.8257727026939392\n",
      "Epoch: 3 Batch: 32000 Loss: 2.5610663890838623\n",
      "Epoch: 3 Batch: 34000 Loss: 2.900620698928833\n",
      "Epoch: 3 Batch: 36000 Loss: 3.601078748703003\n",
      "______________________________________\n",
      "Epoch Loss: 85343.62715474144\n",
      "a run court . inside children <eos> tensor([[ 11, 279, 478,   2, 731, 100,   9]], device='cuda:0')\n",
      "<sos> children run inside a court .\n",
      "_______________________________________\n",
      "['a', 'surfer', 'catches', 'a', 'blue', 'ball', '.']\n",
      "_______________________________________\n",
      "Epoch: 4 Batch: 0 Loss: 2.7663357257843018\n",
      "Epoch: 4 Batch: 2000 Loss: 2.1776068210601807\n",
      "Epoch: 4 Batch: 4000 Loss: 0.509917140007019\n",
      "Epoch: 4 Batch: 6000 Loss: 2.127288341522217\n",
      "Epoch: 4 Batch: 8000 Loss: 2.6526010036468506\n",
      "Epoch: 4 Batch: 10000 Loss: 2.803696393966675\n",
      "Epoch: 4 Batch: 12000 Loss: 3.340100049972534\n",
      "Epoch: 4 Batch: 14000 Loss: 3.2843353748321533\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-a9fdf0e89a71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0moptimizer_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m2000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Batch:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Loss:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "    model_encoder.train()\n",
    "    model_decoder.train()\n",
    "    epoch_loss = 0\n",
    "    for j in range(len(Xtr_e)):\n",
    "        optimizer_encoder.zero_grad()\n",
    "        optimizer_decoder.zero_grad()\n",
    "        Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtr_e[j])]).to(device)\n",
    "        Xd_b = torch.tensor([decoder_encode_decode.get_encoding(Xtr_d[j])]).to(device)\n",
    "        Y_b = torch.tensor([decoder_encode_decode.get_encoding(Ytr[j])]).to(device)\n",
    "        ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "        op,_ = model_decoder(Xd_b,ht_for_decoder)\n",
    "#         ht = ht.detach()\n",
    "#         init_ht_for_encoder = ht\n",
    "        loss = loss_fn(op,Y_b.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer_encoder.step()\n",
    "        optimizer_decoder.step()\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        if j%2000 == 0: print(\"Epoch:\",i,\"Batch:\",j,\"Loss:\",batch_loss)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Epoch Loss:\",epoch_loss)\n",
    "    predict(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "    print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
