{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/Train_400.tsv\",sep=\"\\t\")\n",
    "# test_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/Test_100.tsv\",sep=\"\\t\")\n",
    "# print(train_df.shape, test_df.shape)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32368, 2) (8092, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumbled_sentences</th>\n",
       "      <th>unjumbled_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32760</th>\n",
       "      <td>tools and a man gardening inside two holding a...</td>\n",
       "      <td>a man and two women are inside a greenhouse ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31413</th>\n",
       "      <td>meandering at people of the walkway stand . up...</td>\n",
       "      <td>people stand at the bottom of a meandering wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>standing a rock . on man view the shorts a out...</td>\n",
       "      <td>a man in shorts is standing on a rock looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28232</th>\n",
       "      <td>to a a in on shirt little red a holds pole nea...</td>\n",
       "      <td>a little girl in a red shirt holds on to a pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28438</th>\n",
       "      <td>children two in &lt;unk&gt; play the melting .</td>\n",
       "      <td>two children play in the melting &lt;unk&gt; .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       jumbled_sentences  \\\n",
       "32760  tools and a man gardening inside two holding a...   \n",
       "31413  meandering at people of the walkway stand . up...   \n",
       "4325   standing a rock . on man view the shorts a out...   \n",
       "28232  to a a in on shirt little red a holds pole nea...   \n",
       "28438          children two in <unk> play the melting .    \n",
       "\n",
       "                                     unjumbled_sentences  \n",
       "32760  a man and two women are inside a greenhouse ho...  \n",
       "31413  people stand at the bottom of a meandering wal...  \n",
       "4325   a man in shorts is standing on a rock looking ...  \n",
       "28232  a little girl in a red shirt holds on to a pol...  \n",
       "28438          two children play in the melting <unk> .   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jumbled_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/processed_jumbled.txt\",sep=\"\\t\",header=None)\n",
    "unjumbled_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/processed_unjumbled.txt\",sep=\"\\t\",header=None)\n",
    "jumbled_df.columns = [\"jumbled_sentences\"]\n",
    "unjumbled_df.columns = [\"unjumbled_sentences\"]\n",
    "df = pd.concat([jumbled_df,unjumbled_df],axis=1)\n",
    "train_df = df.sample(frac=0.8, random_state=42) \n",
    "test_df = df.drop(train_df.index)\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Train and Test Data\n",
    "1. Lowercasing, removing stopwords. <br>\n",
    "2. Stemming, Lemmetization. <br>\n",
    "3. Tokenization. <br>\n",
    "4. Here, we are just doing tokenization by splitting on space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenize_on = \" \"\n",
    "    \n",
    "    def tokenize(self,text_string):\n",
    "        '''\n",
    "        text_string = \"This is one sentence.\"\n",
    "        returns token_list = [\"This\",\"is\",\"one\",\"sentence.\"]\n",
    "        '''\n",
    "        token_list = text_string.split(self.tokenize_on)\n",
    "        return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32368, 2) (8092, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumbled_sentences</th>\n",
       "      <th>unjumbled_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32760</th>\n",
       "      <td>[tools, and, a, man, gardening, inside, two, h...</td>\n",
       "      <td>[a, man, and, two, women, are, inside, a, gree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31413</th>\n",
       "      <td>[meandering, at, people, of, the, walkway, sta...</td>\n",
       "      <td>[people, stand, at, the, bottom, of, a, meande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>[standing, a, rock, ., on, man, view, the, sho...</td>\n",
       "      <td>[a, man, in, shorts, is, standing, on, a, rock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28232</th>\n",
       "      <td>[to, a, a, in, on, shirt, little, red, a, hold...</td>\n",
       "      <td>[a, little, girl, in, a, red, shirt, holds, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28438</th>\n",
       "      <td>[children, two, in, &lt;unk&gt;, play, the, melting,...</td>\n",
       "      <td>[two, children, play, in, the, melting, &lt;unk&gt;,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       jumbled_sentences  \\\n",
       "32760  [tools, and, a, man, gardening, inside, two, h...   \n",
       "31413  [meandering, at, people, of, the, walkway, sta...   \n",
       "4325   [standing, a, rock, ., on, man, view, the, sho...   \n",
       "28232  [to, a, a, in, on, shirt, little, red, a, hold...   \n",
       "28438  [children, two, in, <unk>, play, the, melting,...   \n",
       "\n",
       "                                     unjumbled_sentences  \n",
       "32760  [a, man, and, two, women, are, inside, a, gree...  \n",
       "31413  [people, stand, at, the, bottom, of, a, meande...  \n",
       "4325   [a, man, in, shorts, is, standing, on, a, rock...  \n",
       "28232  [a, little, girl, in, a, red, shirt, holds, on...  \n",
       "28438  [two, children, play, in, the, melting, <unk>,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor()\n",
    "train_df[\"jumbled_sentences\"] = train_df[\"jumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "train_df[\"unjumbled_sentences\"] = train_df[\"unjumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "test_df[\"jumbled_sentences\"] = test_df[\"jumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "test_df[\"unjumbled_sentences\"] = test_df[\"unjumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X_Encoder, X_Decoder and Y\n",
    "1. X denotes Input, Y denotes Output. <br>\n",
    "2. X_encoder is the matrix of tokens in jumbled_sentences, each sentence suffixed by \"eos\" token. <br>\n",
    "3. X_decoder is the matrix of tokens in unjumbled_sentences, each sentence prefixed by \"sos\" token. X_decoder is required because we want to do <b>Teacher Forcing</b>, which means we want to provide the correct current token to decoder to predict next token, instead of relying only on its own prediction. <br>\n",
    "4. Y is the matrix of unjumbled_sentences, each sentence suffixed by \"eos\" token. <br>\n",
    "\n",
    "5. Do this for both train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 32368\n",
      "X Encoder test length: 8092\n",
      "Sample X_encoder_train: ['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.', 'women', 'a', 'greenhouse', '', '<eos>']\n",
      "Sample X_decoder_train: ['<sos>', 'a', 'man', 'and', 'two', 'women', 'are', 'inside', 'a', 'greenhouse', 'holding', 'gardening', 'tools', '.', '']\n",
      "Sample Y_train: ['a', 'man', 'and', 'two', 'women', 'are', 'inside', 'a', 'greenhouse', 'holding', 'gardening', 'tools', '.', '', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "def get_Xe_Xd_Y(dataframe, sos_token, eos_token):\n",
    "    jumbled_sentences = dataframe[\"jumbled_sentences\"].tolist()\n",
    "    unjumbled_sentences = dataframe[\"unjumbled_sentences\"].tolist()\n",
    "    X_encoder_tokens = [el + [eos_token] for el in jumbled_sentences]\n",
    "    X_decoder_tokens = [[sos_token] + el for el in unjumbled_sentences]\n",
    "    Y_tokens = [el + [eos_token] for el in unjumbled_sentences]\n",
    "    return X_encoder_tokens, X_decoder_tokens, Y_tokens\n",
    "\n",
    "unknown_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "X_encoder_tokens_tr, X_decoder_tokens_tr, Y_tokens_tr = get_Xe_Xd_Y(train_df, sos_token, eos_token)\n",
    "X_encoder_tokens_test, X_decoder_tokens_test, Y_tokens_test = get_Xe_Xd_Y(test_df, sos_token, eos_token)\n",
    "print(\"X Encoder train length:\",len(X_encoder_tokens_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_tokens_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_tokens_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_tokens_tr[0])\n",
    "print(\"Sample Y_train:\",Y_tokens_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocab\n",
    "1. Generally, Vocab is created from both Encoder and Decoder Tokens, consider Senetence Translation for ex, where encoder and decoder tokens can be in different languages. <br>\n",
    "2. We can create Vocab separately for Encder and Decoder tokens, or can create shared vocab. Shared Vocab is preferable though. <br>\n",
    "3. Also, Vocab is generated from only Training Data. <br>\n",
    "4. In this case, we are using only Encoder tokens to create Vocab because Decoder Tokens are the same. Also, we are using both Train and Test Dataset to create Vocab, as our datasize is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabBuilder:\n",
    "    def __init__(self,token_corpus,unknown_token=None,pad_token=None,sos_token=None,eos_token=None):\n",
    "        '''\n",
    "        token_corpus = ['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.']\n",
    "        '''\n",
    "        self.token_corpus = token_corpus\n",
    "        self.unknown_token = unknown_token or \"<unk>\"\n",
    "        self.pad_token = pad_token or \"<pad>\"\n",
    "        self.sos_token = sos_token or \"<sos>\"\n",
    "        self.eos_token = eos_token or \"<eos>\"\n",
    "        self.word_to_index, self.index_to_word = self.get_vocabs()\n",
    "                        \n",
    "    def get_vocabs(self):\n",
    "        word_to_index = {}\n",
    "        index_count = 0\n",
    "        all_unique_words = set(self.token_corpus).difference(set(\n",
    "            [self.unknown_token, self.pad_token, self.sos_token, self.eos_token]\n",
    "        ))\n",
    "        word_to_index[self.unknown_token] = 0\n",
    "        word_to_index[self.pad_token] = 1\n",
    "        word_to_index[self.sos_token] = 2\n",
    "        word_to_index[self.eos_token] = 3\n",
    "        \n",
    "        for index, word in enumerate(all_unique_words):\n",
    "            word_to_index[word] = index + 4\n",
    "        if self.pad_token not in word_to_index: word_to_index[self.pad_token] = index + 1\n",
    "        if self.sos_token not in word_to_index: word_to_index[self.sos_token] = index + 2\n",
    "        if self.eos_token not in word_to_index: word_to_index[self.eos_token] = index + 3\n",
    "        if self.unknown_token not in word_to_index: word_to_index[self.unknown_token] = index + 4\n",
    "        index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "        return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.', 'women', 'a', 'greenhouse', '', '<eos>', 'meandering', 'at', 'people', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "token_corpus_1 = list(chain.from_iterable(X_encoder_tokens_tr)) # flattens a 2D list ot 1D\n",
    "token_corpus_2 = list(chain.from_iterable(X_encoder_tokens_test))  # flattens a 2D list ot 1D\n",
    "token_corpus = token_corpus_1 + token_corpus_2\n",
    "print(token_corpus[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordToIndex Dict length: 5242\n",
      "IndexToWord Dict length: 5242\n"
     ]
    }
   ],
   "source": [
    "vocab_builder = VocabBuilder(token_corpus,unknown_token,pad_token,sos_token,eos_token)\n",
    "print(\"WordToIndex Dict length:\",len(vocab_builder.word_to_index))\n",
    "print(\"IndexToWord Dict length:\",len(vocab_builder.index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map X_encoder, X_decoder and Y using Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_Index_Mapper:\n",
    "    def __init__(self,token_to_index,index_to_token, unknown_token):\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = index_to_token\n",
    "        self.unknown_token = unknown_token\n",
    "    \n",
    "    def get_encoding(self,sentence):\n",
    "        '''\n",
    "        sentence must be a list of tokens.\n",
    "        Ex: [\"Climate\",\"change\",\"is\",\"a\",\"pressing\",\"global\",\"issue\"]\n",
    "        '''\n",
    "        encoded_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in self.token_to_index: encoded_sentence.append(self.token_to_index[token])\n",
    "            else: encoded_sentence.append(self.token_to_index[self.unknown_token])\n",
    "        return encoded_sentence\n",
    "    \n",
    "    def get_decoding(self,encoded_sentence):\n",
    "        '''\n",
    "        encoded_sentence must be a list of vocab indices.\n",
    "        Ex: encoded_sentence = [24,21,4,1,..] \n",
    "        '''\n",
    "        sentence = [self.index_to_token[index] for index in encoded_sentence]\n",
    "        return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens_to_indices(token_index_mapper, max_sequence_length, token_matrix):\n",
    "    index_matrix = []\n",
    "    for el in token_matrix:\n",
    "        el = el[:max_sequence_length] # truncate sentence to max_seq_length\n",
    "        if len(el) < max_sequence_length:\n",
    "            pad_tokens_to_append = max_sequence_length - len(el)\n",
    "            el = el + [pad_token]*pad_tokens_to_append\n",
    "        index_matrix.append(token_index_mapper.get_encoding(el))\n",
    "    return index_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 32368\n",
      "X Encoder test length: 8092\n",
      "Sample X_encoder_train: [986, 4094, 4391, 3020, 4636, 5047, 2764, 2011, 5124, 4821, 2055, 4391, 4671, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample X_decoder_train: [2, 4391, 3020, 4094, 2764, 2055, 5124, 5047, 4391, 4671, 2011, 4636, 986, 4821, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample Y_train: [4391, 3020, 4094, 2764, 2055, 5124, 5047, 4391, 4671, 2011, 4636, 986, 4821, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 25\n",
    "token_index_mapper = Token_Index_Mapper(vocab_builder.word_to_index, vocab_builder.index_to_word, unknown_token)\n",
    "X_encoder_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_encoder_tokens_tr)\n",
    "X_decoder_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_decoder_tokens_tr)\n",
    "Y_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, Y_tokens_tr)\n",
    "\n",
    "X_encoder_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_encoder_tokens_test)\n",
    "X_decoder_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_decoder_tokens_test)\n",
    "Y_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, Y_tokens_test)\n",
    "print(\"X Encoder train length:\",len(X_encoder_indices_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_indices_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_indices_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_indices_tr[0])\n",
    "print(\"Sample Y_train:\",Y_indices_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model \n",
    "1. Encoder\n",
    "2. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingClass(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_token_length):\n",
    "        super(EmbeddingClass, self).__init__()\n",
    "        self.max_token_length = max_token_length\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_token_length, embedding_dim)\n",
    "    \n",
    "    def forward(self,token_ids, debug = False):\n",
    "        # create position ids\n",
    "        batch_size = token_ids.shape[0]\n",
    "        position_id = range(0,self.max_token_length)\n",
    "        position_ids = torch.tensor([position_id]*batch_size)\n",
    "        \n",
    "        token_embeddings = self.tok_embedding(token_ids)\n",
    "        position_embeddings = self.pos_embedding(position_ids)\n",
    "        \n",
    "        embbeded_x = token_embeddings + position_embeddings\n",
    "        \n",
    "        if debug:\n",
    "            print(\"----------Embedding Class------------\")\n",
    "            print(\"token_ids shape:\",token_ids.shape)\n",
    "            print(\"batch_size:\", batch_size)\n",
    "            print(\"position_ids shape:\", position_ids.shape)\n",
    "            print(\"token_embeddings shape:\", token_embeddings.shape)\n",
    "            print(\"position_embeddings shape:\", position_embeddings.shape)\n",
    "            print(\"embbeded_x shape:\", embbeded_x.shape)\n",
    "        \n",
    "        return embbeded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.W_K = nn.Linear(embedding_dim, head_dim)\n",
    "        self.W_Q = nn.Linear(embedding_dim, head_dim)\n",
    "        self.W_V = nn.Linear(embedding_dim, head_dim)\n",
    "    \n",
    "    def forward(self, embedded_x1, embedded_x2, mask_matrix = None, debug = False):\n",
    "        # Batch_Size = B, Seq_Length = L, Embedding_Dim = D, Head_Dim = H\n",
    "        # embedded_x shape = (B X L X D)\n",
    "        q = self.W_Q(embedded_x1) # q shape = (B X L X H)\n",
    "        k = self.W_K(embedded_x2) # k shape = (B X L X H)\n",
    "        v = self.W_V(embedded_x2) # v shape = (B X L X H)\n",
    "        \n",
    "        scores = torch.bmm(q, k.transpose(1,2))/math.sqrt(self.head_dim)\n",
    "        # scores shape = (B X L X L)\n",
    "        # torch.bmm() does the batch matrix multiplication:\n",
    "        # input_1 = (b X n X m), input_2 = (b X m X p)\n",
    "        # op = torch.zeros(b X n X p)\n",
    "        # for each matrix (n X m) in input_1 and (m X p) in input_2, \n",
    "        # torch.bmm() will output an (n X p) matrix. \n",
    "        # This will be done 'b' times (i.e for all b matrices)\n",
    "        \n",
    "        if mask_matrix is not None and mask_matrix.shape[0] > 0:\n",
    "            scores = scores.masked_fill(mask_matrix==0, float(\"-inf\"))\n",
    "        weights = torch.softmax(scores, dim=2) # (B X L X L)\n",
    "        output = torch.bmm(weights, v) # (B X L X H)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"----------Self-Attention------------\")\n",
    "            print(\"q shape:\", q.shape)\n",
    "            print(\"k shape:\", k.shape)\n",
    "            print(\"v shape:\", v.shape)\n",
    "            if mask_matrix is not None and mask_matrix.shape[0] > 0: print(\"mask_matrix shape:\", mask_matrix.shape)\n",
    "            print(\"scores shape:\", scores.shape)\n",
    "            print(\"weights shape:\", weights.shape)\n",
    "            print(\"output shape:\", output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.sa_list = nn.ModuleList([\n",
    "           SelfAttention(embedding_dim, self.head_dim) \n",
    "            for _ in range(self.num_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, embedded_x1, embedded_x2, mask_matrix = None, debug = False):\n",
    "        multiple_self_attn_op = [\n",
    "            self_attn_head(embedded_x1, embedded_x2, mask_matrix, debug) \n",
    "            for self_attn_head in self.sa_list\n",
    "        ]\n",
    "        concatenated_op = torch.cat(multiple_self_attn_op,axis=2)\n",
    "        linear_op = self.linear(concatenated_op)\n",
    "\n",
    "        if debug:\n",
    "            print(\"----------MHA------------\")\n",
    "            print(\"embedded_x1 shape:\", embedded_x1.shape)\n",
    "            print(\"embedded_x2 shape:\", embedded_x2.shape)\n",
    "            print(\"multiple_self_attn_op len:\", len(multiple_self_attn_op))\n",
    "            print(\"concatenated_op shape:\", concatenated_op.shape)\n",
    "            print(\"linear_op shape:\", linear_op.shape)\n",
    "        return linear_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embedding_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(embedding_dim, 4*embedding_dim)\n",
    "        self.linear_2 = nn.Linear(4*embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self,mha_op, debug=False):\n",
    "        linear_1_op = self.linear_1(mha_op)\n",
    "        linear_1_op = self.gelu(linear_1_op)\n",
    "        linear_2_op = self.linear_2(linear_1_op)\n",
    "        linear_2_op = self.dropout(linear_2_op)\n",
    "        if debug:\n",
    "            print(\"----------Feed Forward------------\")\n",
    "            print(\"mha_op shape:\", mha_op.shape)\n",
    "            print(\"linear_1_op shape:\", linear_1_op.shape)\n",
    "            print(\"linear_2_op shape:\", linear_2_op.shape)\n",
    "        return linear_2_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.ff_layer = FeedForward(embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, embedded_x, mask_matrix, debug=False):\n",
    "        mha_op = self.mha(embedded_x, embedded_x, mask_matrix, debug)\n",
    "        layer_norm_op_1 = self.layer_norm(mha_op + embedded_x)\n",
    "        ff_op = self.ff_layer(layer_norm_op_1, debug)\n",
    "        layer_norm_op_2 = self.layer_norm(ff_op + layer_norm_op_1)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"----------EncoderBlock------------\")\n",
    "            print(\"embedded_x shape:\", embedded_x.shape)\n",
    "            print(\"mha_op shape:\", mha_op.shape)\n",
    "            print(\"layer_norm_op_1 shape:\", layer_norm_op_1.shape)\n",
    "            print(\"ff_op shape:\", ff_op.shape)\n",
    "            print(\"layer_norm_op_2 shape:\", layer_norm_op_2.shape)\n",
    "        return layer_norm_op_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_token_length, num_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_obj = EmbeddingClass(\n",
    "            vocab_size, embedding_dim, max_token_length\n",
    "        )\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            EncoderBlock(embedding_dim, num_heads) for _ in range(6)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, token_ids, debug=False):\n",
    "        embedded_x = self.embedding_obj(token_ids, debug)\n",
    "        for i,block in enumerate(self.encoder_blocks):\n",
    "            if i == 0: block_op  = block(embedded_x, None, debug)\n",
    "            else:\n",
    "                block_op  = block(block_op, None, debug)\n",
    "        if debug:\n",
    "            print(\"----------Encoder------------\")\n",
    "            print(\"token_ids shape:\", token_ids.shape)\n",
    "            print(\"block_op shape:\", block_op.shape)\n",
    "        return block_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.ff_layer = FeedForward(embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, embedded_x, encoder_op, mask_matrix, debug=False):\n",
    "        masked_mha_op = self.mha(embedded_x, embedded_x, mask_matrix, debug)\n",
    "        layer_norm_op_1 = self.layer_norm(masked_mha_op + embedded_x)\n",
    "        \n",
    "        mha_cross_attn_op = self.mha(layer_norm_op_1, encoder_op, None, debug)\n",
    "        layer_norm_op_2 = self.layer_norm(mha_cross_attn_op + layer_norm_op_1)\n",
    "        \n",
    "        ff_op = self.ff_layer(layer_norm_op_2, debug)\n",
    "        layer_norm_op_3 = self.layer_norm(ff_op + layer_norm_op_2)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"----------DecoderBlock------------\")\n",
    "            print(\"embedded_x shape:\", embedded_x.shape)\n",
    "            print(\"encoder_op shape:\", encoder_op.shape)\n",
    "            print(\"masked_mha_op shape:\", masked_mha_op.shape)\n",
    "            print(\"layer_norm_op_1 shape:\", layer_norm_op_1.shape)\n",
    "            print(\"mha_cross_attn_op shape:\", mha_cross_attn_op.shape)\n",
    "            print(\"layer_norm_op_2 shape:\", layer_norm_op_2.shape)\n",
    "            print(\"ff_op shape:\", ff_op.shape)\n",
    "            print(\"layer_norm_op_3 shape:\", layer_norm_op_3.shape)\n",
    "        return layer_norm_op_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_token_length, num_heads):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_obj = EmbeddingClass(\n",
    "            vocab_size, embedding_dim, max_token_length\n",
    "        )\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(embedding_dim, num_heads) for _ in range(6)\n",
    "        ])\n",
    "        self.mask_matrix = torch.tril(\n",
    "            torch.ones(max_token_length, max_token_length)\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, token_ids, encoder_op, debug=False):\n",
    "        embedded_x = self.embedding_obj(token_ids, debug)\n",
    "        for i,block in enumerate(self.encoder_blocks):\n",
    "            if i == 0: block_op  = block(\n",
    "                embedded_x, encoder_op, self.mask_matrix, debug\n",
    "            )\n",
    "            else:\n",
    "                block_op  = block(\n",
    "                    block_op, encoder_op, self.mask_matrix, debug\n",
    "                )\n",
    "        reshaped_output = block_op.reshape(\n",
    "            -1,block_op.shape[2]\n",
    "        )\n",
    "        linear_op = self.linear(reshaped_output)\n",
    "        if debug:\n",
    "            print(\"----------Decoder------------\")\n",
    "            print(\"token_ids shape:\", token_ids.shape)\n",
    "            print(\"block_op shape:\", block_op.shape)\n",
    "            print(\"reshaped_output shape:\", reshaped_output.shape)\n",
    "            print(\"linear_op shape:\", linear_op.shape)\n",
    "        return linear_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__() \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input, debug=False): \n",
    "        encoder_outputs = self.encoder(encoder_input, debug) \n",
    "        decoder_outputs = self.decoder(decoder_input, encoder_outputs, debug)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"----------Seq2Seq------------\")\n",
    "            print(\"encoder_outputs shape:\", encoder_outputs.shape)\n",
    "            print(\"decoder_outputs shape:\", decoder_outputs.shape)\n",
    "            \n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length):\n",
    "    model.eval()\n",
    "    sos_token = [token_index_mapper.token_to_index[\"<sos>\"]] # 1st token as decoder input is <sos>.\n",
    "    unjumbled_sentence = []\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encoder(Xe_b)\n",
    "        for i in range(max_sequence_length):\n",
    "            if i == 0:\n",
    "                decoder_input = sos_token*max_sequence_length\n",
    "                decoder_op = model.decoder(torch.tensor([decoder_input]).to(device), encoder_outputs)\n",
    "            else: \n",
    "                # After 1st time step, input to decoder is the predicted token of previous time step.\n",
    "                # and hidden state input to decoder is the hidden state output of decoder of previous time step.\n",
    "                \n",
    "                # To get the predicted token of previous time step:\n",
    "                # first, do the softmax on decoder_op of previous time step\n",
    "                softmax_op = torch.softmax(decoder_op,axis=1) # decoder_op is (1 X Vocab_Size),\n",
    "#                 print(\"Softmax op shape:\",softmax_op.shape)\n",
    "                # next, take the token with max probability\n",
    "                # (softmax_op is also [1 X Vocab_Size], as we have taken softmax along axis=1, which\n",
    "                # has simply converted the logits to probabilities.)\n",
    "                # torch.argmax() returns a tensor([]). The list will contain as many elements as 0th dimension of softmax_op.\n",
    "                # because we are taking argmax along axis = 1.\n",
    "                # In this case, softmax_op has only 1 token in 0th dimension, so the list has only 1 element.\n",
    "                # torch.tensor([]).tolist() gives out the []\n",
    "                predicted_token = torch.argmax(softmax_op,axis=1).tolist()\n",
    "                decoder_input[i] = predicted_token[i-1]\n",
    "                decoder_op = model.decoder(\n",
    "                    torch.tensor([decoder_input]).to(device), encoder_outputs\n",
    "                )\n",
    "                unjumbled_sentence.append(token_index_mapper.index_to_token[predicted_token[i-1]])\n",
    "                if predicted_token[i-1] == token_index_mapper.token_to_index[\"<eos>\"]: break\n",
    "        print(\"_______________________________________\")\n",
    "        print(unjumbled_sentence)\n",
    "        return unjumbled_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Training And Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(token_index_mapper.token_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(token_index_mapper.token_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 512 \n",
    "DEC_EMB_DIM = 512 \n",
    "NUM_HEADS = 8\n",
    "device = \"cpu\"\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, max_sequence_length, NUM_HEADS) \n",
    "dec = Decoder(INPUT_DIM, DEC_EMB_DIM, max_sequence_length, NUM_HEADS) \n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters()) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_index_mapper.token_to_index[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 25]) torch.Size([5, 25]) torch.Size([5, 25])\n",
      "----------Embedding Class------------\n",
      "token_ids shape: torch.Size([5, 25])\n",
      "batch_size: 5\n",
      "position_ids shape: torch.Size([5, 25])\n",
      "token_embeddings shape: torch.Size([5, 25, 512])\n",
      "position_embeddings shape: torch.Size([5, 25, 512])\n",
      "embbeded_x shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "----------Encoder------------\n",
      "token_ids shape: torch.Size([5, 25])\n",
      "block_op shape: torch.Size([5, 25, 512])\n",
      "----------Embedding Class------------\n",
      "token_ids shape: torch.Size([5, 25])\n",
      "batch_size: 5\n",
      "position_ids shape: torch.Size([5, 25])\n",
      "token_embeddings shape: torch.Size([5, 25, 512])\n",
      "position_embeddings shape: torch.Size([5, 25, 512])\n",
      "embbeded_x shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "encoder_op shape: torch.Size([5, 25, 512])\n",
      "masked_mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "encoder_op shape: torch.Size([5, 25, 512])\n",
      "masked_mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "encoder_op shape: torch.Size([5, 25, 512])\n",
      "masked_mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "encoder_op shape: torch.Size([5, 25, 512])\n",
      "masked_mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "encoder_op shape: torch.Size([5, 25, 512])\n",
      "masked_mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([5, 25, 64])\n",
      "k shape: torch.Size([5, 25, 64])\n",
      "v shape: torch.Size([5, 25, 64])\n",
      "scores shape: torch.Size([5, 25, 25])\n",
      "weights shape: torch.Size([5, 25, 25])\n",
      "output shape: torch.Size([5, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([5, 25, 512])\n",
      "embedded_x2 shape: torch.Size([5, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([5, 25, 512])\n",
      "linear_op shape: torch.Size([5, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([5, 25, 512])\n",
      "linear_1_op shape: torch.Size([5, 25, 2048])\n",
      "linear_2_op shape: torch.Size([5, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([5, 25, 512])\n",
      "encoder_op shape: torch.Size([5, 25, 512])\n",
      "masked_mha_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([5, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([5, 25, 512])\n",
      "ff_op shape: torch.Size([5, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([5, 25, 512])\n",
      "----------Decoder------------\n",
      "token_ids shape: torch.Size([5, 25])\n",
      "block_op shape: torch.Size([5, 25, 512])\n",
      "reshaped_output shape: torch.Size([125, 512])\n",
      "linear_op shape: torch.Size([125, 5242])\n",
      "----------Seq2Seq------------\n",
      "encoder_outputs shape: torch.Size([5, 25, 512])\n",
      "decoder_outputs shape: torch.Size([125, 5242])\n"
     ]
    }
   ],
   "source": [
    "data_index = 6\n",
    "batch_size = 5\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "Xe_b = torch.tensor(X_encoder_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Xd_b = torch.tensor(X_decoder_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Y_b = torch.tensor(Y_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "output = model(Xe_b, Xd_b, debug=True)\n",
    "loss = criterion(output, Y_b.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Jumbled sentence: ['dog', 'out', 'the', 'in', 'is', 'brown', 'the', '.', 'snow', '', '<eos>']\n",
      "Test Unjumbled sentence: ['the', 'brown', 'dog', 'is', 'out', 'in', 'the', 'snow', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['the', 'brown', 'dog', 'is', 'out', 'in', 'the', 'snow', '.', '', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# Randomly select one sentence from Test Data to Predict.\n",
    "data_index = random.randint(0,100)\n",
    "# Since its only 1 sentence, we need to convert into a 2-D list before sending it to torch.tensor()\n",
    "Xe_b = torch.tensor([X_encoder_indices_test[data_index]]).to(device)\n",
    "print(\"Test Jumbled sentence:\",X_encoder_tokens_test[data_index])\n",
    "print(\"Test Unjumbled sentence:\", Y_tokens_test[data_index]) \n",
    "unjumbled_sentence = predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" #torch.device(\"cuda:0\")\n",
    "batch_size = 50\n",
    "INPUT_DIM = len(token_index_mapper.token_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(token_index_mapper.token_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 512 \n",
    "DEC_EMB_DIM = 512 \n",
    "NUM_HEADS = 8\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, max_sequence_length, NUM_HEADS) \n",
    "dec = Decoder(INPUT_DIM, DEC_EMB_DIM, max_sequence_length, NUM_HEADS) \n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_index_mapper.token_to_index[\"<pad>\"])\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 Loss: 0.6898346543312073\n",
      "Epoch: 0 Batch: 500 Loss: 0.7354006171226501\n",
      "Epoch: 0 Batch: 1000 Loss: 0.700539231300354\n",
      "Epoch: 0 Batch: 1500 Loss: 0.6414951086044312\n",
      "Epoch: 0 Batch: 2000 Loss: 0.7601133584976196\n",
      "Epoch: 0 Batch: 2500 Loss: 0.6712945103645325\n",
      "Epoch: 0 Batch: 3000 Loss: 0.6413605809211731\n",
      "Epoch: 0 Batch: 3500 Loss: 0.6541795134544373\n",
      "Epoch: 0 Batch: 4000 Loss: 0.6091183423995972\n",
      "Epoch: 0 Batch: 4500 Loss: 0.6493122577667236\n",
      "Epoch: 0 Batch: 5000 Loss: 0.4903520345687866\n",
      "Epoch: 0 Batch: 5500 Loss: 0.594270646572113\n",
      "Epoch: 0 Batch: 6000 Loss: 0.7431533932685852\n",
      "Epoch: 0 Batch: 6500 Loss: 0.6674348711967468\n",
      "Epoch: 0 Batch: 7000 Loss: 0.7684808373451233\n",
      "Epoch: 0 Batch: 7500 Loss: 0.4872722327709198\n",
      "Epoch: 0 Batch: 8000 Loss: 0.5335842967033386\n",
      "Epoch: 0 Batch: 8500 Loss: 0.6324428915977478\n",
      "Epoch: 0 Batch: 9000 Loss: 0.6997290253639221\n",
      "Epoch: 0 Batch: 9500 Loss: 0.6009303331375122\n",
      "Epoch: 0 Batch: 10000 Loss: 0.9210977554321289\n",
      "Epoch: 0 Batch: 10500 Loss: 0.7057729959487915\n",
      "Epoch: 0 Batch: 11000 Loss: 0.5714663863182068\n",
      "Epoch: 0 Batch: 11500 Loss: 0.6415433883666992\n",
      "Epoch: 0 Batch: 12000 Loss: 0.6833102107048035\n",
      "Epoch: 0 Batch: 12500 Loss: 0.5425558686256409\n",
      "Epoch: 0 Batch: 13000 Loss: 0.6310532689094543\n",
      "Epoch: 0 Batch: 13500 Loss: 0.6704651117324829\n",
      "Epoch: 0 Batch: 14000 Loss: 0.6079528331756592\n",
      "Epoch: 0 Batch: 14500 Loss: 0.5791995525360107\n",
      "Epoch: 0 Batch: 15000 Loss: 0.5133935213088989\n",
      "Epoch: 0 Batch: 15500 Loss: 0.4698745608329773\n",
      "Epoch: 0 Batch: 16000 Loss: 0.65632563829422\n",
      "Epoch: 0 Batch: 16500 Loss: 0.6686804294586182\n",
      "Epoch: 0 Batch: 17000 Loss: 0.4895760118961334\n",
      "Epoch: 0 Batch: 17500 Loss: 0.5272055268287659\n",
      "Epoch: 0 Batch: 18000 Loss: 0.7096786499023438\n",
      "Epoch: 0 Batch: 18500 Loss: 0.5270020365715027\n",
      "Epoch: 0 Batch: 19000 Loss: 0.5195156335830688\n",
      "Epoch: 0 Batch: 19500 Loss: 0.5839247107505798\n",
      "Epoch: 0 Batch: 20000 Loss: 0.45005717873573303\n",
      "Epoch: 0 Batch: 20500 Loss: 0.5225813984870911\n",
      "Epoch: 0 Batch: 21000 Loss: 0.5833311080932617\n",
      "Epoch: 0 Batch: 21500 Loss: 0.5178894996643066\n",
      "Epoch: 0 Batch: 22000 Loss: 0.4988008439540863\n",
      "Epoch: 0 Batch: 22500 Loss: 0.608997642993927\n",
      "Epoch: 0 Batch: 23000 Loss: 0.496738463640213\n",
      "Epoch: 0 Batch: 23500 Loss: 0.42836377024650574\n",
      "Epoch: 0 Batch: 24000 Loss: 0.6004123091697693\n",
      "Epoch: 0 Batch: 24500 Loss: 0.5401493906974792\n",
      "Epoch: 0 Batch: 25000 Loss: 0.5711236596107483\n",
      "Epoch: 0 Batch: 25500 Loss: 0.5209621787071228\n",
      "Epoch: 0 Batch: 26000 Loss: 0.6471483707427979\n",
      "Epoch: 0 Batch: 26500 Loss: 0.5771242380142212\n",
      "Epoch: 0 Batch: 27000 Loss: 0.6221010088920593\n",
      "Epoch: 0 Batch: 27500 Loss: 0.5463320016860962\n",
      "Epoch: 0 Batch: 28000 Loss: 0.4893827438354492\n",
      "Epoch: 0 Batch: 28500 Loss: 0.4941915273666382\n",
      "Epoch: 0 Batch: 29000 Loss: 0.605263888835907\n",
      "Epoch: 0 Batch: 29500 Loss: 0.36207252740859985\n",
      "Epoch: 0 Batch: 30000 Loss: 0.5184421539306641\n",
      "Epoch: 0 Batch: 30500 Loss: 0.5551115870475769\n",
      "Epoch: 0 Batch: 31000 Loss: 0.4274282455444336\n",
      "Epoch: 0 Batch: 31500 Loss: 0.5054249167442322\n",
      "Epoch: 0 Batch: 32000 Loss: 0.44528529047966003\n",
      "______________________________________\n",
      "Epoch Loss: 385.2320253252983\n",
      "Test Jumbled sentence: ['the', 'around', 'road', 'a', '.', 'biker', 'the', 'in', 'is', 'riding', 'curve', '', '<eos>']\n",
      "Test Unjumbled sentence: ['the', 'biker', 'is', 'riding', 'around', 'a', 'curve', 'in', 'the', 'road', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['the', 'biker', 'is', 'riding', 'around', 'in', 'the', 'curve', 'a', 'curve', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 1 Batch: 0 Loss: 0.49631837010383606\n",
      "Epoch: 1 Batch: 500 Loss: 0.5212770104408264\n",
      "Epoch: 1 Batch: 1000 Loss: 0.5161705613136292\n",
      "Epoch: 1 Batch: 1500 Loss: 0.4791542887687683\n",
      "Epoch: 1 Batch: 2000 Loss: 0.5219922065734863\n",
      "Epoch: 1 Batch: 2500 Loss: 0.5476402044296265\n",
      "Epoch: 1 Batch: 3000 Loss: 0.4511462152004242\n",
      "Epoch: 1 Batch: 3500 Loss: 0.4882478713989258\n",
      "Epoch: 1 Batch: 4000 Loss: 0.47109702229499817\n",
      "Epoch: 1 Batch: 4500 Loss: 0.49453386664390564\n",
      "Epoch: 1 Batch: 5000 Loss: 0.34121373295783997\n",
      "Epoch: 1 Batch: 5500 Loss: 0.46650487184524536\n",
      "Epoch: 1 Batch: 6000 Loss: 0.5423616766929626\n",
      "Epoch: 1 Batch: 6500 Loss: 0.5081315040588379\n",
      "Epoch: 1 Batch: 7000 Loss: 0.5200707912445068\n",
      "Epoch: 1 Batch: 7500 Loss: 0.35310372710227966\n",
      "Epoch: 1 Batch: 8000 Loss: 0.35412073135375977\n",
      "Epoch: 1 Batch: 8500 Loss: 0.47253307700157166\n",
      "Epoch: 1 Batch: 9000 Loss: 0.5105013251304626\n",
      "Epoch: 1 Batch: 9500 Loss: 0.4509846866130829\n",
      "Epoch: 1 Batch: 10000 Loss: 0.6280660033226013\n",
      "Epoch: 1 Batch: 10500 Loss: 0.5025448203086853\n",
      "Epoch: 1 Batch: 11000 Loss: 0.3570545017719269\n",
      "Epoch: 1 Batch: 11500 Loss: 0.4796880781650543\n",
      "Epoch: 1 Batch: 12000 Loss: 0.4905586838722229\n",
      "Epoch: 1 Batch: 12500 Loss: 0.3790830969810486\n",
      "Epoch: 1 Batch: 13000 Loss: 0.4383743703365326\n",
      "Epoch: 1 Batch: 13500 Loss: 0.4885751008987427\n",
      "Epoch: 1 Batch: 14000 Loss: 0.41222602128982544\n",
      "Epoch: 1 Batch: 14500 Loss: 0.4126594662666321\n",
      "Epoch: 1 Batch: 15000 Loss: 0.40217816829681396\n",
      "Epoch: 1 Batch: 15500 Loss: 0.30553632974624634\n",
      "Epoch: 1 Batch: 16000 Loss: 0.4368232190608978\n",
      "Epoch: 1 Batch: 16500 Loss: 0.48424023389816284\n",
      "Epoch: 1 Batch: 17000 Loss: 0.33283379673957825\n",
      "Epoch: 1 Batch: 17500 Loss: 0.3346138000488281\n",
      "Epoch: 1 Batch: 18000 Loss: 0.5153332352638245\n",
      "Epoch: 1 Batch: 18500 Loss: 0.34419968724250793\n",
      "Epoch: 1 Batch: 19000 Loss: 0.3942089378833771\n",
      "Epoch: 1 Batch: 19500 Loss: 0.38474610447883606\n",
      "Epoch: 1 Batch: 20000 Loss: 0.3383443355560303\n",
      "Epoch: 1 Batch: 20500 Loss: 0.4166715145111084\n",
      "Epoch: 1 Batch: 21000 Loss: 0.40108707547187805\n",
      "Epoch: 1 Batch: 21500 Loss: 0.39123767614364624\n",
      "Epoch: 1 Batch: 22000 Loss: 0.37912923097610474\n",
      "Epoch: 1 Batch: 22500 Loss: 0.4227166473865509\n",
      "Epoch: 1 Batch: 23000 Loss: 0.4280310869216919\n",
      "Epoch: 1 Batch: 23500 Loss: 0.283886581659317\n",
      "Epoch: 1 Batch: 24000 Loss: 0.4510435461997986\n",
      "Epoch: 1 Batch: 24500 Loss: 0.3851413428783417\n",
      "Epoch: 1 Batch: 25000 Loss: 0.406894326210022\n",
      "Epoch: 1 Batch: 25500 Loss: 0.4150540828704834\n",
      "Epoch: 1 Batch: 26000 Loss: 0.5010527968406677\n",
      "Epoch: 1 Batch: 26500 Loss: 0.44198083877563477\n",
      "Epoch: 1 Batch: 27000 Loss: 0.4809196889400482\n",
      "Epoch: 1 Batch: 27500 Loss: 0.3836359977722168\n",
      "Epoch: 1 Batch: 28000 Loss: 0.3820289075374603\n",
      "Epoch: 1 Batch: 28500 Loss: 0.4439489245414734\n",
      "Epoch: 1 Batch: 29000 Loss: 0.4931706488132477\n",
      "Epoch: 1 Batch: 29500 Loss: 0.23705023527145386\n",
      "Epoch: 1 Batch: 30000 Loss: 0.3682745099067688\n",
      "Epoch: 1 Batch: 30500 Loss: 0.4511514902114868\n",
      "Epoch: 1 Batch: 31000 Loss: 0.3169153928756714\n",
      "Epoch: 1 Batch: 31500 Loss: 0.354500412940979\n",
      "Epoch: 1 Batch: 32000 Loss: 0.30079200863838196\n",
      "______________________________________\n",
      "Epoch Loss: 277.78060960769653\n",
      "Test Jumbled sentence: ['a', 'dog', 'white', 'grassy', 'to', 'in', 'trying', '.', 'over', 'is', 'ball', 'catch', 'field', 'midair', 'a', 'a', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'white', 'dog', 'is', 'trying', 'to', 'catch', 'a', 'ball', 'in', 'midair', 'over', 'a', 'grassy', 'field', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'white', 'dog', 'is', 'in', 'midair', 'trying', 'to', 'catch', 'a', 'ball', 'in', 'a', 'grassy', 'field', '.', '', '<eos>']\n",
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for j in range(0,len(X_encoder_indices_tr),batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        Xe_b = torch.tensor(X_encoder_indices_tr[j:j+batch_size]).to(device)\n",
    "        Xd_b = torch.tensor(X_decoder_indices_tr[j:j+batch_size]).to(device)\n",
    "        Y_b = torch.tensor(Y_indices_tr[j:j+batch_size]).to(device)\n",
    "        op = model(Xe_b,Xd_b)\n",
    "        loss = criterion(op,Y_b.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        if j%500 == 0: print(\"Epoch:\",i,\"Batch:\",j,\"Loss:\",batch_loss)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Epoch Loss:\",epoch_loss)\n",
    "    \n",
    "    # Randomly select one sentence from Test Data to Predict.\n",
    "    data_index = random.randint(0,100)\n",
    "    # Since its only 1 sentence, we need to convert into a 2-D list before sending it to torch.tensor()\n",
    "    Xe_b = torch.tensor([X_encoder_indices_test[data_index]]).to(device)\n",
    "    print(\"Test Jumbled sentence:\",X_encoder_tokens_test[data_index])\n",
    "    print(\"Test Unjumbled sentence:\", Y_tokens_test[data_index]) \n",
    "    unjumbled_sentence = predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length)\n",
    "    print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
