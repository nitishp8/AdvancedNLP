{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import math\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from create_seq2seq_data import Sequence2SequenceData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jumbled_file = \"../../Datasets/Jumble_Unjumble/OriginalJumbled.txt\"\n",
    "unjumbled_file = \"../../Datasets/Jumble_Unjumble/OriginalUnJumbled.txt\"\n",
    "full_data_object = Sequence2SequenceData(\n",
    "    source_data_file=jumbled_file,\n",
    "    target_data_file=unjumbled_file,\n",
    "    extra_token_config={\"unknown\":\"<unk>\",\"pad\":\"<pad>\",\"eos\":\"<eos>\",\"sos\":\"<sos>\"},\n",
    "    max_sequence_length=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing full dataset, Vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>up entry climbing A in dress an a in way. of a...</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>into going building. wooden girl A a</td>\n",
       "      <td>A girl going into a wooden building.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>climbing wooden a little girl into playhouse. A</td>\n",
       "      <td>A little girl climbing into a wooden playhouse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to her playhouse. girl the A little stairs cli...</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wooden dress in cabin. into pink little girl A...</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Source  \\\n",
       "0  up entry climbing A in dress an a in way. of a...   \n",
       "1               into going building. wooden girl A a   \n",
       "2    climbing wooden a little girl into playhouse. A   \n",
       "3  to her playhouse. girl the A little stairs cli...   \n",
       "4  wooden dress in cabin. into pink little girl A...   \n",
       "\n",
       "                                              Target  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1               A girl going into a wooden building.  \n",
       "2    A little girl climbing into a wooden playhouse.  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_object.full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9018\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size:\",len(full_data_object.token_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing raw tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Encoder token length: 40460\n",
      "Sample X_Encoder tokens: ['up', 'entry', 'climbing', 'a', 'in', 'dress', 'an', 'a', 'in', 'way', '.', 'of', 'a', 'child', 'is', 'stairs', 'pink', 'set', '<eos>']\n",
      "Sample X_Decoder tokens: ['<sos>', 'a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way', '.']\n",
      "Sample Y tokens: ['a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(\"X_Encoder token length:\",len(full_data_object.X_encoder_tokens))\n",
    "print(\"Sample X_Encoder tokens:\",full_data_object.X_encoder_tokens[0])\n",
    "print(\"Sample X_Decoder tokens:\",full_data_object.X_decoder_tokens[0])\n",
    "print(\"Sample Y tokens:\",full_data_object.Y_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Encoderindices length: 40460\n",
      "Sample X_Encoder indices length: 25\n",
      "Sample X_Encoder indices: [2175, 2287, 4175, 7903, 8081, 7063, 7323, 7903, 8081, 8430, 170, 5235, 7903, 5835, 2741, 6804, 1890, 4182, 2, 1, 1, 1, 1, 1, 1]\n",
      "Sample X_Decoder indices: [3, 7903, 5835, 8081, 7903, 1890, 7063, 2741, 4175, 2175, 7903, 4182, 5235, 6804, 8081, 7323, 2287, 8430, 170, 1, 1, 1, 1, 1, 1]\n",
      "Sample Y indices: [7903, 5835, 8081, 7903, 1890, 7063, 2741, 4175, 2175, 7903, 4182, 5235, 6804, 8081, 7323, 2287, 8430, 170, 2, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"X_Encoderindices length:\",len(full_data_object.X_encoder_indices))\n",
    "print(\"Sample X_Encoder indices length:\",len(full_data_object.X_encoder_indices[0]))\n",
    "print(\"Sample X_Encoder indices:\",full_data_object.X_encoder_indices[0])\n",
    "print(\"Sample X_Decoder indices:\",full_data_object.X_decoder_indices[0])\n",
    "print(\"Sample Y indices:\",full_data_object.Y_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model \n",
    "1. Encoder\n",
    "2. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingClass(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_token_length):\n",
    "        super(EmbeddingClass, self).__init__()\n",
    "        self.max_token_length = max_token_length\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_token_length, embedding_dim)\n",
    "    \n",
    "    def forward(self,token_ids, debug = False):\n",
    "        # create position ids\n",
    "        batch_size = token_ids.shape[0]\n",
    "        position_id = range(0,self.max_token_length)\n",
    "        position_ids = torch.tensor([position_id]*batch_size)\n",
    "        \n",
    "        token_embeddings = self.tok_embedding(token_ids)\n",
    "        position_embeddings = self.pos_embedding(position_ids)\n",
    "        \n",
    "        embbeded_x = token_embeddings + position_embeddings\n",
    "        \n",
    "        if debug:\n",
    "            print(\"----------Embedding Class------------\")\n",
    "            print(\"token_ids shape:\",token_ids.shape)\n",
    "            print(\"batch_size:\", batch_size)\n",
    "            print(\"position_ids shape:\", position_ids.shape)\n",
    "            print(\"token_embeddings shape:\", token_embeddings.shape)\n",
    "            print(\"position_embeddings shape:\", position_embeddings.shape)\n",
    "            print(\"embbeded_x shape:\", embbeded_x.shape)\n",
    "        \n",
    "        return embbeded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.W_K = nn.Linear(embedding_dim, head_dim)\n",
    "        self.W_Q = nn.Linear(embedding_dim, head_dim)\n",
    "        self.W_V = nn.Linear(embedding_dim, head_dim)\n",
    "    \n",
    "    def forward(self, embedded_x1, embedded_x2, mask_matrix = None, debug = False):\n",
    "        # Batch_Size = B, Seq_Length = L, Embedding_Dim = D, Head_Dim = H\n",
    "        # embedded_x shape = (B X L X D)\n",
    "        q = self.W_Q(embedded_x1) # q shape = (B X L X H)\n",
    "        k = self.W_K(embedded_x2) # k shape = (B X L X H)\n",
    "        v = self.W_V(embedded_x2) # v shape = (B X L X H)\n",
    "        \n",
    "        scores = torch.bmm(q, k.transpose(1,2))/math.sqrt(self.head_dim)\n",
    "        # scores shape = (B X L X L)\n",
    "        # torch.bmm() does the batch matrix multiplication:\n",
    "        # input_1 = (b X n X m), input_2 = (b X m X p)\n",
    "        # op = torch.zeros(b X n X p)\n",
    "        # for each matrix (n X m) in input_1 and (m X p) in input_2, \n",
    "        # torch.bmm() will output an (n X p) matrix. \n",
    "        # This will be done 'b' times (i.e for all b matrices)\n",
    "        \n",
    "        if mask_matrix is not None and mask_matrix.shape[0] > 0:\n",
    "            scores = scores.masked_fill(mask_matrix==0, float(\"-inf\"))\n",
    "        weights = torch.softmax(scores, dim=2) # (B X L X L)\n",
    "        output = torch.bmm(weights, v) # (B X L X H)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"----------Self-Attention------------\")\n",
    "            print(\"q shape:\", q.shape)\n",
    "            print(\"k shape:\", k.shape)\n",
    "            print(\"v shape:\", v.shape)\n",
    "            if mask_matrix is not None and mask_matrix.shape[0] > 0: print(\"mask_matrix shape:\", mask_matrix.shape)\n",
    "            print(\"scores shape:\", scores.shape)\n",
    "            print(\"weights shape:\", weights.shape)\n",
    "            print(\"output shape:\", output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.sa_list = nn.ModuleList([\n",
    "           SelfAttention(embedding_dim, self.head_dim) \n",
    "            for _ in range(self.num_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, embedded_x1, embedded_x2, mask_matrix = None, debug = False):\n",
    "        multiple_self_attn_op = [\n",
    "            self_attn_head(embedded_x1, embedded_x2, mask_matrix, debug) \n",
    "            for self_attn_head in self.sa_list\n",
    "        ]\n",
    "        concatenated_op = torch.cat(multiple_self_attn_op,axis=2)\n",
    "        linear_op = self.linear(concatenated_op)\n",
    "\n",
    "        if debug:\n",
    "            print(\"----------MHA------------\")\n",
    "            print(\"embedded_x1 shape:\", embedded_x1.shape)\n",
    "            print(\"embedded_x2 shape:\", embedded_x2.shape)\n",
    "            print(\"multiple_self_attn_op len:\", len(multiple_self_attn_op))\n",
    "            print(\"concatenated_op shape:\", concatenated_op.shape)\n",
    "            print(\"linear_op shape:\", linear_op.shape)\n",
    "        return linear_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embedding_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(embedding_dim, 4*embedding_dim)\n",
    "        self.linear_2 = nn.Linear(4*embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self,mha_op, debug=False):\n",
    "        linear_1_op = self.linear_1(mha_op)\n",
    "        linear_1_op = self.gelu(linear_1_op)\n",
    "        linear_2_op = self.linear_2(linear_1_op)\n",
    "        linear_2_op = self.dropout(linear_2_op)\n",
    "        if debug:\n",
    "            print(\"----------Feed Forward------------\")\n",
    "            print(\"mha_op shape:\", mha_op.shape)\n",
    "            print(\"linear_1_op shape:\", linear_1_op.shape)\n",
    "            print(\"linear_2_op shape:\", linear_2_op.shape)\n",
    "        return linear_2_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.ff_layer = FeedForward(embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, embedded_x, mask_matrix, debug=False):\n",
    "        mha_op = self.mha(embedded_x, embedded_x, mask_matrix, debug)\n",
    "        layer_norm_op_1 = self.layer_norm(mha_op + embedded_x)\n",
    "        ff_op = self.ff_layer(layer_norm_op_1, debug)\n",
    "        layer_norm_op_2 = self.layer_norm(ff_op + layer_norm_op_1)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"----------EncoderBlock------------\")\n",
    "            print(\"embedded_x shape:\", embedded_x.shape)\n",
    "            print(\"mha_op shape:\", mha_op.shape)\n",
    "            print(\"layer_norm_op_1 shape:\", layer_norm_op_1.shape)\n",
    "            print(\"ff_op shape:\", ff_op.shape)\n",
    "            print(\"layer_norm_op_2 shape:\", layer_norm_op_2.shape)\n",
    "        return layer_norm_op_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_token_length, num_heads, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_obj = EmbeddingClass(\n",
    "            vocab_size, embedding_dim, max_token_length\n",
    "        )\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            EncoderBlock(embedding_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, token_ids, debug=False):\n",
    "        embedded_x = self.embedding_obj(token_ids, debug)\n",
    "        for i,block in enumerate(self.encoder_blocks):\n",
    "            if i == 0: block_op  = block(embedded_x, None, debug)\n",
    "            else:\n",
    "                block_op  = block(block_op, None, debug)\n",
    "        if debug:\n",
    "            print(\"----------Encoder------------\")\n",
    "            print(\"token_ids shape:\", token_ids.shape)\n",
    "            print(\"block_op shape:\", block_op.shape)\n",
    "        return block_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.ff_layer = FeedForward(embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, embedded_x, encoder_op, mask_matrix, debug=False):\n",
    "        masked_mha_op = self.mha(embedded_x, embedded_x, mask_matrix, debug)\n",
    "        layer_norm_op_1 = self.layer_norm(masked_mha_op + embedded_x)\n",
    "        \n",
    "        mha_cross_attn_op = self.mha(layer_norm_op_1, encoder_op, None, debug)\n",
    "        layer_norm_op_2 = self.layer_norm(mha_cross_attn_op + layer_norm_op_1)\n",
    "        \n",
    "        ff_op = self.ff_layer(layer_norm_op_2, debug)\n",
    "        layer_norm_op_3 = self.layer_norm(ff_op + layer_norm_op_2)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"----------DecoderBlock------------\")\n",
    "            print(\"embedded_x shape:\", embedded_x.shape)\n",
    "            print(\"encoder_op shape:\", encoder_op.shape)\n",
    "            print(\"masked_mha_op shape:\", masked_mha_op.shape)\n",
    "            print(\"layer_norm_op_1 shape:\", layer_norm_op_1.shape)\n",
    "            print(\"mha_cross_attn_op shape:\", mha_cross_attn_op.shape)\n",
    "            print(\"layer_norm_op_2 shape:\", layer_norm_op_2.shape)\n",
    "            print(\"ff_op shape:\", ff_op.shape)\n",
    "            print(\"layer_norm_op_3 shape:\", layer_norm_op_3.shape)\n",
    "        return layer_norm_op_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_token_length, num_heads, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_obj = EmbeddingClass(\n",
    "            vocab_size, embedding_dim, max_token_length\n",
    "        )\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(embedding_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.mask_matrix = torch.tril(\n",
    "            torch.ones(max_token_length, max_token_length)\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, token_ids, encoder_op, debug=False):\n",
    "        embedded_x = self.embedding_obj(token_ids, debug)\n",
    "        for i,block in enumerate(self.encoder_blocks):\n",
    "            if i == 0: block_op  = block(\n",
    "                embedded_x, encoder_op, self.mask_matrix, debug\n",
    "            )\n",
    "            else:\n",
    "                block_op  = block(\n",
    "                    block_op, encoder_op, self.mask_matrix, debug\n",
    "                )\n",
    "        reshaped_output = block_op.reshape(\n",
    "            -1,block_op.shape[2]\n",
    "        )\n",
    "        linear_op = self.linear(reshaped_output)\n",
    "        if debug:\n",
    "            print(\"----------Decoder------------\")\n",
    "            print(\"token_ids shape:\", token_ids.shape)\n",
    "            print(\"block_op shape:\", block_op.shape)\n",
    "            print(\"reshaped_output shape:\", reshaped_output.shape)\n",
    "            print(\"linear_op shape:\", linear_op.shape)\n",
    "        return linear_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, full_data_object, device):\n",
    "        super(Seq2Seq, self).__init__() \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.full_data_object = full_data_object\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input, teacher_forcing_ratio, debug=False): \n",
    "        # encoder_input shape: Batch_Size X Seq_Len\n",
    "        # decoder_input shape: Batch_Size X Seq_Len\n",
    "        \n",
    "        encoder_outputs = self.encoder(encoder_input, debug)\n",
    "        \n",
    "        for i in range(self.full_data_object.max_sequence_length):\n",
    "            if i == 0:\n",
    "                # At 1st Time step, we need to pass <sos> token \n",
    "                # which is already provided in decoder_input.\n",
    "                decoder_outputs = self.decoder(decoder_input, encoder_outputs, debug)\n",
    "                # decoder_outputs shape: (Batch_Size*Seq_Len X Vocab_Size)\n",
    "            else:\n",
    "                softmax_op = torch.softmax(decoder_outputs,axis=1) # softmax_op is (Batch_Size*Seq_Len X Vocab_Size)\n",
    "                pred_tokens = torch.argmax(softmax_op,axis=1) # pred_tokens is torch.tensor([]) of len = (Batch_Size*Seq_Len)\n",
    "                reshaped_pred_tokens = pred_tokens.reshape(\n",
    "                    -1,self.full_data_object.max_sequence_length\n",
    "                ) # reshape to (Batch_Size X Seq_Len)\n",
    "                \n",
    "                teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "                if teacher_force:\n",
    "                    decoder_outputs = self.decoder(decoder_input, encoder_outputs, debug=False)\n",
    "                else:\n",
    "                    # Replace i-th token in decoder_input with (i-1)th predicted_token\n",
    "                    decoder_input[:,i] = reshaped_pred_tokens[:,i-1]\n",
    "                    decoder_outputs = self.decoder(decoder_input, encoder_outputs, debug=False)\n",
    "        if debug:\n",
    "            print(\"----------Seq2Seq------------\")\n",
    "            print(\"encoder_outputs shape:\", encoder_outputs.shape)\n",
    "            print(\"decoder_outputs shape:\", decoder_outputs.shape)\n",
    "            \n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_1_input(model, Xe_b, full_data_object, device):\n",
    "    model.eval()\n",
    "    sos_token = [full_data_object.token_to_index[\"<sos>\"]]\n",
    "    unjumbled_sentence = []\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encoder(Xe_b)\n",
    "        # At each time step, unlike RNN models, we can provide\n",
    "        # full sequence to decoder, leveraging the masking functionality in the\n",
    "        # self-attention of decoder.\n",
    "        for i in range(full_data_object.max_sequence_length):\n",
    "            if i == 0:\n",
    "                # At 1st time step, we only have <sos> token,\n",
    "                # so we provide one full sequence of <sos> tokens ([<sos>,<sos>,...,<sos>]) to decoder\n",
    "                decoder_input = sos_token*full_data_object.max_sequence_length \n",
    "                decoder_op = model.decoder(torch.tensor([decoder_input]).to(device), encoder_outputs)\n",
    "            else: \n",
    "                # After 1st time step, input to decoder is the predicted token of previous time step.\n",
    "                \n",
    "                # To get the predicted token of previous time step:\n",
    "                # first, do the softmax on decoder_op of previous time step\n",
    "                softmax_op = torch.softmax(decoder_op,axis=1) # decoder_op is (Seq_Len X Vocab_Size),\n",
    "\n",
    "                # next, take the token with max probability\n",
    "                predicted_token = torch.argmax(softmax_op,axis=1).tolist()\n",
    "                # torch.argmax() returns a tensor([]), tolist() gives out the [].\n",
    "                # The list will contain as many elements as 0th dimension of softmax_op.\n",
    "                # as we are taking argmax along axis = 1.\n",
    "                # In this case, softmax_op has (Seq_Len) tokens in 0th dimension, so the list has (Seq_Len) elements.\n",
    "                \n",
    "                decoder_input[i] = predicted_token[i-1]\n",
    "                # At i-th time step, replace the <sos> token with (i-1)-th step's prediction\n",
    "                decoder_op = model.decoder(\n",
    "                    torch.tensor([decoder_input]).to(device), encoder_outputs\n",
    "                )\n",
    "                unjumbled_sentence.append(full_data_object.index_to_token[predicted_token[i-1]])\n",
    "                if (\n",
    "                    predicted_token[i-1] == full_data_object.token_to_index[\"<eos>\"]\n",
    "                    or\n",
    "                    predicted_token[i-1] == full_data_object.token_to_index[\".\"]\n",
    "                ): break\n",
    "        print(\"_______________________________________\")\n",
    "        print(unjumbled_sentence)\n",
    "        return unjumbled_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36414 4046 36414 4046 36414 4046\n"
     ]
    }
   ],
   "source": [
    "Xe_tokens_tr, Xe_tokens_test, Xd_tokens_tr, Xd_tokens_test, Y_tokens_tr, Y_tokens_test,\\\n",
    "Xe_indices_tr, Xe_indices_test, Xd_indices_tr, Xd_indices_test, Y_indices_tr, Y_indices_test= train_test_split(\n",
    "    full_data_object.X_encoder_tokens, full_data_object.X_decoder_tokens, full_data_object.Y_tokens,\n",
    "    full_data_object.X_encoder_indices, full_data_object.X_decoder_indices, full_data_object.Y_indices,\n",
    "    test_size = 0.1\n",
    ")\n",
    "print(\n",
    "    len(Xe_tokens_tr), len(Xe_tokens_test), \n",
    "    len(Xd_tokens_tr), len(Xd_tokens_test), \n",
    "    len(Y_tokens_tr), len(Y_tokens_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Training And Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "INPUT_DIM = len(full_data_object.token_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(full_data_object.token_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 512 \n",
    "DEC_EMB_DIM = 512 \n",
    "NUM_ENC_LAYER = 6\n",
    "NUM_DEC_LAYER = 6\n",
    "NUM_HEADS = 8\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, full_data_object.max_sequence_length, NUM_HEADS, NUM_ENC_LAYER) \n",
    "dec = Decoder(INPUT_DIM, DEC_EMB_DIM, full_data_object.max_sequence_length, NUM_HEADS, NUM_DEC_LAYER) \n",
    "model = Seq2Seq(enc, dec, full_data_object, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters()) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=full_data_object.token_to_index[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 25]) torch.Size([50, 25]) torch.Size([50, 25])\n",
      "----------Embedding Class------------\n",
      "token_ids shape: torch.Size([50, 25])\n",
      "batch_size: 50\n",
      "position_ids shape: torch.Size([50, 25])\n",
      "token_embeddings shape: torch.Size([50, 25, 512])\n",
      "position_embeddings shape: torch.Size([50, 25, 512])\n",
      "embbeded_x shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------EncoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "----------Encoder------------\n",
      "token_ids shape: torch.Size([50, 25])\n",
      "block_op shape: torch.Size([50, 25, 512])\n",
      "----------Embedding Class------------\n",
      "token_ids shape: torch.Size([50, 25])\n",
      "batch_size: 50\n",
      "position_ids shape: torch.Size([50, 25])\n",
      "token_embeddings shape: torch.Size([50, 25, 512])\n",
      "position_embeddings shape: torch.Size([50, 25, 512])\n",
      "embbeded_x shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "encoder_op shape: torch.Size([50, 25, 512])\n",
      "masked_mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "encoder_op shape: torch.Size([50, 25, 512])\n",
      "masked_mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "encoder_op shape: torch.Size([50, 25, 512])\n",
      "masked_mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "encoder_op shape: torch.Size([50, 25, 512])\n",
      "masked_mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "encoder_op shape: torch.Size([50, 25, 512])\n",
      "masked_mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "mask_matrix shape: torch.Size([25, 25])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------Self-Attention------------\n",
      "q shape: torch.Size([50, 25, 64])\n",
      "k shape: torch.Size([50, 25, 64])\n",
      "v shape: torch.Size([50, 25, 64])\n",
      "scores shape: torch.Size([50, 25, 25])\n",
      "weights shape: torch.Size([50, 25, 25])\n",
      "output shape: torch.Size([50, 25, 64])\n",
      "----------MHA------------\n",
      "embedded_x1 shape: torch.Size([50, 25, 512])\n",
      "embedded_x2 shape: torch.Size([50, 25, 512])\n",
      "multiple_self_attn_op len: 8\n",
      "concatenated_op shape: torch.Size([50, 25, 512])\n",
      "linear_op shape: torch.Size([50, 25, 512])\n",
      "----------Feed Forward------------\n",
      "mha_op shape: torch.Size([50, 25, 512])\n",
      "linear_1_op shape: torch.Size([50, 25, 2048])\n",
      "linear_2_op shape: torch.Size([50, 25, 512])\n",
      "----------DecoderBlock------------\n",
      "embedded_x shape: torch.Size([50, 25, 512])\n",
      "encoder_op shape: torch.Size([50, 25, 512])\n",
      "masked_mha_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_1 shape: torch.Size([50, 25, 512])\n",
      "mha_cross_attn_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_2 shape: torch.Size([50, 25, 512])\n",
      "ff_op shape: torch.Size([50, 25, 512])\n",
      "layer_norm_op_3 shape: torch.Size([50, 25, 512])\n",
      "----------Decoder------------\n",
      "token_ids shape: torch.Size([50, 25])\n",
      "block_op shape: torch.Size([50, 25, 512])\n",
      "reshaped_output shape: torch.Size([1250, 512])\n",
      "linear_op shape: torch.Size([1250, 9018])\n",
      "----------Seq2Seq------------\n",
      "encoder_outputs shape: torch.Size([50, 25, 512])\n",
      "decoder_outputs shape: torch.Size([1250, 9018])\n",
      "Time taken in seconds: 8.866394758224487\n"
     ]
    }
   ],
   "source": [
    "data_index = 6\n",
    "batch_size = 50\n",
    "\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "Xe_b = torch.tensor(Xe_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Xd_b = torch.tensor(Xd_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Y_b = torch.tensor(Y_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "output = model(Xe_b, Xd_b, 0.9, debug=True)\n",
    "loss = criterion(output, Y_b.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "end_time = time.time()\n",
    "print(\"Time taken in seconds:\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Jumbled sentence: ['fenced', 'with', 'area', 'animals', '.', 'farm', 'playing', 'two', 'grassy', 'children', 'in', 'a', 'in', '<eos>']\n",
      "Test Unjumbled sentence: ['two', 'children', 'playing', 'in', 'a', 'fenced', 'in', 'grassy', 'area', 'with', 'farm', 'animals', '.', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'man', 'and', 'woman', 'points', ',', 'to', 'put', 'put', 'put', 'put', 'put', 'her', 'friend', 'interviewed', 'to', 'put', 'her', 'hand', 'put', 'put', 'put', 'put', 'put']\n"
     ]
    }
   ],
   "source": [
    "# Randomly select one sentence from Test Data to Predict.\n",
    "data_index = random.randint(0,100)\n",
    "# Since its only 1 sentence, we need to convert into a 2-D list before sending it to torch.tensor()\n",
    "Xe_b = torch.tensor([Xe_indices_test[data_index]]).to(device)\n",
    "print(\"Test Jumbled sentence:\",Xe_tokens_test[data_index])\n",
    "print(\"Test Unjumbled sentence:\", Y_tokens_test[data_index]) \n",
    "unjumbled_sentence = predict_on_1_input(model, Xe_b, full_data_object, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Per Epoch: 729\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\" #torch.device(\"cuda:0\")\n",
    "batch_size = 50\n",
    "INPUT_DIM = len(full_data_object.token_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(full_data_object.token_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 512 \n",
    "DEC_EMB_DIM = 512\n",
    "NUM_ENC_LAYER = 6\n",
    "NUM_DEC_LAYER = 6\n",
    "NUM_HEADS = 8\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, full_data_object.max_sequence_length, NUM_HEADS, NUM_ENC_LAYER) \n",
    "dec = Decoder(INPUT_DIM, DEC_EMB_DIM, full_data_object.max_sequence_length, NUM_HEADS, NUM_DEC_LAYER) \n",
    "model = Seq2Seq(enc, dec, full_data_object,device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=full_data_object.token_to_index[\"<pad>\"])\n",
    "epochs = 5\n",
    "steps_per_epoch = len(Xe_indices_tr)//batch_size + 1\n",
    "print(\"Steps Per Epoch:\",steps_per_epoch)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, epochs=epochs, steps_per_epoch=steps_per_epoch, max_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher forcing ratio for epoch 0 is 1.0\n",
      "Epoch: 0 Batch: 0 Loss: 9.263079643249512\n",
      "Epoch: 0 Batch: 500 Loss: 6.893364906311035\n",
      "Epoch: 0 Batch: 1000 Loss: 6.652351379394531\n",
      "Epoch: 0 Batch: 1500 Loss: 6.297564506530762\n",
      "Epoch: 0 Batch: 2000 Loss: 6.205170154571533\n",
      "Epoch: 0 Batch: 2500 Loss: 5.979619979858398\n",
      "Epoch: 0 Batch: 3000 Loss: 5.5181403160095215\n",
      "Epoch: 0 Batch: 3500 Loss: 5.453036785125732\n",
      "Epoch: 0 Batch: 4000 Loss: 5.256730079650879\n",
      "Epoch: 0 Batch: 4500 Loss: 5.124663829803467\n",
      "Epoch: 0 Batch: 5000 Loss: 5.095658779144287\n",
      "Epoch: 0 Batch: 5500 Loss: 5.089066505432129\n",
      "Epoch: 0 Batch: 6000 Loss: 4.757066249847412\n",
      "Epoch: 0 Batch: 6500 Loss: 4.592711448669434\n",
      "Epoch: 0 Batch: 7000 Loss: 4.648641109466553\n",
      "Epoch: 0 Batch: 7500 Loss: 4.4117302894592285\n",
      "Epoch: 0 Batch: 8000 Loss: 4.340845108032227\n",
      "Epoch: 0 Batch: 8500 Loss: 4.301884651184082\n",
      "Epoch: 0 Batch: 9000 Loss: 4.1593780517578125\n",
      "Epoch: 0 Batch: 9500 Loss: 4.153980731964111\n",
      "Epoch: 0 Batch: 10000 Loss: 3.9925413131713867\n",
      "Epoch: 0 Batch: 10500 Loss: 3.911827325820923\n",
      "Epoch: 0 Batch: 11000 Loss: 3.9487762451171875\n",
      "Epoch: 0 Batch: 11500 Loss: 4.183073997497559\n",
      "Epoch: 0 Batch: 12000 Loss: 3.8276946544647217\n",
      "Epoch: 0 Batch: 12500 Loss: 3.7936348915100098\n",
      "Epoch: 0 Batch: 13000 Loss: 3.844327926635742\n",
      "Epoch: 0 Batch: 13500 Loss: 3.5923969745635986\n",
      "Epoch: 0 Batch: 14000 Loss: 3.6354527473449707\n",
      "Epoch: 0 Batch: 14500 Loss: 3.5855982303619385\n",
      "Epoch: 0 Batch: 15000 Loss: 3.537299633026123\n",
      "Epoch: 0 Batch: 15500 Loss: 3.55592679977417\n",
      "Epoch: 0 Batch: 16000 Loss: 3.6602134704589844\n",
      "Epoch: 0 Batch: 16500 Loss: 3.537447690963745\n",
      "Epoch: 0 Batch: 17000 Loss: 3.4723730087280273\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing = 1.0\n",
    "for i in range(epochs):\n",
    "    teacher_forcing /= (i+1)\n",
    "    print(\"Teacher forcing ratio for epoch {0} is {1}\".format(i,teacher_forcing))\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for j in range(0,len(Xe_indices_tr),batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        Xe_b = torch.tensor(Xe_indices_tr[j:j+batch_size]).to(device)\n",
    "        Xd_b = torch.tensor(Xe_indices_tr[j:j+batch_size]).to(device)\n",
    "        Y_b = torch.tensor(Y_indices_tr[j:j+batch_size]).to(device)\n",
    "        op = model(Xe_b,Xd_b, teacher_forcing)\n",
    "        loss = criterion(op,Y_b.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        if j%500 == 0: print(\"Epoch:\",i,\"Batch:\",j,\"Loss:\",batch_loss)\n",
    "        scheduler.step() # OneCycleLR has to take a step after every batch.\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Epoch Loss:\",epoch_loss)\n",
    "    \n",
    "    # Randomly select one sentence from Test Data to Predict.\n",
    "    data_index = random.randint(0,100)\n",
    "    # Since its only 1 sentence, we need to convert into a 2-D list before sending it to torch.tensor()\n",
    "    Xe_b = torch.tensor([Xe_indices_test[data_index]]).to(device)\n",
    "    print(\"Test Jumbled sentence:\",Xe_tokens_test[data_index])\n",
    "    print(\"Test Unjumbled sentence:\", Y_tokens_test[data_index]) \n",
    "    unjumbled_sentence = predict_on_1_input(model, Xe_b, full_data_object, device)\n",
    "    print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
