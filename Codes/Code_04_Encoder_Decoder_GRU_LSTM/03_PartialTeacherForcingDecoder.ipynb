{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> In this notebook, we will try to unjumble a sentence using Encoder-Decoder Architecture built using <br><br>\n",
    " Recurrent Networks like GRU, LSTM and Bi-directional LSTMs.</h6>\n",
    "<h6> The Data is located here: ../../Datasets/Jumble_Unjumble/ </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/Train_400.tsv\",sep=\"\\t\")\n",
    "# test_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/Test_100.tsv\",sep=\"\\t\")\n",
    "# print(train_df.shape, test_df.shape)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32368, 2) (8092, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumbled_sentences</th>\n",
       "      <th>unjumbled_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32760</th>\n",
       "      <td>tools and a man gardening inside two holding a...</td>\n",
       "      <td>a man and two women are inside a greenhouse ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31413</th>\n",
       "      <td>meandering at people of the walkway stand . up...</td>\n",
       "      <td>people stand at the bottom of a meandering wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>standing a rock . on man view the shorts a out...</td>\n",
       "      <td>a man in shorts is standing on a rock looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28232</th>\n",
       "      <td>to a a in on shirt little red a holds pole nea...</td>\n",
       "      <td>a little girl in a red shirt holds on to a pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28438</th>\n",
       "      <td>children two in &lt;unk&gt; play the melting .</td>\n",
       "      <td>two children play in the melting &lt;unk&gt; .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       jumbled_sentences  \\\n",
       "32760  tools and a man gardening inside two holding a...   \n",
       "31413  meandering at people of the walkway stand . up...   \n",
       "4325   standing a rock . on man view the shorts a out...   \n",
       "28232  to a a in on shirt little red a holds pole nea...   \n",
       "28438          children two in <unk> play the melting .    \n",
       "\n",
       "                                     unjumbled_sentences  \n",
       "32760  a man and two women are inside a greenhouse ho...  \n",
       "31413  people stand at the bottom of a meandering wal...  \n",
       "4325   a man in shorts is standing on a rock looking ...  \n",
       "28232  a little girl in a red shirt holds on to a pol...  \n",
       "28438          two children play in the melting <unk> .   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jumbled_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/processed_jumbled.txt\",sep=\"\\t\",header=None)\n",
    "unjumbled_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/processed_unjumbled.txt\",sep=\"\\t\",header=None)\n",
    "jumbled_df.columns = [\"jumbled_sentences\"]\n",
    "unjumbled_df.columns = [\"unjumbled_sentences\"]\n",
    "df = pd.concat([jumbled_df,unjumbled_df],axis=1)\n",
    "train_df = df.sample(frac=0.8, random_state=42) \n",
    "test_df = df.drop(train_df.index)\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Train and Test Data\n",
    "1. Lowercasing, removing stopwords. <br>\n",
    "2. Stemming, Lemmetization. <br>\n",
    "3. Tokenization. <br>\n",
    "4. Here, we are just doing tokenization by splitting on space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenize_on = \" \"\n",
    "    \n",
    "    def tokenize(self,text_string):\n",
    "        '''\n",
    "        text_string = \"This is one sentence.\"\n",
    "        returns token_list = [\"This\",\"is\",\"one\",\"sentence.\"]\n",
    "        '''\n",
    "        token_list = text_string.split(self.tokenize_on)\n",
    "        return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32368, 2) (8092, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumbled_sentences</th>\n",
       "      <th>unjumbled_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32760</th>\n",
       "      <td>[tools, and, a, man, gardening, inside, two, h...</td>\n",
       "      <td>[a, man, and, two, women, are, inside, a, gree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31413</th>\n",
       "      <td>[meandering, at, people, of, the, walkway, sta...</td>\n",
       "      <td>[people, stand, at, the, bottom, of, a, meande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>[standing, a, rock, ., on, man, view, the, sho...</td>\n",
       "      <td>[a, man, in, shorts, is, standing, on, a, rock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28232</th>\n",
       "      <td>[to, a, a, in, on, shirt, little, red, a, hold...</td>\n",
       "      <td>[a, little, girl, in, a, red, shirt, holds, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28438</th>\n",
       "      <td>[children, two, in, &lt;unk&gt;, play, the, melting,...</td>\n",
       "      <td>[two, children, play, in, the, melting, &lt;unk&gt;,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       jumbled_sentences  \\\n",
       "32760  [tools, and, a, man, gardening, inside, two, h...   \n",
       "31413  [meandering, at, people, of, the, walkway, sta...   \n",
       "4325   [standing, a, rock, ., on, man, view, the, sho...   \n",
       "28232  [to, a, a, in, on, shirt, little, red, a, hold...   \n",
       "28438  [children, two, in, <unk>, play, the, melting,...   \n",
       "\n",
       "                                     unjumbled_sentences  \n",
       "32760  [a, man, and, two, women, are, inside, a, gree...  \n",
       "31413  [people, stand, at, the, bottom, of, a, meande...  \n",
       "4325   [a, man, in, shorts, is, standing, on, a, rock...  \n",
       "28232  [a, little, girl, in, a, red, shirt, holds, on...  \n",
       "28438  [two, children, play, in, the, melting, <unk>,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor()\n",
    "train_df[\"jumbled_sentences\"] = train_df[\"jumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "train_df[\"unjumbled_sentences\"] = train_df[\"unjumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "test_df[\"jumbled_sentences\"] = test_df[\"jumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "test_df[\"unjumbled_sentences\"] = test_df[\"unjumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X_Encoder, X_Decoder and Y\n",
    "1. X denotes Input, Y denotes Output. <br>\n",
    "2. X_encoder is the matrix of tokens in jumbled_sentences, each sentence suffixed by \"eos\" token. <br>\n",
    "3. X_decoder is the matrix of tokens in unjumbled_sentences, each sentence prefixed by \"sos\" token. X_decoder is required because we want to do <b>Teacher Forcing</b>, which means we want to provide the correct current token to decoder to predict next token, instead of relying only on its own prediction. <br>\n",
    "4. Y is the matrix of unjumbled_sentences, each sentence suffixed by \"eos\" token. <br>\n",
    "\n",
    "5. Do this for both train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 32368\n",
      "X Encoder test length: 8092\n",
      "Sample X_encoder_train: ['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.', 'women', 'a', 'greenhouse', '', '<eos>']\n",
      "Sample X_decoder_train: ['<sos>', 'a', 'man', 'and', 'two', 'women', 'are', 'inside', 'a', 'greenhouse', 'holding', 'gardening', 'tools', '.', '']\n",
      "Sample Y_train: ['a', 'man', 'and', 'two', 'women', 'are', 'inside', 'a', 'greenhouse', 'holding', 'gardening', 'tools', '.', '', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "def get_Xe_Xd_Y(dataframe, sos_token, eos_token):\n",
    "    jumbled_sentences = dataframe[\"jumbled_sentences\"].tolist()\n",
    "    unjumbled_sentences = dataframe[\"unjumbled_sentences\"].tolist()\n",
    "    X_encoder_tokens = [el + [eos_token] for el in jumbled_sentences]\n",
    "    X_decoder_tokens = [[sos_token] + el for el in unjumbled_sentences]\n",
    "    Y_tokens = [el + [eos_token] for el in unjumbled_sentences]\n",
    "    return X_encoder_tokens, X_decoder_tokens, Y_tokens\n",
    "\n",
    "unknown_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "X_encoder_tokens_tr, X_decoder_tokens_tr, Y_tokens_tr = get_Xe_Xd_Y(train_df, sos_token, eos_token)\n",
    "X_encoder_tokens_test, X_decoder_tokens_test, Y_tokens_test = get_Xe_Xd_Y(test_df, sos_token, eos_token)\n",
    "print(\"X Encoder train length:\",len(X_encoder_tokens_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_tokens_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_tokens_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_tokens_tr[0])\n",
    "print(\"Sample Y_train:\",Y_tokens_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocab\n",
    "1. Generally, Vocab is created from both Encoder and Decoder Tokens, consider Senetence Translation for ex, where encoder and decoder tokens can be in different languages. <br>\n",
    "2. We can create Vocab separately for Encder and Decoder tokens, or can create shared vocab. Shared Vocab is preferable though. <br>\n",
    "3. Also, Vocab is generated from only Training Data. <br>\n",
    "4. In this case, we are using only Encoder tokens to create Vocab because Decoder Tokens are the same. Also, we are using both Train and Test Dataset to create Vocab, as our datasize is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabBuilder:\n",
    "    def __init__(self,token_corpus,unknown_token=None,pad_token=None,sos_token=None,eos_token=None):\n",
    "        '''\n",
    "        token_corpus = ['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.']\n",
    "        '''\n",
    "        self.token_corpus = token_corpus\n",
    "        self.unknown_token = unknown_token or \"<unk>\"\n",
    "        self.pad_token = pad_token or \"<pad>\"\n",
    "        self.sos_token = sos_token or \"<sos>\"\n",
    "        self.eos_token = eos_token or \"<eos>\"\n",
    "        self.word_to_index, self.index_to_word = self.get_vocabs()\n",
    "                        \n",
    "    def get_vocabs(self):\n",
    "        word_to_index = {}\n",
    "        index_count = 0\n",
    "        all_unique_words = set(self.token_corpus).difference(set(\n",
    "            [self.unknown_token, self.pad_token, self.sos_token, self.eos_token]\n",
    "        ))\n",
    "        word_to_index[self.unknown_token] = 0\n",
    "        word_to_index[self.pad_token] = 1\n",
    "        word_to_index[self.sos_token] = 2\n",
    "        word_to_index[self.eos_token] = 3\n",
    "        \n",
    "        for index, word in enumerate(all_unique_words):\n",
    "            word_to_index[word] = index + 4\n",
    "        if self.pad_token not in word_to_index: word_to_index[self.pad_token] = index + 1\n",
    "        if self.sos_token not in word_to_index: word_to_index[self.sos_token] = index + 2\n",
    "        if self.eos_token not in word_to_index: word_to_index[self.eos_token] = index + 3\n",
    "        if self.unknown_token not in word_to_index: word_to_index[self.unknown_token] = index + 4\n",
    "        index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "        return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.', 'women', 'a', 'greenhouse', '', '<eos>', 'meandering', 'at', 'people', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "token_corpus_1 = list(chain.from_iterable(X_encoder_tokens_tr)) # flattens a 2D list ot 1D\n",
    "token_corpus_2 = list(chain.from_iterable(X_encoder_tokens_test))  # flattens a 2D list ot 1D\n",
    "token_corpus = token_corpus_1 + token_corpus_2\n",
    "print(token_corpus[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordToIndex Dict length: 5242\n",
      "IndexToWord Dict length: 5242\n"
     ]
    }
   ],
   "source": [
    "vocab_builder = VocabBuilder(token_corpus,unknown_token,pad_token,sos_token,eos_token)\n",
    "print(\"WordToIndex Dict length:\",len(vocab_builder.word_to_index))\n",
    "print(\"IndexToWord Dict length:\",len(vocab_builder.index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map X_encoder, X_decoder and Y using Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_Index_Mapper:\n",
    "    def __init__(self,token_to_index,index_to_token, unknown_token):\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = index_to_token\n",
    "        self.unknown_token = unknown_token\n",
    "    \n",
    "    def get_encoding(self,sentence):\n",
    "        '''\n",
    "        sentence must be a list of tokens.\n",
    "        Ex: [\"Climate\",\"change\",\"is\",\"a\",\"pressing\",\"global\",\"issue\"]\n",
    "        '''\n",
    "        encoded_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in self.token_to_index: encoded_sentence.append(self.token_to_index[token])\n",
    "            else: encoded_sentence.append(self.token_to_index[self.unknown_token])\n",
    "        return encoded_sentence\n",
    "    \n",
    "    def get_decoding(self,encoded_sentence):\n",
    "        '''\n",
    "        encoded_sentence must be a list of vocab indices.\n",
    "        Ex: encoded_sentence = [24,21,4,1,..] \n",
    "        '''\n",
    "        sentence = [self.index_to_token[index] for index in encoded_sentence]\n",
    "        return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens_to_indices(token_index_mapper, max_sequence_length, token_matrix):\n",
    "    index_matrix = []\n",
    "    for el in token_matrix:\n",
    "        el = el[:max_sequence_length] # truncate sentence to max_seq_length\n",
    "        if len(el) < max_sequence_length:\n",
    "            pad_tokens_to_append = max_sequence_length - len(el)\n",
    "            el = el + [pad_token]*pad_tokens_to_append\n",
    "        index_matrix.append(token_index_mapper.get_encoding(el))\n",
    "    return index_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 32368\n",
      "X Encoder test length: 8092\n",
      "Sample X_encoder_train: [880, 2139, 2864, 2701, 301, 2700, 1406, 2761, 2548, 1326, 5002, 2864, 3319, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample X_decoder_train: [2, 2864, 2701, 2139, 1406, 5002, 2548, 2700, 2864, 3319, 2761, 301, 880, 1326, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample Y_train: [2864, 2701, 2139, 1406, 5002, 2548, 2700, 2864, 3319, 2761, 301, 880, 1326, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 25\n",
    "token_index_mapper = Token_Index_Mapper(vocab_builder.word_to_index, vocab_builder.index_to_word, unknown_token)\n",
    "X_encoder_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_encoder_tokens_tr)\n",
    "X_decoder_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_decoder_tokens_tr)\n",
    "Y_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, Y_tokens_tr)\n",
    "\n",
    "X_encoder_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_encoder_tokens_test)\n",
    "X_decoder_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_decoder_tokens_test)\n",
    "Y_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, Y_tokens_test)\n",
    "print(\"X Encoder train length:\",len(X_encoder_indices_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_indices_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_indices_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_indices_tr[0])\n",
    "print(\"Sample Y_train:\",Y_indices_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model - (UniDirectional + BatchFirst + SingleLayer + StateLess)  GRU, Partial TeacherForcing Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, debug=False):\n",
    "        # X shape: (Batch_Size X Seq_Length)\n",
    "        \n",
    "        embedded = self.embedding(x)   \n",
    "        # Embedded shape: (Batch_Size X Seq_Length X Embedding_Dim)\n",
    "        \n",
    "        dropped_out = self.dropout(embedded) \n",
    "        # DroppedOut shape: (Batch_Size X Seq_Length X Embedding_Dim)\n",
    "        \n",
    "        output, hidden = self.gru(dropped_out) \n",
    "        # Output shape: (Batch_Size X Seq_Length X D*Hidden_Dim), where D == 2 if Bi-directional, else 1. \n",
    "        # It contains h_t for all tokens, hence the Seq_Length in shape.\n",
    "        # If more GRU layers are present, it contains h_t for all tokens but only from last layer.\n",
    "        \n",
    "        # Hidden shape: (D*NumLayers X Batch_Size X Hidden_Dim)\n",
    "        # It contains the h_t for only last token of each sequence in the batch.\n",
    "        # In case of multiple GRU layers:,\n",
    "        # It can be thought of as a stack of (Batch_Size X Hidden_Dim) matrix, where each GRU Layer\n",
    "        # is contributing 2 matrices (one for each direction in case of Bi-directional), \n",
    "        # such that the last layer's matrix is at the top of the stack and can be accessed using h_t[-1].\n",
    "        \n",
    "        hidden_unsqueezed = hidden[-1].unsqueeze(dim = 0)\n",
    "        # This operation has no meaning for the UniDirectional + SingleLayer case, but helpful for other cases.\n",
    "        # hidden[-1] shape = Batch_Size X Hidden_Dim, we are only taking the last layer's matrix.\n",
    "        # hidden[-1].unsqueeze(dim = 0) shape: We are converting the 2-D matrix back to 3-D matrix,\n",
    "        # with the first dimension (or the 0th dimension) == 1.\n",
    "        \n",
    "        if debug: \n",
    "            print(\"-----------Encoder----------:\")\n",
    "            print(\"Input Data shape:\",x.shape)\n",
    "            print(\"After Embedding Layer:\",embedded.shape)\n",
    "            print(\"After Dropout Layer:\",dropped_out.shape)\n",
    "            print(\"Outputs and hidden shape from GRU:\",output.shape, hidden.shape)\n",
    "            print(\"Unsqueezed hidden shape:\", hidden_unsqueezed.shape)\n",
    "            \n",
    "        # Returning both the output and hidden state, but Decoder will only need hidden state.    \n",
    "        return output, hidden_unsqueezed\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim,  batch_first=True) # Decoders are almost always unidirectional.\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, encoder_hidden, debug=False):\n",
    "        # X shape: (Batch_Size X Seq_Length)\n",
    "        # X (decoder) is only required when we do Teacher Forcing.\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        # Embedded shape: (Batch_Size X Seq_Length X Embedding_Dim)\n",
    "        \n",
    "        output, hidden = self.gru(embedded, encoder_hidden)\n",
    "        # Output shape: (Batch_Size X Seq_Length X D*Hidden_Dim), where D == 2 if Bi-directional, else 1.\n",
    "        # Bi-directional Decoder does not make sense, except for very rare cases.\n",
    "        # Hidden shape: (D*NumLayers X Batch_Size X Hidden_Dim)\n",
    "        # It contains the h_t for only last token of each sequence in the batch.\n",
    "        \n",
    "        reshaped_output = output.reshape(-1,output.shape[2])\n",
    "        # Reshaped_Output shape = (Batch_Size*Seq_Length X Hidden_Dim) [Basically a 2-D Matrix]\n",
    "        # Since we need to calculate loss on each token of the batch, the output has to be \n",
    "        # reshaped into Batch_Size*Seq_Length X Hidden_Dim.\n",
    "        # -1 in the reshape function tells Pytorch to figure out the size of Tensor by itself.\n",
    "        # output.shape[2] is the Hidden_Dim (obvious from Output Shape: Batch_Size X Seq_Length X D*Hidden_Dim)\n",
    "        # So, the reshape operation figures out the size of 0th dimension, \n",
    "        # given that size of 1st dimension ==  Hidden_Dim. \n",
    "        \n",
    "        prediction = self.fc_out(reshaped_output)\n",
    "        # prediction shape: (Batch_Size*Seq_Length X Vocab_Size)\n",
    "        # For each input token, we get the vector of vocab size consisting of logits.\n",
    "        # We take the softmax of each vector to convert vector of logits to vector of probabilities.\n",
    "        # Then we take argmax of each vector to get the predicted token.\n",
    "        \n",
    "        if debug: \n",
    "            print(\"-----------Decoder----------:\")\n",
    "            print(\"Input Data shape, X:\",x.shape, \", Encoder hidden state:\", encoder_hidden.shape)\n",
    "            print(\"After Embedding Layer:\",embedded.shape)\n",
    "            print(\"Outputs and hidden shape from GRU:\",output.shape, hidden.shape)\n",
    "            print(\"Reshaped Output:\", reshaped_output.shape)\n",
    "            print(\"After FC layer:\", prediction.shape)\n",
    "        return prediction, hidden\n",
    "    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio, max_seq_len, token_index_mapper):\n",
    "        super(Seq2Seq, self).__init__() \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_index_mapper = token_index_mapper\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input, debug=False): \n",
    "        # In Partial teacher forcing, for some of the timesteps we provide decoder's previous step output\n",
    "        # as the current step input. This is controlled be teacher_forcing_ratio. Usually the implementation is\n",
    "        # such that teacher_forcing_ratio=1 means full reliance on correct decoder input generated from data.\n",
    "        \n",
    "        # Encoder Input: (Batch_Size X Seq_Length)\n",
    "        # Encoder Hidden State Output: (1 X Batch_Size X Hidden_Dim), since the Encoder is UniDirectional, One Layer\n",
    "        \n",
    "        # Decoder Input: Since we have to send input to decoder one token at a time, so at each time step,\n",
    "        # Decoder Input shape is (Batch_Size X 1)\n",
    "        # At first time step, decoder_input is <sos>. So sos_tokens shape = (Batch_Size X 1)\n",
    "        # At later time steps: decoder_input will either be the predicted_token from previous time step or the teacher token.\n",
    "        \n",
    "        batch_size = encoder_input.shape[0]\n",
    "        sos_tokens = [[token_index_mapper.token_to_index[\"<sos>\"]]*batch_size]\n",
    "        sos_tokens = torch.tensor(sos_tokens).reshape(-1,1).to(self.device) # 1st token to decoder is <sos>.\n",
    "        \n",
    "        # the output matrix will have shape (Batch_Size, Seq_Length, Vocab_Size)\n",
    "        outputs = torch.zeros(batch_size,self.max_seq_len,len(token_index_mapper.token_to_index)).to(self.device)\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = self.encoder(encoder_input, debug)\n",
    "        for i in range(self.max_seq_len):\n",
    "            if i == 0: \n",
    "                decoder_op, decoder_hidden = self.decoder(sos_tokens, encoder_hidden, debug)\n",
    "            else:\n",
    "                teacher_force = torch.rand(1).item() < self.teacher_forcing_ratio\n",
    "                if teacher_force:\n",
    "                    # we need the ith token of all sequences in the batch\n",
    "                    # decoder_input[:,i] will be of len (BatchSize)\n",
    "                    # reshape(-1,1), means calculate the 0th dimension given that 1st dimension == 1\n",
    "                    # So, decoder_input[:,i].reshape(-1,1) will be (Batch_Size X 1)\n",
    "                    decoder_op, decoder_hidden = self.decoder(decoder_input[:,i].reshape(-1,1), decoder_hidden, debug)\n",
    "                else:\n",
    "                    # decoder_op is (Batch_Size X Vocab_Size)\n",
    "                    # decoder_hidden like encoder_hiddern is (1 X Batch_Size X Hidden_Dim)\n",
    "                    softmax_op = torch.softmax(decoder_op,axis=1) # softmax_op is (Batch_Size X Vocab_Size)\n",
    "                    pred_tokens = torch.argmax(softmax_op,axis=1) # pred_tokens is torch.tensor([]) of len (Batch_Size)\n",
    "                    reshaped_pred_tokens = pred_tokens.reshape(-1,1) # reshape to (Batch_Size X 1)\n",
    "                    decoder_op, decoder_hidden = self.decoder(reshaped_pred_tokens.to(self.device),decoder_hidden, debug)\n",
    "                    if debug:\n",
    "                        print(\"Seq2Seq softmax_op\",softmax_op.shape)\n",
    "                        print(\"Seq2Seq pred_tokens\",pred_tokens.shape)\n",
    "                        print(\"Seq2Seq reshaped_pred_tokens\",reshaped_pred_tokens.shape)\n",
    "            # We need to put the decoder_op for each token into output matrix.\n",
    "            outputs[:,i,:] = decoder_op\n",
    "        \n",
    "        # Finally we have to return the output in the shape (Batch_Size*Seq_Length, Vocab_Size)\n",
    "        # So, we are again using the same concept of reshape() to return final_matrix.\n",
    "        return outputs.reshape(-1,len(token_index_mapper.token_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length):\n",
    "    model.eval()\n",
    "    sos_token = torch.tensor([[token_index_mapper.token_to_index[\"<sos>\"]]]).to(device) # 1st token as decoder input is <sos>.\n",
    "    unjumbled_sentence = []\n",
    "    with torch.no_grad():\n",
    "        encoder_op, encoder_hidden = model.encoder(Xe_b)\n",
    "        for i in range(max_sequence_length):\n",
    "            if i == 0: \n",
    "                # At 1st time step, input to decoder is the index of <sos> token.\n",
    "                # And the hidden state input to decoder is Encoder's hidden state.\n",
    "                decoder_op, decoder_hidden = model.decoder(sos_token,encoder_hidden)\n",
    "            else: \n",
    "                # After 1st time step, input to decoder is the predicted token of previous time step.\n",
    "                # and hidden state input to decoder is the hidden state output of decoder of previous time step.\n",
    "                \n",
    "                # To get the predicted token of previous time step:\n",
    "                # first, do the softmax on decoder_op of previous time step\n",
    "                softmax_op = torch.softmax(decoder_op,axis=1) # decoder_op is (1 X Vocab_Size),\n",
    "\n",
    "                # next, take the token with max probability\n",
    "                # (softmax_op is also [1 X Vocab_Size], as we have taken softmax along axis=1, which\n",
    "                # has simply converted the logits to probabilities.)\n",
    "                # torch.argmax() returns a tensor([]). The list will contain as many elements as 0th dimension of softmax_op.\n",
    "                # because we are taking argamx along axis = 1.\n",
    "                # In this case, softmax_op has only 1 token in 0th dimension, so the list has only 1 element.\n",
    "                # torch.tensor([]).tolist() gives out the []\n",
    "                predicted_token = torch.argmax(softmax_op,axis=1).tolist() \n",
    "                decoder_op, decoder_hidden = model.decoder(torch.tensor([predicted_token]).to(device),decoder_hidden)\n",
    "                \n",
    "                unjumbled_sentence.append(token_index_mapper.index_to_token[predicted_token[0]])\n",
    "                if predicted_token[0] == token_index_mapper.token_to_index[\"<eos>\"]: break\n",
    "        print(\"_______________________________________\")\n",
    "        print(unjumbled_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Training And Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(token_index_mapper.token_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(token_index_mapper.token_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 256 \n",
    "DEC_EMB_DIM = 256 \n",
    "HID_DIM = 512 \n",
    "ENC_DROPOUT = 0.5 \n",
    "device = \"cpu\"\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT) \n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM) \n",
    "model = Seq2Seq(enc, dec, device, 0.7, max_sequence_length, token_index_mapper).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters()) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_index_mapper.token_to_index[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 25]) torch.Size([5, 25]) torch.Size([5, 25])\n",
      "-----------Encoder----------:\n",
      "Input Data shape: torch.Size([5, 25])\n",
      "After Embedding Layer: torch.Size([5, 25, 256])\n",
      "After Dropout Layer: torch.Size([5, 25, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 25, 512]) torch.Size([1, 5, 512])\n",
      "Unsqueezed hidden shape: torch.Size([1, 5, 512])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([5, 512])\n",
      "After FC layer: torch.Size([5, 5242])\n"
     ]
    }
   ],
   "source": [
    "data_index = 6\n",
    "batch_size = 5\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "Xe_b = torch.tensor(X_encoder_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Xd_b = torch.tensor(X_decoder_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Y_b = torch.tensor(Y_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "output = model(Xe_b, Xd_b, debug=True)\n",
    "loss = criterion(output, Y_b.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Jumbled sentence: ['a', 'leaps', 'to', 'ball', 'field', 'a', 'dog', 'a', '.', 'catch', 'in', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'dog', 'leaps', 'to', 'catch', 'a', 'ball', 'in', 'a', 'field', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'a', 'a', 'thriller', 'being', 'redbull', 'jumpsuit', 'approaching', 'medium', 'straight', 'pours', 'system', 'pole', 'dark', 'jack', 'snow-capped', 'snow-capped', 'couch', 'bottle', 'eagle', 'grinding', 'winning', 'frightened', 'seashore']\n"
     ]
    }
   ],
   "source": [
    "# Randomly select one sentence from Test Data to Predict.\n",
    "data_index = random.randint(0,100)\n",
    "# Since its only 1 sentence, we need to convert into a 2-D list before sending it to torch.tensor()\n",
    "Xe_b = torch.tensor([X_encoder_indices_test[data_index]]).to(device)\n",
    "print(\"Test Jumbled sentence:\",X_encoder_tokens_test[data_index])\n",
    "print(\"Test Unjumbled sentence:\", Y_tokens_test[data_index]) \n",
    "predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" #torch.device(\"cuda:0\") \n",
    "batch_size = 30\n",
    "INPUT_DIM = len(token_index_mapper.token_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(token_index_mapper.token_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 128 \n",
    "DEC_EMB_DIM = 128 \n",
    "HID_DIM = 400 \n",
    "ENC_DROPOUT = 0.5 \n",
    "DEC_DROPOUT = 0.5\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT) \n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM) \n",
    "model = Seq2Seq(enc, dec, device, 0.7, max_sequence_length, token_index_mapper).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters()) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_index_mapper.token_to_index[\"<pad>\"])\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 Loss: 0.7893884181976318\n",
      "Epoch: 0 Batch: 6000 Loss: 1.2249112129211426\n",
      "Epoch: 0 Batch: 12000 Loss: 0.9823906421661377\n",
      "Epoch: 0 Batch: 18000 Loss: 1.066917896270752\n",
      "Epoch: 0 Batch: 24000 Loss: 0.717788577079773\n",
      "Epoch: 0 Batch: 30000 Loss: 0.6527489423751831\n",
      "______________________________________\n",
      "Epoch Loss: 948.5007968246937\n",
      "Test Jumbled sentence: ['a', 'in', 'play', 'two', '.', 'boys', 'puddle', '', '<eos>']\n",
      "Test Unjumbled sentence: ['two', 'boys', 'play', 'in', 'a', 'puddle', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['two', 'boys', 'play', 'in', 'a', 'house', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 1 Batch: 0 Loss: 0.5903277397155762\n",
      "Epoch: 1 Batch: 6000 Loss: 0.9072443246841431\n",
      "Epoch: 1 Batch: 12000 Loss: 0.7678691744804382\n",
      "Epoch: 1 Batch: 18000 Loss: 0.9957275390625\n",
      "Epoch: 1 Batch: 24000 Loss: 0.7852197289466858\n",
      "Epoch: 1 Batch: 30000 Loss: 0.5788002610206604\n",
      "______________________________________\n",
      "Epoch Loss: 889.686778485775\n",
      "Test Jumbled sentence: ['head', 'white', 'its', 'with', 'tiled', 'its', 'floor', 'dog', 'eyes', 'on', 'a', 'a', 'open', 'resting', '.', 'is', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'white', 'dog', 'is', 'resting', 'its', 'head', 'on', 'a', 'tiled', 'floor', 'with', 'its', 'eyes', 'open', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'white', 'dog', 'is', 'laying', 'on', 'its', 'head', 'is', 'chewing', 'on', 'a', 'piece', 'of', 'paper', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 2 Batch: 0 Loss: 0.8047106266021729\n",
      "Epoch: 2 Batch: 6000 Loss: 1.0145056247711182\n",
      "Epoch: 2 Batch: 12000 Loss: 0.8447098731994629\n",
      "Epoch: 2 Batch: 18000 Loss: 0.9441179037094116\n",
      "Epoch: 2 Batch: 24000 Loss: 0.6151196956634521\n",
      "Epoch: 2 Batch: 30000 Loss: 0.5903992652893066\n",
      "______________________________________\n",
      "Epoch Loss: 835.3795092999935\n",
      "Test Jumbled sentence: ['of', 'tunnel', 'on', 'out', 'running', 'course', 'obstacle', 'dog', '', '<eos>']\n",
      "Test Unjumbled sentence: ['dog', 'running', 'out', 'of', 'tunnel', 'on', 'obstacle', 'course', '', '<eos>']\n",
      "_______________________________________\n",
      "['dog', 'running', 'on', 'course', 'on', 'his', 'board', 'course', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 3 Batch: 0 Loss: 0.46185562014579773\n",
      "Epoch: 3 Batch: 6000 Loss: 0.781269907951355\n",
      "Epoch: 3 Batch: 12000 Loss: 0.8210907578468323\n",
      "Epoch: 3 Batch: 18000 Loss: 1.3408273458480835\n",
      "Epoch: 3 Batch: 24000 Loss: 0.7750959396362305\n",
      "Epoch: 3 Batch: 30000 Loss: 0.48332706093788147\n",
      "______________________________________\n",
      "Epoch Loss: 783.6254730522633\n",
      "Test Jumbled sentence: ['looks', 'boats', 'below', 'boy', 'many', 'railing', 'in', 'the', 'at', 'a', 'rafts', '.', 'over', 'the', 'a', 'water', 'and', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'boy', 'looks', 'over', 'a', 'railing', 'at', 'the', 'many', 'boats', 'and', 'rafts', 'below', 'in', 'the', 'water', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'boy', 'looks', 'at', 'the', 'water', 'at', 'the', 'water', 'and', 'a', 'boy', 'in', 'a', 'in', 'it', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 4 Batch: 0 Loss: 0.47634950280189514\n",
      "Epoch: 4 Batch: 6000 Loss: 0.6760522723197937\n",
      "Epoch: 4 Batch: 12000 Loss: 0.8621684312820435\n",
      "Epoch: 4 Batch: 18000 Loss: 0.8106586337089539\n",
      "Epoch: 4 Batch: 24000 Loss: 0.48037800192832947\n",
      "Epoch: 4 Batch: 30000 Loss: 0.4647257924079895\n",
      "______________________________________\n",
      "Epoch Loss: 735.1351964771748\n",
      "Test Jumbled sentence: ['hole', 'person', 'a', 'the', 'in', 'snow', 'drilling', 'in', 'the', 'ice', 'a', '.', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'person', 'in', 'the', 'snow', 'drilling', 'a', 'hole', 'in', 'the', 'ice', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'person', 'enjoys', 'the', 'snow', 'in', 'a', 'cow', 'in', 'a', 'snow', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 5 Batch: 0 Loss: 0.38599148392677307\n",
      "Epoch: 5 Batch: 6000 Loss: 0.8667786121368408\n",
      "Epoch: 5 Batch: 12000 Loss: 0.960740327835083\n",
      "Epoch: 5 Batch: 18000 Loss: 1.2272874116897583\n",
      "Epoch: 5 Batch: 24000 Loss: 0.4649972915649414\n",
      "Epoch: 5 Batch: 30000 Loss: 0.6543509364128113\n",
      "______________________________________\n",
      "Epoch Loss: 700.3577381074429\n",
      "Test Jumbled sentence: ['a', 'in', 'play', 'two', '.', 'boys', 'puddle', '', '<eos>']\n",
      "Test Unjumbled sentence: ['two', 'boys', 'play', 'in', 'a', 'puddle', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['two', 'boys', 'play', 'in', 'a', 'puddle', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 6 Batch: 0 Loss: 0.6549103260040283\n",
      "Epoch: 6 Batch: 6000 Loss: 0.9398388862609863\n",
      "Epoch: 6 Batch: 12000 Loss: 0.7636210918426514\n",
      "Epoch: 6 Batch: 18000 Loss: 0.8830170035362244\n",
      "Epoch: 6 Batch: 24000 Loss: 0.5958391427993774\n",
      "Epoch: 6 Batch: 30000 Loss: 0.3659863770008087\n",
      "______________________________________\n",
      "Epoch Loss: 672.9181762635708\n",
      "Test Jumbled sentence: ['people', 'of', '.', 'beach', 'crowd', 'the', 'at', '', '<eos>']\n",
      "Test Unjumbled sentence: ['crowd', 'of', 'people', 'at', 'the', 'beach', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['crowd', 'of', 'people', 'at', 'the', 'beach', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 7 Batch: 0 Loss: 0.361431747674942\n",
      "Epoch: 7 Batch: 6000 Loss: 0.6469243764877319\n",
      "Epoch: 7 Batch: 12000 Loss: 0.6102194786071777\n",
      "Epoch: 7 Batch: 18000 Loss: 0.560776948928833\n",
      "Epoch: 7 Batch: 24000 Loss: 0.5167667269706726\n",
      "Epoch: 7 Batch: 30000 Loss: 0.5233868956565857\n",
      "______________________________________\n",
      "Epoch Loss: 634.3083892166615\n",
      "Test Jumbled sentence: ['people', 'of', '.', 'beach', 'crowd', 'the', 'at', '', '<eos>']\n",
      "Test Unjumbled sentence: ['crowd', 'of', 'people', 'at', 'the', 'beach', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['crowd', 'of', 'people', 'at', 'the', 'beach', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 8 Batch: 0 Loss: 0.5120754837989807\n",
      "Epoch: 8 Batch: 6000 Loss: 0.7315385937690735\n",
      "Epoch: 8 Batch: 12000 Loss: 0.602777361869812\n",
      "Epoch: 8 Batch: 18000 Loss: 0.7510891556739807\n",
      "Epoch: 8 Batch: 24000 Loss: 0.3512943387031555\n",
      "Epoch: 8 Batch: 30000 Loss: 0.4899606704711914\n",
      "______________________________________\n",
      "Epoch Loss: 605.7931466847658\n",
      "Test Jumbled sentence: ['front', 'in', 'standing', 'man', 'skyscraper', 'is', 'of', 'a', 'a', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'man', 'is', 'standing', 'in', 'front', 'of', 'a', 'skyscraper', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'man', 'is', 'standing', 'in', 'front', 'of', 'a', 'garage', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 9 Batch: 0 Loss: 0.5625945329666138\n",
      "Epoch: 9 Batch: 6000 Loss: 0.4885024428367615\n",
      "Epoch: 9 Batch: 12000 Loss: 0.4241746962070465\n",
      "Epoch: 9 Batch: 18000 Loss: 0.5543568730354309\n",
      "Epoch: 9 Batch: 24000 Loss: 0.5819146633148193\n",
      "Epoch: 9 Batch: 30000 Loss: 0.5163951516151428\n",
      "______________________________________\n",
      "Epoch Loss: 573.309942573309\n",
      "Test Jumbled sentence: ['a', 'a', 'climbing', 'man', 'wall', 'rock', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'man', 'climbing', 'a', 'rock', 'wall', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'man', 'climbing', 'a', 'rock', 'wall', '', '<eos>']\n",
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for j in range(0,len(X_encoder_indices_tr),batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        Xe_b = torch.tensor(X_encoder_indices_tr[j:j+batch_size]).to(device)\n",
    "        Xd_b = torch.tensor(X_decoder_indices_tr[j:j+batch_size]).to(device)\n",
    "        Y_b = torch.tensor(Y_indices_tr[j:j+batch_size]).to(device)\n",
    "        op = model(Xe_b,Xd_b)\n",
    "        loss = criterion(op,Y_b.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        if j%2000 == 0: print(\"Epoch:\",i,\"Batch:\",j,\"Loss:\",batch_loss)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Epoch Loss:\",epoch_loss)\n",
    "    \n",
    "    # Randomly select one sentence from Test Data to Predict.\n",
    "    data_index = random.randint(0,100)\n",
    "    # Since its only 1 sentence, we need to convert into a 2-D list before sending it to torch.tensor()\n",
    "    Xe_b = torch.tensor([X_encoder_indices_test[data_index]]).to(device)\n",
    "    print(\"Test Jumbled sentence:\",X_encoder_tokens_test[data_index])\n",
    "    print(\"Test Unjumbled sentence:\", Y_tokens_test[data_index]) \n",
    "    predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length)\n",
    "    print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
