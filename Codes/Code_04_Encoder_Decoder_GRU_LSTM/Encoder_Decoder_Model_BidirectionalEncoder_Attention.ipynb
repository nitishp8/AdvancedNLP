{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> In this notebook, we will try to unjumble a sentence using Encoder-Decoder Architecture built using <br><br>\n",
    " Recurrent Networks like GRU, LSTM and Bi-directional LSTMs.</h6>\n",
    "<h6> The Data is located here: ../../Datasets/Jumble_Unjumble/ </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/Train_400.tsv\",sep=\"\\t\")\n",
    "# test_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/Test_100.tsv\",sep=\"\\t\")\n",
    "# print(train_df.shape, test_df.shape)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32368, 2) (8092, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumbled_sentences</th>\n",
       "      <th>unjumbled_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32760</th>\n",
       "      <td>tools and a man gardening inside two holding a...</td>\n",
       "      <td>a man and two women are inside a greenhouse ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31413</th>\n",
       "      <td>meandering at people of the walkway stand . up...</td>\n",
       "      <td>people stand at the bottom of a meandering wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>standing a rock . on man view the shorts a out...</td>\n",
       "      <td>a man in shorts is standing on a rock looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28232</th>\n",
       "      <td>to a a in on shirt little red a holds pole nea...</td>\n",
       "      <td>a little girl in a red shirt holds on to a pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28438</th>\n",
       "      <td>children two in &lt;unk&gt; play the melting .</td>\n",
       "      <td>two children play in the melting &lt;unk&gt; .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       jumbled_sentences  \\\n",
       "32760  tools and a man gardening inside two holding a...   \n",
       "31413  meandering at people of the walkway stand . up...   \n",
       "4325   standing a rock . on man view the shorts a out...   \n",
       "28232  to a a in on shirt little red a holds pole nea...   \n",
       "28438          children two in <unk> play the melting .    \n",
       "\n",
       "                                     unjumbled_sentences  \n",
       "32760  a man and two women are inside a greenhouse ho...  \n",
       "31413  people stand at the bottom of a meandering wal...  \n",
       "4325   a man in shorts is standing on a rock looking ...  \n",
       "28232  a little girl in a red shirt holds on to a pol...  \n",
       "28438          two children play in the melting <unk> .   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jumbled_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/processed_jumbled.txt\",sep=\"\\t\",header=None)\n",
    "unjumbled_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/processed_unjumbled.txt\",sep=\"\\t\",header=None)\n",
    "jumbled_df.columns = [\"jumbled_sentences\"]\n",
    "unjumbled_df.columns = [\"unjumbled_sentences\"]\n",
    "df = pd.concat([jumbled_df,unjumbled_df],axis=1)\n",
    "train_df = df.sample(frac=0.8, random_state=42) \n",
    "test_df = df.drop(train_df.index)\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocab using jumbled_sentences of Train + Test dataset. Ideally only Train dataset should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabBuilder:\n",
    "    def __init__(self,text_corpus,unknown_token=None,pad_token=None,sos_token=None,eos_token=None):\n",
    "        '''\n",
    "        text_corpus = \"This is first sentence. This is second sentence. This is another sentence\"\n",
    "        '''\n",
    "        self.text_corpus = text_corpus\n",
    "        self.unknown_token = unknown_token or \"<unk>\"\n",
    "        self.pad_token = pad_token or \"<pad>\"\n",
    "        self.sos_token = sos_token or \"<sos>\"\n",
    "        self.eos_token = eos_token or \"<eos>\"\n",
    "        self.word_to_index, self.index_to_word = self.get_vocabs()\n",
    "                        \n",
    "    def get_vocabs(self):\n",
    "        word_to_index = {}\n",
    "        index_count = 0\n",
    "        all_unique_words = set(self.text_corpus.split(\" \"))\n",
    "        for index, word in enumerate(all_unique_words):\n",
    "            word_to_index[word] = index\n",
    "        if self.pad_token not in word_to_index: word_to_index[self.pad_token] = index + 1\n",
    "        if self.sos_token not in word_to_index: word_to_index[self.sos_token] = index + 2\n",
    "        if self.eos_token not in word_to_index: word_to_index[self.eos_token] = index + 3\n",
    "        if self.unknown_token not in word_to_index: word_to_index[self.unknown_token] = index + 4\n",
    "        index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "        return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tools and a man gardening inside two holding are . women a greenhouse  meandering at people of the walkway stand . uphill that bottom goes a  standing a rock . on man view the shorts a out from lookin\n"
     ]
    }
   ],
   "source": [
    "text_corpus_1 = \" \".join(train_df[\"jumbled_sentences\"].tolist())\n",
    "text_corpus_2 = \" \".join(test_df[\"jumbled_sentences\"].tolist())\n",
    "text_corpus = text_corpus_1 + \" \" + text_corpus_2\n",
    "print(text_corpus[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordToIndex Dict length: 5242\n",
      "IndexToWord Dict length: 5242\n"
     ]
    }
   ],
   "source": [
    "unknown_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "vocab_builder = VocabBuilder(text_corpus,unknown_token,pad_token,sos_token,eos_token)\n",
    "print(\"WordToIndex Dict length:\",len(vocab_builder.word_to_index))\n",
    "print(\"IndexToWord Dict length:\",len(vocab_builder.index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X_Encoder, X_Decoder and Y\n",
    "X_encoder is the matrix of words in jumbled_sentences, each sentence suffixed by \"eos\" token <br>\n",
    "X_decoder is the matrix of unjumbled_sentences, each sentence prefixed by \"sos\" token <br>\n",
    "Y is the matrix of unjumbled_sentences, each sentence suffixed by \"eos\" token <br>\n",
    "\n",
    "Do this for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 32368\n",
      "X Encoder test length: 8092\n",
      "Sample X_encoder_train: ['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.', 'women', 'a', 'greenhouse', '', '<eos>']\n",
      "Sample X_decoder_train: ['<sos>', 'a', 'man', 'and', 'two', 'women', 'are', 'inside', 'a', 'greenhouse', 'holding', 'gardening', 'tools', '.', '']\n",
      "Sample Y_train: ['a', 'man', 'and', 'two', 'women', 'are', 'inside', 'a', 'greenhouse', 'holding', 'gardening', 'tools', '.', '', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "def get_Xe_Xd_Y(dataframe, sos_token, eos_token):\n",
    "    jumbled_sentences = dataframe[\"jumbled_sentences\"].tolist()\n",
    "    unjumbled_sentences = dataframe[\"unjumbled_sentences\"].tolist()\n",
    "    X_encoder_words = [el.split(\" \") + [eos_token] for el in jumbled_sentences]\n",
    "    X_decoder_words = [[sos_token] + el.split(\" \") for el in unjumbled_sentences]\n",
    "    Y_words = [el.split(\" \") + [eos_token] for el in unjumbled_sentences]\n",
    "    return X_encoder_words, X_decoder_words, Y_words\n",
    "\n",
    "X_encoder_words_tr, X_decoder_words_tr, Y_words_tr = get_Xe_Xd_Y(train_df, sos_token, eos_token)\n",
    "X_encoder_words_test, X_decoder_words_test, Y_words_test = get_Xe_Xd_Y(test_df, sos_token, eos_token)\n",
    "print(\"X Encoder train length:\",len(X_encoder_words_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_words_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_words_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_words_tr[0])\n",
    "print(\"Sample Y_train:\",Y_words_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map X_encoder, X_decoder and Y using Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Index_Mapper:\n",
    "    def __init__(self,word_to_index,index_to_word, unknown_token):\n",
    "        self.word_to_index = word_to_index\n",
    "        self.index_to_word = index_to_word\n",
    "        self.unknown_token = unknown_token\n",
    "    \n",
    "    def get_encoding(self,sentence):\n",
    "        '''\n",
    "        sentence must be a list of words.\n",
    "        Ex: [\"Climate\",\"change\",\"is\",\"a\",\"pressing\",\"global\",\"issue\"]\n",
    "        '''\n",
    "        encoded_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in self.word_to_index: encoded_sentence.append(self.word_to_index[word])\n",
    "            else: encoded_sentence.append(self.word_to_index[self.unknown_token])\n",
    "        return encoded_sentence\n",
    "    \n",
    "    def get_decoding(self,encoded_sentence):\n",
    "        '''\n",
    "        encoded_sentence must be a list of vocab indices.\n",
    "        Ex: encoded_sentence = [24,21,4,1,..] \n",
    "        '''\n",
    "        sentence = [self.index_to_word[index] for index in encoded_sentence]\n",
    "        return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_words_to_indices(word_index_mapper, max_sequence_length, word_matrix):\n",
    "    index_matrix = []\n",
    "    for el in word_matrix:\n",
    "        el = el[:max_sequence_length]\n",
    "        if len(el) < max_sequence_length:\n",
    "            pad_tokens_to_append = max_sequence_length - len(el)\n",
    "            el = el + [pad_token]*pad_tokens_to_append\n",
    "        index_matrix.append(word_index_mapper.get_encoding(el))\n",
    "    return index_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 32368\n",
      "X Encoder test length: 8092\n",
      "Sample X_encoder_train: [857, 3489, 4223, 4354, 1988, 2041, 3618, 58, 5033, 3732, 412, 4223, 2350, 0, 5241, 5239, 5239, 5239, 5239, 5239, 5239, 5239, 5239, 5239, 5239]\n",
      "Sample X_decoder_train: [5240, 4223, 4354, 3489, 3618, 412, 5033, 2041, 4223, 2350, 58, 1988, 857, 3732, 0, 5239, 5239, 5239, 5239, 5239, 5239, 5239, 5239, 5239, 5239]\n",
      "Sample Y_train: [4223, 4354, 3489, 3618, 412, 5033, 2041, 4223, 2350, 58, 1988, 857, 3732, 0, 5241, 5239, 5239, 5239, 5239, 5239, 5239, 5239, 5239, 5239, 5239]\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 25\n",
    "word_index_mapper = Word_Index_Mapper(vocab_builder.word_to_index, vocab_builder.index_to_word, unknown_token)\n",
    "X_encoder_indices_tr = map_words_to_indices(word_index_mapper, max_sequence_length, X_encoder_words_tr)\n",
    "X_decoder_indices_tr = map_words_to_indices(word_index_mapper, max_sequence_length, X_decoder_words_tr)\n",
    "Y_indices_tr = map_words_to_indices(word_index_mapper, max_sequence_length, Y_words_tr)\n",
    "X_encoder_indices_test = map_words_to_indices(word_index_mapper, max_sequence_length, X_encoder_words_test)\n",
    "X_decoder_indices_test = map_words_to_indices(word_index_mapper, max_sequence_length, X_decoder_words_test)\n",
    "Y_indices_test = map_words_to_indices(word_index_mapper, max_sequence_length, Y_words_test)\n",
    "print(\"X Encoder train length:\",len(X_encoder_indices_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_indices_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_indices_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_indices_tr[0])\n",
    "print(\"Sample Y_train:\",Y_indices_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unknown token statistics - must be 0, as both Train and Test dataset has been used for vocab creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741\n",
      "<sos> a small child grips onto the red ropes at the playground .  <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "X_test_temp = []\n",
    "unknown_token_counts = 0\n",
    "for el in X_decoder_indices_test:\n",
    "    temp_list = word_index_mapper.get_decoding(el)\n",
    "    unknown_token_counts += temp_list.count(unknown_token)\n",
    "    X_test_temp.append(temp_list)\n",
    "print(unknown_token_counts)\n",
    "print(X_test_temp[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout, debug):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.debug = debug\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True, bidirectional = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        dropped_out = self.dropout(embedded) \n",
    "        output, hidden = self.gru(dropped_out)\n",
    "        hidden_concatenated = torch.cat((hidden[-1],hidden[-2]),axis=1)\n",
    "        hidden_unsqueezed = hidden_concatenated.unsqueeze(0)\n",
    "        if self.debug: \n",
    "            print(\"-----------Encoder----------:\")\n",
    "            print(\"Input Data shape:\",x.shape)\n",
    "            print(\"After Embedding Layer:\",embedded.shape)\n",
    "            print(\"After Dropout Layer:\",dropped_out.shape)\n",
    "            print(\"Outputs and hidden shape from GRU:\",output.shape, hidden.shape)\n",
    "            print(\"Concatenated hidden shape:\", hidden_concatenated.shape)\n",
    "            print(\"Unsqueezed hidden shape:\", hidden_unsqueezed.shape)\n",
    "        return output, hidden_unsqueezed\n",
    "\n",
    "class BahadnauAttention(nn.Module):\n",
    "    def __init__(self,attention_neurons,debug):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.rnn = nn.RNNCell(input_size=attention_neurons,hidden_size=attention_neurons,bias=False)\n",
    "        self.linear = nn.Linear(in_features = attention_neurons, out_features = 1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "    \n",
    "    def forward(self,op_from_enoder,st_minus_one_from_decoder):\n",
    "        \n",
    "        # Reshape the op_from_enoder from (batch_size,seq_length,lstm_neurons) to (batch_size*seq_length,lstm_neurons)\n",
    "        # And reshape st_minus_one_from_decoder from (1,batch_size,lstm_neurons) to (batch_size,lstm_neurons)\n",
    "        # And repeat st_minus_one_from_decoder to seq_length times to get (batch_size*seq_length,lstm_neurons)\n",
    "        if self.debug: \n",
    "            print(\"_______________________________\")\n",
    "            print(\"\\t\\tAttention\\t\\t\")\n",
    "            print(\"_______________________________\")\n",
    "        seq_length = op_from_enoder.shape[1]\n",
    "        op_from_enoder = op_from_enoder.reshape(-1,op_from_enoder.shape[2])\n",
    "        st_minus_one_from_decoder = st_minus_one_from_decoder[-1]\n",
    "        st_minus_one_from_decoder = st_minus_one_from_decoder.repeat(seq_length,1)\n",
    "        if self.debug: print(\"Shape of op_from_encoder:\",op_from_enoder.shape,\n",
    "                             \"Shape of st_minus_one_from_decoder:\",st_minus_one_from_decoder.shape)\n",
    "            \n",
    "        rnn_op = self.rnn(op_from_enoder,st_minus_one_from_decoder)\n",
    "        if self.debug: print(\"RNN Cell Op:\",rnn_op.shape)\n",
    "            \n",
    "        linear_op = self.linear(rnn_op)\n",
    "        if self.debug: print(\"Linear Op:\",linear_op.shape)\n",
    "            \n",
    "        softmax_op = self.softmax(linear_op)\n",
    "        if self.debug: print(\"Softmax Op:\",softmax_op.shape)\n",
    "            \n",
    "        ct = torch.sum(torch.mul(op_from_enoder,softmax_op),dim=0).unsqueeze(0)\n",
    "        if self.debug: print(\"Weighted Averaged h vectors:\",ct.shape)\n",
    "        \n",
    "        return ct,softmax_op\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, model_attention, output_dim, emb_dim, hid_dim, debug):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.debug = debug\n",
    "        self.model_attention = model_attention\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim,  batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, encoder_hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded, encoder_hidden)\n",
    "        reshaped_output = output.reshape(-1,output.shape[2])\n",
    "        prediction = self.fc_out(reshaped_output)\n",
    "        if self.debug: \n",
    "            print(\"-----------Decoder----------:\")\n",
    "            print(\"Input Data shape, X:\",x.shape, \", Encoder hidden state:\", encoder_hidden.shape)\n",
    "            print(\"After Embedding Layer:\",embedded.shape)\n",
    "            print(\"Outputs and hidden shape from GRU:\",output.shape, hidden.shape)\n",
    "            print(\"Reshaped Output:\", reshaped_output.shape)\n",
    "            print(\"After FC layer:\", prediction.shape)\n",
    "        return prediction, hidden\n",
    "    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__() \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input, teacher_forcing_ratio=0.5): \n",
    "        encoder_outputs, encoder_hidden = self.encoder(encoder_input) \n",
    "        outputs, _ = self.decoder(decoder_input,encoder_hidden)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X_encoder_indices_test, X_decoder_indices_test, \n",
    "            X_encoder_words_test, X_decoder_words_test,\n",
    "            word_index_mapper, device):\n",
    "    data_index = random.randint(0,100)\n",
    "    Xe_b = torch.tensor([X_encoder_indices_test[data_index]]).to(device)\n",
    "    print(data_index)\n",
    "    print(X_encoder_words_test[data_index])\n",
    "    print(X_decoder_words_test[data_index])\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ht,ht_for_decoder = model.encoder(Xe_b)\n",
    "        sos_word = torch.tensor([[word_index_mapper.word_to_index[\"<sos>\"]]]).to(device)\n",
    "        op,ht = model.decoder(sos_word,ht_for_decoder)\n",
    "        unjumbled_sentence = []\n",
    "        for i in range(25):\n",
    "            predicted_word = torch.argmax(op,axis=1).tolist()\n",
    "#             print(\"Predicted .....................\",predicted_word)\n",
    "            if predicted_word[0] == word_index_mapper.word_to_index[\"<eos>\"]: break\n",
    "            unjumbled_sentence.append(word_index_mapper.index_to_word[predicted_word[0]])\n",
    "            op,ht = model.decoder(torch.tensor([predicted_word]).to(device),ht)\n",
    "        print(\"_______________________________________\")\n",
    "        print(unjumbled_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Training And Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(word_index_mapper.word_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(word_index_mapper.word_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 256 \n",
    "DEC_EMB_DIM = 256 \n",
    "HID_DIM = 512 \n",
    "N_LAYERS = 2 \n",
    "ENC_DROPOUT = 0.5 \n",
    "DEC_DROPOUT = 0.5\n",
    "device = \"cpu\"\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT, debug = True) \n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM*2, debug = True) \n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters()) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_index_mapper.word_to_index[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 25]) torch.Size([5, 25]) torch.Size([5, 25])\n",
      "-----------Encoder----------:\n",
      "Input Data shape: torch.Size([5, 25])\n",
      "After Embedding Layer: torch.Size([5, 25, 256])\n",
      "After Dropout Layer: torch.Size([5, 25, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 25, 1024]) torch.Size([2, 5, 512])\n",
      "Concatenated hidden shape: torch.Size([5, 1024])\n",
      "Unsqueezed hidden shape: torch.Size([1, 5, 1024])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 25]) , Encoder hidden state: torch.Size([1, 5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 25, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 25, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([125, 1024])\n",
      "After FC layer: torch.Size([125, 5242])\n",
      "---------------------------------------------\n",
      "torch.Size([5, 25]) torch.Size([5, 25]) torch.Size([5, 25])\n",
      "-----------Encoder----------:\n",
      "Input Data shape: torch.Size([5, 25])\n",
      "After Embedding Layer: torch.Size([5, 25, 256])\n",
      "After Dropout Layer: torch.Size([5, 25, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 25, 1024]) torch.Size([2, 5, 512])\n",
      "Concatenated hidden shape: torch.Size([5, 1024])\n",
      "Unsqueezed hidden shape: torch.Size([1, 5, 1024])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 25]) , Encoder hidden state: torch.Size([1, 5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 25, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 25, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([125, 1024])\n",
      "After FC layer: torch.Size([125, 5242])\n"
     ]
    }
   ],
   "source": [
    "data_index = 6\n",
    "batch_size = 5\n",
    "model.train()\n",
    "\n",
    "optimizer.zero_grad()\n",
    "Xe_b = torch.tensor(X_encoder_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Xd_b = torch.tensor(X_decoder_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Y_b = torch.tensor(Y_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "\n",
    "output = model(Xe_b, Xd_b)\n",
    "loss = criterion(output, Y_b.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "optimizer.zero_grad()\n",
    "Xe_b = torch.tensor(X_encoder_indices_tr[data_index+1:data_index+batch_size+1]).to(device)\n",
    "Xd_b = torch.tensor(X_decoder_indices_tr[data_index+1:data_index+batch_size+1]).to(device)\n",
    "Y_b = torch.tensor(Y_indices_tr[data_index+1:data_index+batch_size+1]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "\n",
    "output = model(Xe_b, Xd_b)\n",
    "loss = criterion(output, Y_b.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "['a', 'jumping', 'a', 'water', 'boy', 'on', '<unk>', '', '<eos>']\n",
      "['<sos>', 'a', 'boy', 'jumping', 'on', 'a', 'water', '<unk>', '']\n",
      "-----------Encoder----------:\n",
      "Input Data shape: torch.Size([1, 25])\n",
      "After Embedding Layer: torch.Size([1, 25, 256])\n",
      "After Dropout Layer: torch.Size([1, 25, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([1, 25, 1024]) torch.Size([2, 1, 512])\n",
      "Concatenated hidden shape: torch.Size([1, 1024])\n",
      "Unsqueezed hidden shape: torch.Size([1, 1, 1024])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([1, 1]) , Encoder hidden state: torch.Size([1, 1, 1024])\n",
      "After Embedding Layer: torch.Size([1, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024])\n",
      "Reshaped Output: torch.Size([1, 1024])\n",
      "After FC layer: torch.Size([1, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([1, 1]) , Encoder hidden state: torch.Size([1, 1, 1024])\n",
      "After Embedding Layer: torch.Size([1, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024])\n",
      "Reshaped Output: torch.Size([1, 1024])\n",
      "After FC layer: torch.Size([1, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([1, 1]) , Encoder hidden state: torch.Size([1, 1, 1024])\n",
      "After Embedding Layer: torch.Size([1, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024])\n",
      "Reshaped Output: torch.Size([1, 1024])\n",
      "After FC layer: torch.Size([1, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([1, 1]) , Encoder hidden state: torch.Size([1, 1, 1024])\n",
      "After Embedding Layer: torch.Size([1, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024])\n",
      "Reshaped Output: torch.Size([1, 1024])\n",
      "After FC layer: torch.Size([1, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([1, 1]) , Encoder hidden state: torch.Size([1, 1, 1024])\n",
      "After Embedding Layer: torch.Size([1, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024])\n",
      "Reshaped Output: torch.Size([1, 1024])\n",
      "After FC layer: torch.Size([1, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([1, 1]) , Encoder hidden state: torch.Size([1, 1, 1024])\n",
      "After Embedding Layer: torch.Size([1, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024])\n",
      "Reshaped Output: torch.Size([1, 1024])\n",
      "After FC layer: torch.Size([1, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([1, 1]) , Encoder hidden state: torch.Size([1, 1, 1024])\n",
      "After Embedding Layer: torch.Size([1, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024])\n",
      "Reshaped Output: torch.Size([1, 1024])\n",
      "After FC layer: torch.Size([1, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([1, 1]) , Encoder hidden state: torch.Size([1, 1, 1024])\n",
      "After Embedding Layer: torch.Size([1, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024])\n",
      "Reshaped Output: torch.Size([1, 1024])\n",
      "After FC layer: torch.Size([1, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([1, 1]) , Encoder hidden state: torch.Size([1, 1, 1024])\n",
      "After Embedding Layer: torch.Size([1, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024])\n",
      "Reshaped Output: torch.Size([1, 1024])\n",
      "After FC layer: torch.Size([1, 5242])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([1, 1]) , Encoder hidden state: torch.Size([1, 1, 1024])\n",
      "After Embedding Layer: torch.Size([1, 1, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024])\n",
      "Reshaped Output: torch.Size([1, 1024])\n",
      "After FC layer: torch.Size([1, 5242])\n",
      "_______________________________________\n",
      "['a', 'a', 'a', 'a', 'a', 'the', 'the', '.', '']\n"
     ]
    }
   ],
   "source": [
    "predict(model,X_encoder_indices_test,X_decoder_indices_test, X_encoder_words_test, X_decoder_words_test, word_index_mapper, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" #torch.device(\"cuda:0\")\n",
    "batch_size = 50\n",
    "INPUT_DIM = len(word_index_mapper.word_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(word_index_mapper.word_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 128 \n",
    "DEC_EMB_DIM = 128 \n",
    "HID_DIM = 250 \n",
    "N_LAYERS = 2 \n",
    "ENC_DROPOUT = 0.5 \n",
    "DEC_DROPOUT = 0.5\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT, debug = False) \n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM*2, debug = False) \n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters()) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_index_mapper.word_to_index[\"<pad>\"])\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 Loss: 8.595221519470215\n",
      "Epoch: 0 Batch: 1000 Loss: 4.545675277709961\n",
      "Epoch: 0 Batch: 2000 Loss: 4.309252738952637\n",
      "Epoch: 0 Batch: 3000 Loss: 4.02888298034668\n",
      "Epoch: 0 Batch: 4000 Loss: 3.6883935928344727\n",
      "Epoch: 0 Batch: 5000 Loss: 3.4635846614837646\n",
      "Epoch: 0 Batch: 6000 Loss: 3.6870267391204834\n",
      "Epoch: 0 Batch: 7000 Loss: 3.6009042263031006\n",
      "Epoch: 0 Batch: 8000 Loss: 3.0374643802642822\n",
      "Epoch: 0 Batch: 9000 Loss: 3.26840877532959\n",
      "Epoch: 0 Batch: 10000 Loss: 3.410982370376587\n",
      "Epoch: 0 Batch: 11000 Loss: 2.937910318374634\n",
      "Epoch: 0 Batch: 12000 Loss: 3.0410337448120117\n",
      "Epoch: 0 Batch: 13000 Loss: 2.9135076999664307\n",
      "Epoch: 0 Batch: 14000 Loss: 2.733027458190918\n",
      "Epoch: 0 Batch: 15000 Loss: 2.8130998611450195\n",
      "Epoch: 0 Batch: 16000 Loss: 2.8506321907043457\n",
      "Epoch: 0 Batch: 17000 Loss: 2.542210578918457\n",
      "Epoch: 0 Batch: 18000 Loss: 2.770841598510742\n",
      "Epoch: 0 Batch: 19000 Loss: 2.5976221561431885\n",
      "Epoch: 0 Batch: 20000 Loss: 2.3566253185272217\n",
      "Epoch: 0 Batch: 21000 Loss: 2.632746934890747\n",
      "Epoch: 0 Batch: 22000 Loss: 2.5321524143218994\n",
      "Epoch: 0 Batch: 23000 Loss: 2.4075348377227783\n",
      "Epoch: 0 Batch: 24000 Loss: 2.679337978363037\n",
      "Epoch: 0 Batch: 25000 Loss: 2.36570143699646\n",
      "Epoch: 0 Batch: 26000 Loss: 2.56558895111084\n",
      "Epoch: 0 Batch: 27000 Loss: 2.6155505180358887\n",
      "Epoch: 0 Batch: 28000 Loss: 2.2333970069885254\n",
      "Epoch: 0 Batch: 29000 Loss: 2.4358339309692383\n",
      "Epoch: 0 Batch: 30000 Loss: 2.31803035736084\n",
      "Epoch: 0 Batch: 31000 Loss: 2.0983681678771973\n",
      "Epoch: 0 Batch: 32000 Loss: 2.0771164894104004\n",
      "______________________________________\n",
      "Epoch Loss: 1936.92684340477\n",
      "57\n",
      "['a', 'down', '.', 'miniature', 'boy', 'little', 'a', 'on', 'a', 'a', 'bike', 'dirt', 'bike', 'rides', 'hill', '', '<eos>']\n",
      "['<sos>', 'a', 'little', 'boy', 'rides', 'a', 'bike', 'down', 'a', 'hill', 'on', 'a', 'miniature', 'dirt', 'bike', '.', '']\n",
      "_______________________________________\n",
      "['a', 'boy', 'does', 'a', 'trick', 'on', 'a', 'skateboard', 'trick', 'on', 'a', 'skateboard', '.', '']\n",
      "_______________________________________\n",
      "Epoch: 1 Batch: 0 Loss: 2.313316822052002\n",
      "Epoch: 1 Batch: 1000 Loss: 2.145298719406128\n",
      "Epoch: 1 Batch: 2000 Loss: 2.3419694900512695\n",
      "Epoch: 1 Batch: 3000 Loss: 2.179781913757324\n",
      "Epoch: 1 Batch: 4000 Loss: 2.0888872146606445\n",
      "Epoch: 1 Batch: 5000 Loss: 1.9526437520980835\n",
      "Epoch: 1 Batch: 6000 Loss: 2.2494494915008545\n",
      "Epoch: 1 Batch: 7000 Loss: 2.2032358646392822\n",
      "Epoch: 1 Batch: 8000 Loss: 1.8401328325271606\n",
      "Epoch: 1 Batch: 9000 Loss: 2.0976133346557617\n",
      "Epoch: 1 Batch: 10000 Loss: 2.327033519744873\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "#     init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for j in range(0,len(X_encoder_indices_tr),batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        Xe_b = torch.tensor(X_encoder_indices_tr[j:j+batch_size]).to(device)\n",
    "        Xd_b = torch.tensor(X_decoder_indices_tr[j:j+batch_size]).to(device)\n",
    "        Y_b = torch.tensor(Y_indices_tr[j:j+batch_size]).to(device)\n",
    "        op = model(Xe_b,Xd_b)\n",
    "        loss = criterion(op,Y_b.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        if j%1000 == 0: print(\"Epoch:\",i,\"Batch:\",j,\"Loss:\",batch_loss)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Epoch Loss:\",epoch_loss)\n",
    "    predict(model,X_encoder_indices_test,X_decoder_indices_test, \n",
    "            X_encoder_words_test, X_decoder_words_test, word_index_mapper, device)\n",
    "    print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
