{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GRU Based Encoder-Decoder - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32368, 2) (8092, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumbled_sentences</th>\n",
       "      <th>unjumbled_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32760</th>\n",
       "      <td>tools and a man gardening inside two holding a...</td>\n",
       "      <td>a man and two women are inside a greenhouse ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31413</th>\n",
       "      <td>meandering at people of the walkway stand . up...</td>\n",
       "      <td>people stand at the bottom of a meandering wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>standing a rock . on man view the shorts a out...</td>\n",
       "      <td>a man in shorts is standing on a rock looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28232</th>\n",
       "      <td>to a a in on shirt little red a holds pole nea...</td>\n",
       "      <td>a little girl in a red shirt holds on to a pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28438</th>\n",
       "      <td>children two in &lt;unk&gt; play the melting .</td>\n",
       "      <td>two children play in the melting &lt;unk&gt; .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       jumbled_sentences  \\\n",
       "32760  tools and a man gardening inside two holding a...   \n",
       "31413  meandering at people of the walkway stand . up...   \n",
       "4325   standing a rock . on man view the shorts a out...   \n",
       "28232  to a a in on shirt little red a holds pole nea...   \n",
       "28438          children two in <unk> play the melting .    \n",
       "\n",
       "                                     unjumbled_sentences  \n",
       "32760  a man and two women are inside a greenhouse ho...  \n",
       "31413  people stand at the bottom of a meandering wal...  \n",
       "4325   a man in shorts is standing on a rock looking ...  \n",
       "28232  a little girl in a red shirt holds on to a pol...  \n",
       "28438          two children play in the melting <unk> .   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jumbled_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/processed_jumbled.txt\",sep=\"\\t\",header=None)\n",
    "unjumbled_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/processed_unjumbled.txt\",sep=\"\\t\",header=None)\n",
    "jumbled_df.columns = [\"jumbled_sentences\"]\n",
    "unjumbled_df.columns = [\"unjumbled_sentences\"]\n",
    "df = pd.concat([jumbled_df,unjumbled_df],axis=1)\n",
    "train_df = df.sample(frac=0.8, random_state=42) \n",
    "test_df = df.drop(train_df.index)\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Train and Test Data\n",
    "1. Lowercasing, removing stopwords. <br>\n",
    "2. Stemming, Lemmetization. <br>\n",
    "3. Tokenization. <br>\n",
    "4. Here, we are just doing tokenization by splitting on space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenize_on = \" \"\n",
    "    \n",
    "    def tokenize(self,text_string):\n",
    "        '''\n",
    "        text_string = \"This is one sentence.\"\n",
    "        returns token_list = [\"This\",\"is\",\"one\",\"sentence.\"]\n",
    "        '''\n",
    "        token_list = text_string.split(self.tokenize_on)\n",
    "        return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32368, 2) (8092, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumbled_sentences</th>\n",
       "      <th>unjumbled_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32760</th>\n",
       "      <td>[tools, and, a, man, gardening, inside, two, h...</td>\n",
       "      <td>[a, man, and, two, women, are, inside, a, gree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31413</th>\n",
       "      <td>[meandering, at, people, of, the, walkway, sta...</td>\n",
       "      <td>[people, stand, at, the, bottom, of, a, meande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>[standing, a, rock, ., on, man, view, the, sho...</td>\n",
       "      <td>[a, man, in, shorts, is, standing, on, a, rock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28232</th>\n",
       "      <td>[to, a, a, in, on, shirt, little, red, a, hold...</td>\n",
       "      <td>[a, little, girl, in, a, red, shirt, holds, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28438</th>\n",
       "      <td>[children, two, in, &lt;unk&gt;, play, the, melting,...</td>\n",
       "      <td>[two, children, play, in, the, melting, &lt;unk&gt;,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       jumbled_sentences  \\\n",
       "32760  [tools, and, a, man, gardening, inside, two, h...   \n",
       "31413  [meandering, at, people, of, the, walkway, sta...   \n",
       "4325   [standing, a, rock, ., on, man, view, the, sho...   \n",
       "28232  [to, a, a, in, on, shirt, little, red, a, hold...   \n",
       "28438  [children, two, in, <unk>, play, the, melting,...   \n",
       "\n",
       "                                     unjumbled_sentences  \n",
       "32760  [a, man, and, two, women, are, inside, a, gree...  \n",
       "31413  [people, stand, at, the, bottom, of, a, meande...  \n",
       "4325   [a, man, in, shorts, is, standing, on, a, rock...  \n",
       "28232  [a, little, girl, in, a, red, shirt, holds, on...  \n",
       "28438  [two, children, play, in, the, melting, <unk>,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor()\n",
    "train_df[\"jumbled_sentences\"] = train_df[\"jumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "train_df[\"unjumbled_sentences\"] = train_df[\"unjumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "test_df[\"jumbled_sentences\"] = test_df[\"jumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "test_df[\"unjumbled_sentences\"] = test_df[\"unjumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X_Encoder, X_Decoder and Y\n",
    "1. X denotes Input, Y denotes Output. \n",
    "2. X_encoder is the matrix of tokens in jumbled_sentences, each sentence suffixed by \"eos\" token. \n",
    "3. X_decoder is the matrix of tokens in unjumbled_sentences, each sentence prefixed by \"sos\" token. X_decoder is required because we want to do __Teacher Forcing__, which means we want to provide the correct current token to decoder to predict next token, instead of relying only on its own prediction. \n",
    "4. Y is the matrix of unjumbled_sentences, each sentence suffixed by \"eos\" token. \n",
    "\n",
    "5. Do this for both train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 32368\n",
      "X Encoder test length: 8092\n",
      "Sample X_encoder_train: ['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.', 'women', 'a', 'greenhouse', '', '<eos>']\n",
      "Sample X_decoder_train: ['<sos>', 'a', 'man', 'and', 'two', 'women', 'are', 'inside', 'a', 'greenhouse', 'holding', 'gardening', 'tools', '.', '']\n",
      "Sample Y_train: ['a', 'man', 'and', 'two', 'women', 'are', 'inside', 'a', 'greenhouse', 'holding', 'gardening', 'tools', '.', '', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "def get_Xe_Xd_Y(dataframe, sos_token, eos_token):\n",
    "    jumbled_sentences = dataframe[\"jumbled_sentences\"].tolist()\n",
    "    unjumbled_sentences = dataframe[\"unjumbled_sentences\"].tolist()\n",
    "    X_encoder_tokens = [el + [eos_token] for el in jumbled_sentences]\n",
    "    X_decoder_tokens = [[sos_token] + el for el in unjumbled_sentences]\n",
    "    Y_tokens = [el + [eos_token] for el in unjumbled_sentences]\n",
    "    return X_encoder_tokens, X_decoder_tokens, Y_tokens\n",
    "\n",
    "unknown_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "X_encoder_tokens_tr, X_decoder_tokens_tr, Y_tokens_tr = get_Xe_Xd_Y(train_df, sos_token, eos_token)\n",
    "X_encoder_tokens_test, X_decoder_tokens_test, Y_tokens_test = get_Xe_Xd_Y(test_df, sos_token, eos_token)\n",
    "print(\"X Encoder train length:\",len(X_encoder_tokens_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_tokens_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_tokens_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_tokens_tr[0])\n",
    "print(\"Sample Y_train:\",Y_tokens_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab\n",
    "1. Generally, Vocab is created from both Encoder and Decoder Tokens, consider Senetence Translation for ex, where encoder and decoder tokens can be in different languages.\n",
    "2. We can create Vocab separately for Encder and Decoder tokens, or can create shared vocab. Shared Vocab is preferable though.\n",
    "3. Also, Vocab is generated from only Training Data.\n",
    "4. In this case, we are using only Encoder tokens to create Vocab because Decoder Tokens are the same. Also, we are using both Train and Test Dataset to create Vocab, as our datasize is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabBuilder:\n",
    "    def __init__(self,token_corpus,unknown_token=None,pad_token=None,sos_token=None,eos_token=None):\n",
    "        '''\n",
    "        token_corpus = ['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.']\n",
    "        '''\n",
    "        self.token_corpus = token_corpus\n",
    "        self.unknown_token = unknown_token or \"<unk>\"\n",
    "        self.pad_token = pad_token or \"<pad>\"\n",
    "        self.sos_token = sos_token or \"<sos>\"\n",
    "        self.eos_token = eos_token or \"<eos>\"\n",
    "        self.word_to_index, self.index_to_word = self.get_vocabs()\n",
    "                        \n",
    "    def get_vocabs(self):\n",
    "        word_to_index = {}\n",
    "        index_count = 0\n",
    "        all_unique_words = set(self.token_corpus).difference(set(\n",
    "            [self.unknown_token, self.pad_token, self.sos_token, self.eos_token]\n",
    "        ))\n",
    "        word_to_index[self.unknown_token] = 0\n",
    "        word_to_index[self.pad_token] = 1\n",
    "        word_to_index[self.sos_token] = 2\n",
    "        word_to_index[self.eos_token] = 3\n",
    "        \n",
    "        for index, word in enumerate(all_unique_words):\n",
    "            word_to_index[word] = index + 4\n",
    "        if self.pad_token not in word_to_index: word_to_index[self.pad_token] = index + 1\n",
    "        if self.sos_token not in word_to_index: word_to_index[self.sos_token] = index + 2\n",
    "        if self.eos_token not in word_to_index: word_to_index[self.eos_token] = index + 3\n",
    "        if self.unknown_token not in word_to_index: word_to_index[self.unknown_token] = index + 4\n",
    "        index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "        return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.', 'women', 'a', 'greenhouse', '', '<eos>', 'meandering', 'at', 'people', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "token_corpus_1 = list(chain.from_iterable(X_encoder_tokens_tr)) # flattens a 2D list to 1D\n",
    "token_corpus_2 = list(chain.from_iterable(X_encoder_tokens_test))\n",
    "token_corpus = token_corpus_1 + token_corpus_2\n",
    "print(token_corpus[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordToIndex Dict length: 5242\n",
      "IndexToWord Dict length: 5242\n"
     ]
    }
   ],
   "source": [
    "vocab_builder = VocabBuilder(token_corpus,unknown_token,pad_token,sos_token,eos_token)\n",
    "print(\"WordToIndex Dict length:\",len(vocab_builder.word_to_index))\n",
    "print(\"IndexToWord Dict length:\",len(vocab_builder.index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map X_encoder, X_decoder and Y using Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_Index_Mapper:\n",
    "    def __init__(self,token_to_index,index_to_token, unknown_token):\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = index_to_token\n",
    "        self.unknown_token = unknown_token\n",
    "    \n",
    "    def get_encoding(self,sentence):\n",
    "        '''\n",
    "        sentence must be a list of tokens.\n",
    "        Ex: [\"Climate\",\"change\",\"is\",\"a\",\"pressing\",\"global\",\"issue\"]\n",
    "        '''\n",
    "        encoded_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in self.token_to_index: encoded_sentence.append(self.token_to_index[token])\n",
    "            else: encoded_sentence.append(self.token_to_index[self.unknown_token])\n",
    "        return encoded_sentence\n",
    "    \n",
    "    def get_decoding(self,encoded_sentence):\n",
    "        '''\n",
    "        encoded_sentence must be a list of vocab indices.\n",
    "        Ex: encoded_sentence = [24,21,4,1,..] \n",
    "        '''\n",
    "        sentence = [self.index_to_token[index] for index in encoded_sentence]\n",
    "        return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens_to_indices(token_index_mapper, max_sequence_length, token_matrix):\n",
    "    index_matrix = []\n",
    "    for el in token_matrix:\n",
    "        el = el[:max_sequence_length] # truncate sentence to max_seq_length\n",
    "        if len(el) < max_sequence_length:\n",
    "            pad_tokens_to_append = max_sequence_length - len(el)\n",
    "            el = el + [pad_token]*pad_tokens_to_append\n",
    "        index_matrix.append(token_index_mapper.get_encoding(el))\n",
    "    return index_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 32368\n",
      "X Encoder test length: 8092\n",
      "Sample X_encoder_train: [861, 3207, 3097, 114, 3766, 1889, 4931, 878, 652, 769, 3588, 3097, 3033, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample X_decoder_train: [2, 3097, 114, 3207, 4931, 3588, 652, 1889, 3097, 3033, 878, 3766, 861, 769, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample Y_train: [3097, 114, 3207, 4931, 3588, 652, 1889, 3097, 3033, 878, 3766, 861, 769, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 25\n",
    "token_index_mapper = Token_Index_Mapper(vocab_builder.word_to_index, vocab_builder.index_to_word, unknown_token)\n",
    "X_encoder_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_encoder_tokens_tr)\n",
    "X_decoder_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_decoder_tokens_tr)\n",
    "Y_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, Y_tokens_tr)\n",
    "\n",
    "X_encoder_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_encoder_tokens_test)\n",
    "X_decoder_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_decoder_tokens_test)\n",
    "Y_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, Y_tokens_test)\n",
    "print(\"X Encoder train length:\",len(X_encoder_indices_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_indices_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_indices_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_indices_tr[0])\n",
    "print(\"Sample Y_train:\",Y_indices_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification:\n",
    "1. Encoder: (Single Layer + Uni-Directional + StateLess) GRU\n",
    "2. Decoder: 100% Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchSize: B\n",
    "# Embedding_Dim: D\n",
    "# Hidden_Dim: H\n",
    "# Sequence_Length: L\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, debug=False):\n",
    "        # X shape: (B X L)\n",
    "        embedded = self.embedding(x)  # (B X L X D)\n",
    "        dropped_out = self.dropout(embedded) # (B X L X D)\n",
    "        output, hidden = self.gru(dropped_out) \n",
    "        # Output shape: (B X L X C*Hidden_Dim), where C == 2 if Bi-directional, else 1. \n",
    "        # It contains h_t for all tokens, hence the 'L' in shape.\n",
    "        # If more GRU layers are present, it contains h_t for all tokens but only from last layer.\n",
    "        \n",
    "        # Hidden shape: (C*NumLayers X B X H)\n",
    "        # It contains the h_t for only last token of each sequence in the batch.\n",
    "        # In case of multiple GRU layers:,\n",
    "        # It can be thought of as a stack of (B X H) matrix, where each GRU Layer\n",
    "        # is contributing 2 matrices (one for each direction in case of Bi-directional), \n",
    "        # such that the last layer's matrix is at the top of the stack and can be accessed using h_t[-1].\n",
    "        \n",
    "        hidden_unsqueezed = hidden[-1].unsqueeze(dim = 0)\n",
    "        # This operation has no meaning for the UniDirectional + SingleLayer case, but helpful for other cases.\n",
    "        # hidden[-1] shape = (B X H), we are only taking the last layer's matrix.\n",
    "        # unsqueeze(dim = 0): As the name suggests, adds extra dimension in matrix.\n",
    "        # dim = 0 means, extra dim is the 1st dimension (0th dimension).\n",
    "        # By definition the extra dimension value == 1, as\n",
    "        # it was 1 single matrix which was unsqueezed..\n",
    "        # hidden[-1].unsqueeze(dim = 0) shape: (1 X B X H)\n",
    "        \n",
    "        if debug: \n",
    "            print(\"-----------Encoder----------:\")\n",
    "            print(\"Input Data shape:\",x.shape)\n",
    "            print(\"After Embedding Layer:\",embedded.shape)\n",
    "            print(\"After Dropout Layer:\",dropped_out.shape)\n",
    "            print(\"Outputs and hidden shape from GRU:\",output.shape, hidden.shape)\n",
    "            print(\"Unsqueezed hidden shape:\", hidden_unsqueezed.shape)\n",
    "            \n",
    "        # Returning both the output and hidden state, but Decoder will only need hidden state.    \n",
    "        return output, hidden_unsqueezed\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim,  batch_first=True) # Decoders are almost always unidirectional.\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, encoder_hidden, debug=False):\n",
    "        # X shape: (B X L)\n",
    "        # X (decoder) is only required when we do Teacher Forcing. In this case we are doing 100% teacher forcing.\n",
    "        \n",
    "        embedded = self.embedding(x) # (B X L X D)\n",
    "        output, hidden = self.gru(embedded, encoder_hidden) # Same shape as in Encoder.\n",
    "        \n",
    "        reshaped_output = output.reshape(-1,output.shape[2])\n",
    "        # Reshaped_Output shape = (B*L X H) [Basically a 2-D Matrix]\n",
    "        # Since we need to calculate loss on each token of the batch, \n",
    "        # the output has to be reshaped into B*L X H.\n",
    "        # -1 in the reshape function tells Pytorch to figure out the size of Tensor by itself.\n",
    "        # output.shape[2] is H (obvious from Output Shape: B X L X H)\n",
    "        # So, the reshape operation figures out the size of 0th dimension, \n",
    "        # given that size of 1st dimension ==  H. \n",
    "        \n",
    "        prediction = self.fc_out(reshaped_output)\n",
    "        # prediction shape: (B*L X Vocab_Size)\n",
    "        # For each input token, we get the vector of vocab size consisting of logits.\n",
    "        # We take the softmax of each vector to convert vector of logits to vector of probabilities.\n",
    "        # Then we take argmax of each vector to get the predicted token.\n",
    "        \n",
    "        if debug: \n",
    "            print(\"-----------Decoder----------:\")\n",
    "            print(\"Input Data shape, X:\",x.shape, \", Encoder hidden state:\", encoder_hidden.shape)\n",
    "            print(\"After Embedding Layer:\",embedded.shape)\n",
    "            print(\"Outputs and hidden shape from GRU:\",output.shape, hidden.shape)\n",
    "            print(\"Reshaped Output:\", reshaped_output.shape)\n",
    "            print(\"After FC layer:\", prediction.shape)\n",
    "        return prediction, hidden\n",
    "    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__() \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input, debug=False): \n",
    "        encoder_outputs, encoder_hidden = self.encoder(encoder_input, debug) \n",
    "        outputs, _ = self.decoder(decoder_input, encoder_hidden, debug)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length):\n",
    "    model.eval()\n",
    "    sos_token = torch.tensor([[token_index_mapper.token_to_index[\"<sos>\"]]]).to(device) # 1st token as decoder input is <sos>.\n",
    "    unjumbled_sentence = []\n",
    "    with torch.no_grad():\n",
    "        encoder_op, encoder_hidden = model.encoder(Xe_b)\n",
    "        for i in range(max_sequence_length):\n",
    "            if i == 0: \n",
    "                # At 1st time step, input to decoder is <sos>.\n",
    "                # And the hidden state input to decoder is Encoder's hidden state.\n",
    "                decoder_op, decoder_hidden = model.decoder(sos_token,encoder_hidden)\n",
    "            else: \n",
    "                # After 1st time step, input to decoder is the predicted token of previous time step.\n",
    "                # and hidden state input to decoder is the hidden state output of decoder of previous time step.\n",
    "                \n",
    "                # To get the predicted token of previous time step:\n",
    "                # first, do the softmax on decoder_op of previous time step\n",
    "                softmax_op = torch.softmax(decoder_op,axis=1) # decoder_op is (1 X Vocab_Size),\n",
    "\n",
    "                # next, take the token with max probability\n",
    "                # (softmax_op is also [1 X Vocab_Size], as we have taken softmax along axis=1, which\n",
    "                # has simply converted the logits to probabilities.)\n",
    "                # torch.argmax() returns a tensor([]). The list will contain as many elements as 0th dimension of softmax_op.\n",
    "                # because we are taking argamx along axis = 1.\n",
    "                # In this case, softmax_op has only 1 token in 0th dimension, so the list has only 1 element.\n",
    "                # torch.tensor([]).tolist() gives out the []\n",
    "                predicted_token = torch.argmax(softmax_op,axis=1).tolist() \n",
    "                decoder_op, decoder_hidden = model.decoder(torch.tensor([predicted_token]).to(device),decoder_hidden)\n",
    "                \n",
    "                unjumbled_sentence.append(token_index_mapper.index_to_token[predicted_token[0]])\n",
    "                if predicted_token[0] == token_index_mapper.token_to_index[\"<eos>\"]: break\n",
    "        print(\"_______________________________________\")\n",
    "        print(unjumbled_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Training And Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(token_index_mapper.token_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(token_index_mapper.token_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 256 \n",
    "DEC_EMB_DIM = 256 \n",
    "HID_DIM = 512 \n",
    "ENC_DROPOUT = 0.5 \n",
    "device = \"cpu\"\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT) \n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM) \n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters()) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_index_mapper.token_to_index[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 25]) torch.Size([5, 25]) torch.Size([5, 25])\n",
      "-----------Encoder----------:\n",
      "Input Data shape: torch.Size([5, 25])\n",
      "After Embedding Layer: torch.Size([5, 25, 256])\n",
      "After Dropout Layer: torch.Size([5, 25, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 25, 512]) torch.Size([1, 5, 512])\n",
      "Unsqueezed hidden shape: torch.Size([1, 5, 512])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 25]) , Encoder hidden state: torch.Size([1, 5, 512])\n",
      "After Embedding Layer: torch.Size([5, 25, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 25, 512]) torch.Size([1, 5, 512])\n",
      "Reshaped Output: torch.Size([125, 512])\n",
      "After FC layer: torch.Size([125, 5242])\n"
     ]
    }
   ],
   "source": [
    "data_index = 6\n",
    "batch_size = 5\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "Xe_b = torch.tensor(X_encoder_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Xd_b = torch.tensor(X_decoder_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Y_b = torch.tensor(Y_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "output = model(Xe_b, Xd_b, debug=True)\n",
    "loss = criterion(output, Y_b.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Jumbled sentence: ['dog', 'tree', 'jumping', 'fallen', 'blue', 'collar', 'black', '.', 'mottled', 'a', 'over', 'grey', 'and', 'a', 'a', 'in', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'mottled', 'black', 'and', 'grey', 'dog', 'in', 'a', 'blue', 'collar', 'jumping', 'over', 'a', 'fallen', 'tree', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'a', 'child', 'it', 'baked', 'ignoring', 'n', 'hips', 'graduation', 'bared', 'carying', 'highland', 'brunettes', '<unk>', 'sheppard', 'foamy', 'loading', 'statues', 'brightly', 'tube', 'laugh', 'flat', 'long-sleeved', 'flat']\n"
     ]
    }
   ],
   "source": [
    "# Randomly select one sentence from Test Data to Predict.\n",
    "data_index = random.randint(0,100)\n",
    "# Since its only 1 sentence, we need to convert into a 2-D list before sending it to torch.tensor()\n",
    "Xe_b = torch.tensor([X_encoder_indices_test[data_index]]).to(device)\n",
    "print(\"Test Jumbled sentence:\",X_encoder_tokens_test[data_index])\n",
    "print(\"Test Unjumbled sentence:\", Y_tokens_test[data_index]) \n",
    "predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training And Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" #torch.device(\"cuda:0\")\n",
    "batch_size = 50\n",
    "INPUT_DIM = len(token_index_mapper.token_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(token_index_mapper.token_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 128 \n",
    "DEC_EMB_DIM = 128 \n",
    "HID_DIM = 500 \n",
    "ENC_DROPOUT = 0.8 \n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT) \n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM) \n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters()) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_index_mapper.token_to_index[\"<pad>\"])\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 Loss: 8.5993070602417\n",
      "Epoch: 0 Batch: 5000 Loss: 3.5956757068634033\n",
      "Epoch: 0 Batch: 10000 Loss: 3.646019458770752\n",
      "Epoch: 0 Batch: 15000 Loss: 3.1218230724334717\n",
      "Epoch: 0 Batch: 20000 Loss: 2.7600269317626953\n",
      "Epoch: 0 Batch: 25000 Loss: 2.771336793899536\n",
      "Epoch: 0 Batch: 30000 Loss: 2.7072560787200928\n",
      "______________________________________\n",
      "Epoch Loss: 2117.4885334968567\n",
      "Test Jumbled sentence: ['a', 'a', 'climbing', 'man', 'wall', 'rock', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'man', 'climbing', 'a', 'rock', 'wall', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'man', 'wearing', 'a', 'hat', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 1 Batch: 0 Loss: 2.668757677078247\n",
      "Epoch: 1 Batch: 5000 Loss: 2.3769640922546387\n",
      "Epoch: 1 Batch: 10000 Loss: 2.707134962081909\n",
      "Epoch: 1 Batch: 15000 Loss: 2.2910971641540527\n",
      "Epoch: 1 Batch: 20000 Loss: 2.0833005905151367\n",
      "Epoch: 1 Batch: 25000 Loss: 2.174213409423828\n",
      "Epoch: 1 Batch: 30000 Loss: 2.152735471725464\n",
      "______________________________________\n",
      "Epoch Loss: 1504.7123371362686\n",
      "Test Jumbled sentence: ['.', 'a', 'jacket', 'rides', 'mountain', 'and', 'dressed', 'brown', 'blue', 'horse', 'and', 'jeans', 'frozen', 'near', 'a', 'woman', 'a', 'in', 'snow-covered', 'lake', 'a', 'blue', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'woman', 'dressed', 'in', 'a', 'blue', 'jacket', 'and', 'blue', 'jeans', 'rides', 'a', 'brown', 'horse', 'near', 'a', 'frozen', 'lake', 'and', 'snow-covered', 'mountain', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'woman', 'in', 'a', 'blue', 'shirt', 'and', 'a', 'blue', 'shirt', ',', 'a', 'blue', 'shirt', ',', 'and', 'a', 'hat', 'in', 'a', 'crowd', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 2 Batch: 0 Loss: 2.161973476409912\n",
      "Epoch: 2 Batch: 5000 Loss: 1.8976621627807617\n",
      "Epoch: 2 Batch: 10000 Loss: 2.279240369796753\n",
      "Epoch: 2 Batch: 15000 Loss: 1.9058325290679932\n",
      "Epoch: 2 Batch: 20000 Loss: 1.7684341669082642\n",
      "Epoch: 2 Batch: 25000 Loss: 1.881790280342102\n",
      "Epoch: 2 Batch: 30000 Loss: 1.877007007598877\n",
      "______________________________________\n",
      "Epoch Loss: 1268.1142455339432\n",
      "Test Jumbled sentence: ['a', 'one', 'of', 'climbing', 'person', 'cliff', 'a', 'collage', '.', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'collage', 'of', 'one', 'person', 'climbing', 'a', 'cliff', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'person', 'climbing', 'a', 'rock', 'wall', 'while', 'people', 'watch', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 3 Batch: 0 Loss: 1.8630636930465698\n",
      "Epoch: 3 Batch: 5000 Loss: 1.6354436874389648\n",
      "Epoch: 3 Batch: 10000 Loss: 2.0033929347991943\n",
      "Epoch: 3 Batch: 15000 Loss: 1.6377859115600586\n",
      "Epoch: 3 Batch: 20000 Loss: 1.5632851123809814\n",
      "Epoch: 3 Batch: 25000 Loss: 1.641150712966919\n",
      "Epoch: 3 Batch: 30000 Loss: 1.592286229133606\n",
      "______________________________________\n",
      "Epoch Loss: 1104.5557574033737\n",
      "Test Jumbled sentence: ['two', 'people', 'three', 'snowmobiles', 'and', '.', '', '<eos>']\n",
      "Test Unjumbled sentence: ['three', 'people', 'and', 'two', 'snowmobiles', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['two', 'people', 'and', 'three', 'women', 'walk', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 4 Batch: 0 Loss: 1.607526183128357\n",
      "Epoch: 4 Batch: 5000 Loss: 1.435957908630371\n",
      "Epoch: 4 Batch: 10000 Loss: 1.6849113702774048\n",
      "Epoch: 4 Batch: 15000 Loss: 1.4820207357406616\n",
      "Epoch: 4 Batch: 20000 Loss: 1.3556939363479614\n",
      "Epoch: 4 Batch: 25000 Loss: 1.4901107549667358\n",
      "Epoch: 4 Batch: 30000 Loss: 1.4033769369125366\n",
      "______________________________________\n",
      "Epoch Loss: 971.2371056079865\n",
      "Test Jumbled sentence: ['person', 'a', 'vest', 'red', 'a', 'up', 'wearing', '.', 'climbs', 'rock', 'a', 'steep', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'person', 'wearing', 'a', 'red', 'vest', 'climbs', 'up', 'a', 'steep', 'rock', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'person', 'wearing', 'a', 'red', 'hat', 'climbing', 'a', 'rock', 'face', '.', '', '<eos>']\n",
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for j in range(0,len(X_encoder_indices_tr),batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        Xe_b = torch.tensor(X_encoder_indices_tr[j:j+batch_size]).to(device)\n",
    "        Xd_b = torch.tensor(X_decoder_indices_tr[j:j+batch_size]).to(device)\n",
    "        Y_b = torch.tensor(Y_indices_tr[j:j+batch_size]).to(device)\n",
    "        op = model(Xe_b,Xd_b)\n",
    "        loss = criterion(op,Y_b.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        if j%5000 == 0: print(\"Epoch:\",i,\"Batch:\",j,\"Loss:\",batch_loss)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Epoch Loss:\",epoch_loss)\n",
    "    \n",
    "    # Randomly select one sentence from Test Data to Predict.\n",
    "    data_index = random.randint(0,100)\n",
    "    # Since its only 1 sentence, we need to convert into a 2-D list before sending it to torch.tensor()\n",
    "    Xe_b = torch.tensor([X_encoder_indices_test[data_index]]).to(device)\n",
    "    print(\"Test Jumbled sentence:\",X_encoder_tokens_test[data_index])\n",
    "    print(\"Test Unjumbled sentence:\", Y_tokens_test[data_index]) \n",
    "    predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length)\n",
    "    print(\"_______________________________________\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
