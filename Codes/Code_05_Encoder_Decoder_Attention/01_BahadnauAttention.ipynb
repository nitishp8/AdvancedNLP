{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> In this notebook, we will try to unjumble a sentence using Encoder-Attention-Decoder Architecture built using <br><br>\n",
    " Recurrent Networks like GRU, LSTM and Bi-directional LSTMs.</h6>\n",
    "<h6> The Data is located here: ../../Datasets/Jumble_Unjumble/ </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/Train_400.tsv\",sep=\"\\t\")\n",
    "# test_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/Test_100.tsv\",sep=\"\\t\")\n",
    "# print(train_df.shape, test_df.shape)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32368, 2) (8092, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumbled_sentences</th>\n",
       "      <th>unjumbled_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32760</th>\n",
       "      <td>tools and a man gardening inside two holding a...</td>\n",
       "      <td>a man and two women are inside a greenhouse ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31413</th>\n",
       "      <td>meandering at people of the walkway stand . up...</td>\n",
       "      <td>people stand at the bottom of a meandering wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>standing a rock . on man view the shorts a out...</td>\n",
       "      <td>a man in shorts is standing on a rock looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28232</th>\n",
       "      <td>to a a in on shirt little red a holds pole nea...</td>\n",
       "      <td>a little girl in a red shirt holds on to a pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28438</th>\n",
       "      <td>children two in &lt;unk&gt; play the melting .</td>\n",
       "      <td>two children play in the melting &lt;unk&gt; .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       jumbled_sentences  \\\n",
       "32760  tools and a man gardening inside two holding a...   \n",
       "31413  meandering at people of the walkway stand . up...   \n",
       "4325   standing a rock . on man view the shorts a out...   \n",
       "28232  to a a in on shirt little red a holds pole nea...   \n",
       "28438          children two in <unk> play the melting .    \n",
       "\n",
       "                                     unjumbled_sentences  \n",
       "32760  a man and two women are inside a greenhouse ho...  \n",
       "31413  people stand at the bottom of a meandering wal...  \n",
       "4325   a man in shorts is standing on a rock looking ...  \n",
       "28232  a little girl in a red shirt holds on to a pol...  \n",
       "28438          two children play in the melting <unk> .   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jumbled_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/processed_jumbled.txt\",sep=\"\\t\",header=None)\n",
    "unjumbled_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/processed_unjumbled.txt\",sep=\"\\t\",header=None)\n",
    "jumbled_df.columns = [\"jumbled_sentences\"]\n",
    "unjumbled_df.columns = [\"unjumbled_sentences\"]\n",
    "df = pd.concat([jumbled_df,unjumbled_df],axis=1)\n",
    "train_df = df.sample(frac=0.8, random_state=42) \n",
    "test_df = df.drop(train_df.index)\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Train and Test Data\n",
    "1. Lowercasing, removing stopwords. <br>\n",
    "2. Stemming, Lemmetization. <br>\n",
    "3. Tokenization. <br>\n",
    "4. Here, we are just doing tokenization by splitting on space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenize_on = \" \"\n",
    "    \n",
    "    def tokenize(self,text_string):\n",
    "        '''\n",
    "        text_string = \"This is one sentence.\"\n",
    "        returns token_list = [\"This\",\"is\",\"one\",\"sentence.\"]\n",
    "        '''\n",
    "        token_list = text_string.split(self.tokenize_on)\n",
    "        return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32368, 2) (8092, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumbled_sentences</th>\n",
       "      <th>unjumbled_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32760</th>\n",
       "      <td>[tools, and, a, man, gardening, inside, two, h...</td>\n",
       "      <td>[a, man, and, two, women, are, inside, a, gree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31413</th>\n",
       "      <td>[meandering, at, people, of, the, walkway, sta...</td>\n",
       "      <td>[people, stand, at, the, bottom, of, a, meande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>[standing, a, rock, ., on, man, view, the, sho...</td>\n",
       "      <td>[a, man, in, shorts, is, standing, on, a, rock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28232</th>\n",
       "      <td>[to, a, a, in, on, shirt, little, red, a, hold...</td>\n",
       "      <td>[a, little, girl, in, a, red, shirt, holds, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28438</th>\n",
       "      <td>[children, two, in, &lt;unk&gt;, play, the, melting,...</td>\n",
       "      <td>[two, children, play, in, the, melting, &lt;unk&gt;,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       jumbled_sentences  \\\n",
       "32760  [tools, and, a, man, gardening, inside, two, h...   \n",
       "31413  [meandering, at, people, of, the, walkway, sta...   \n",
       "4325   [standing, a, rock, ., on, man, view, the, sho...   \n",
       "28232  [to, a, a, in, on, shirt, little, red, a, hold...   \n",
       "28438  [children, two, in, <unk>, play, the, melting,...   \n",
       "\n",
       "                                     unjumbled_sentences  \n",
       "32760  [a, man, and, two, women, are, inside, a, gree...  \n",
       "31413  [people, stand, at, the, bottom, of, a, meande...  \n",
       "4325   [a, man, in, shorts, is, standing, on, a, rock...  \n",
       "28232  [a, little, girl, in, a, red, shirt, holds, on...  \n",
       "28438  [two, children, play, in, the, melting, <unk>,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor()\n",
    "train_df[\"jumbled_sentences\"] = train_df[\"jumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "train_df[\"unjumbled_sentences\"] = train_df[\"unjumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "test_df[\"jumbled_sentences\"] = test_df[\"jumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "test_df[\"unjumbled_sentences\"] = test_df[\"unjumbled_sentences\"].apply(lambda x: preprocessor.tokenize(x))\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X_Encoder, X_Decoder and Y\n",
    "1. X denotes Input, Y denotes Output. <br>\n",
    "2. X_encoder is the matrix of tokens in jumbled_sentences, each sentence suffixed by \"eos\" token. <br>\n",
    "3. X_decoder is the matrix of tokens in unjumbled_sentences, each sentence prefixed by \"sos\" token. X_decoder is required because we want to do <b>Teacher Forcing</b>, which means we want to provide the correct current token to decoder to predict next token, instead of relying only on its own prediction. <br>\n",
    "4. Y is the matrix of unjumbled_sentences, each sentence suffixed by \"eos\" token. <br>\n",
    "\n",
    "5. Do this for both train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 32368\n",
      "X Encoder test length: 8092\n",
      "Sample X_encoder_train: ['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.', 'women', 'a', 'greenhouse', '', '<eos>']\n",
      "Sample X_decoder_train: ['<sos>', 'a', 'man', 'and', 'two', 'women', 'are', 'inside', 'a', 'greenhouse', 'holding', 'gardening', 'tools', '.', '']\n",
      "Sample Y_train: ['a', 'man', 'and', 'two', 'women', 'are', 'inside', 'a', 'greenhouse', 'holding', 'gardening', 'tools', '.', '', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "def get_Xe_Xd_Y(dataframe, sos_token, eos_token):\n",
    "    jumbled_sentences = dataframe[\"jumbled_sentences\"].tolist()\n",
    "    unjumbled_sentences = dataframe[\"unjumbled_sentences\"].tolist()\n",
    "    X_encoder_tokens = [el + [eos_token] for el in jumbled_sentences]\n",
    "    X_decoder_tokens = [[sos_token] + el for el in unjumbled_sentences]\n",
    "    Y_tokens = [el + [eos_token] for el in unjumbled_sentences]\n",
    "    return X_encoder_tokens, X_decoder_tokens, Y_tokens\n",
    "\n",
    "unknown_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "X_encoder_tokens_tr, X_decoder_tokens_tr, Y_tokens_tr = get_Xe_Xd_Y(train_df, sos_token, eos_token)\n",
    "X_encoder_tokens_test, X_decoder_tokens_test, Y_tokens_test = get_Xe_Xd_Y(test_df, sos_token, eos_token)\n",
    "print(\"X Encoder train length:\",len(X_encoder_tokens_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_tokens_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_tokens_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_tokens_tr[0])\n",
    "print(\"Sample Y_train:\",Y_tokens_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocab\n",
    "1. Generally, Vocab is created from both Encoder and Decoder Tokens, consider Senetence Translation for ex, where encoder and decoder tokens can be in different languages. <br>\n",
    "2. We can create Vocab separately for Encder and Decoder tokens, or can create shared vocab. Shared Vocab is preferable though. <br>\n",
    "3. Also, Vocab is generated from only Training Data. <br>\n",
    "4. In this case, we are using only Encoder tokens to create Vocab because Decoder Tokens are the same. Also, we are using both Train and Test Dataset to create Vocab, as our datasize is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabBuilder:\n",
    "    def __init__(self,token_corpus,unknown_token=None,pad_token=None,sos_token=None,eos_token=None):\n",
    "        '''\n",
    "        token_corpus = ['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.']\n",
    "        '''\n",
    "        self.token_corpus = token_corpus\n",
    "        self.unknown_token = unknown_token or \"<unk>\"\n",
    "        self.pad_token = pad_token or \"<pad>\"\n",
    "        self.sos_token = sos_token or \"<sos>\"\n",
    "        self.eos_token = eos_token or \"<eos>\"\n",
    "        self.word_to_index, self.index_to_word = self.get_vocabs()\n",
    "                        \n",
    "    def get_vocabs(self):\n",
    "        word_to_index = {}\n",
    "        index_count = 0\n",
    "        all_unique_words = set(self.token_corpus).difference(set(\n",
    "            [self.unknown_token, self.pad_token, self.sos_token, self.eos_token]\n",
    "        ))\n",
    "        word_to_index[self.unknown_token] = 0\n",
    "        word_to_index[self.pad_token] = 1\n",
    "        word_to_index[self.sos_token] = 2\n",
    "        word_to_index[self.eos_token] = 3\n",
    "        \n",
    "        for index, word in enumerate(all_unique_words):\n",
    "            word_to_index[word] = index + 4\n",
    "        if self.pad_token not in word_to_index: word_to_index[self.pad_token] = index + 1\n",
    "        if self.sos_token not in word_to_index: word_to_index[self.sos_token] = index + 2\n",
    "        if self.eos_token not in word_to_index: word_to_index[self.eos_token] = index + 3\n",
    "        if self.unknown_token not in word_to_index: word_to_index[self.unknown_token] = index + 4\n",
    "        index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "        return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tools', 'and', 'a', 'man', 'gardening', 'inside', 'two', 'holding', 'are', '.', 'women', 'a', 'greenhouse', '', '<eos>', 'meandering', 'at', 'people', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "token_corpus_1 = list(chain.from_iterable(X_encoder_tokens_tr)) # flattens a 2D list ot 1D\n",
    "token_corpus_2 = list(chain.from_iterable(X_encoder_tokens_test))  # flattens a 2D list ot 1D\n",
    "token_corpus = token_corpus_1 + token_corpus_2\n",
    "print(token_corpus[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordToIndex Dict length: 5242\n",
      "IndexToWord Dict length: 5242\n"
     ]
    }
   ],
   "source": [
    "vocab_builder = VocabBuilder(token_corpus,unknown_token,pad_token,sos_token,eos_token)\n",
    "print(\"WordToIndex Dict length:\",len(vocab_builder.word_to_index))\n",
    "print(\"IndexToWord Dict length:\",len(vocab_builder.index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map X_encoder, X_decoder and Y using Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_Index_Mapper:\n",
    "    def __init__(self,token_to_index,index_to_token, unknown_token):\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = index_to_token\n",
    "        self.unknown_token = unknown_token\n",
    "    \n",
    "    def get_encoding(self,sentence):\n",
    "        '''\n",
    "        sentence must be a list of tokens.\n",
    "        Ex: [\"Climate\",\"change\",\"is\",\"a\",\"pressing\",\"global\",\"issue\"]\n",
    "        '''\n",
    "        encoded_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in self.token_to_index: encoded_sentence.append(self.token_to_index[token])\n",
    "            else: encoded_sentence.append(self.token_to_index[self.unknown_token])\n",
    "        return encoded_sentence\n",
    "    \n",
    "    def get_decoding(self,encoded_sentence):\n",
    "        '''\n",
    "        encoded_sentence must be a list of vocab indices.\n",
    "        Ex: encoded_sentence = [24,21,4,1,..] \n",
    "        '''\n",
    "        sentence = [self.index_to_token[index] for index in encoded_sentence]\n",
    "        return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens_to_indices(token_index_mapper, max_sequence_length, token_matrix):\n",
    "    index_matrix = []\n",
    "    for el in token_matrix:\n",
    "        el = el[:max_sequence_length] # truncate sentence to max_seq_length\n",
    "        if len(el) < max_sequence_length:\n",
    "            pad_tokens_to_append = max_sequence_length - len(el)\n",
    "            el = el + [pad_token]*pad_tokens_to_append\n",
    "        index_matrix.append(token_index_mapper.get_encoding(el))\n",
    "    return index_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 32368\n",
      "X Encoder test length: 8092\n",
      "Sample X_encoder_train: [324, 527, 3696, 1077, 891, 1744, 784, 15, 2841, 4202, 4549, 3696, 3904, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample X_decoder_train: [2, 3696, 1077, 527, 784, 4549, 2841, 1744, 3696, 3904, 15, 891, 324, 4202, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample Y_train: [3696, 1077, 527, 784, 4549, 2841, 1744, 3696, 3904, 15, 891, 324, 4202, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 25\n",
    "token_index_mapper = Token_Index_Mapper(vocab_builder.word_to_index, vocab_builder.index_to_word, unknown_token)\n",
    "X_encoder_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_encoder_tokens_tr)\n",
    "X_decoder_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_decoder_tokens_tr)\n",
    "Y_indices_tr = map_tokens_to_indices(token_index_mapper, max_sequence_length, Y_tokens_tr)\n",
    "\n",
    "X_encoder_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_encoder_tokens_test)\n",
    "X_decoder_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, X_decoder_tokens_test)\n",
    "Y_indices_test = map_tokens_to_indices(token_index_mapper, max_sequence_length, Y_tokens_test)\n",
    "print(\"X Encoder train length:\",len(X_encoder_indices_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_indices_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_indices_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_indices_tr[0])\n",
    "print(\"Sample Y_train:\",Y_indices_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model \n",
    "1. Encoder: (Bi-Directional + BatchFirst + 2-Layer + StateLess)  GRU <br>\n",
    "2. Attention: Bahadnau <br>\n",
    "3. Decoder: Single Layer + TeacherForcing <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True, bidirectional = True, num_layers = 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, debug=False):\n",
    "        # X shape: (Batch_Size X Seq_Length)\n",
    "        \n",
    "        embedded = self.embedding(x)   \n",
    "        # Embedded shape: (Batch_Size X Seq_Length X Embedding_Dim)\n",
    "        \n",
    "        dropped_out = self.dropout(embedded) \n",
    "        # DroppedOut shape: (Batch_Size X Seq_Length X Embedding_Dim)\n",
    "        \n",
    "        output, hidden = self.gru(dropped_out) \n",
    "        # Output shape: (Batch_Size X Seq_Length X D*Hidden_Dim), where D == 2 if Bi-directional, else 1. \n",
    "        # It contains h_t for all tokens, hence the Seq_Length in shape.\n",
    "        # If more GRU layers are present, it contains h_t for all tokens but only from last layer.\n",
    "        \n",
    "        # Hidden shape: (D*NumLayers X Batch_Size X Hidden_Dim)\n",
    "        # It contains the h_t for only last token of each sequence in the batch.\n",
    "        # In case of multiple GRU layers:,\n",
    "        # It can be thought of as a stack of (Batch_Size X Hidden_Dim) matrix, where each GRU Layer\n",
    "        # is contributing 2 matrices (one for each direction in case of Bi-directional), \n",
    "        # such that the last layer's matrix is at the top of the stack and can be accessed using h_t[-1].\n",
    "        \n",
    "        hidden_concatenated = torch.cat((hidden[-1],hidden[-2]),axis=1)\n",
    "        # hidden[-1] shape = Batch_Size X Hidden_Dim, we are only taking the last layer's matrix.\n",
    "        # Since, the GRU is bi-directional we have 2 matrices of last layer, one for each direction (hidden[-1], hidden[-2]).\n",
    "        # We need to concatenate them along Hidden_Dim.\n",
    "        # So, hidden_concatenated shape = Batch_Size X 2*Hidden_Dim\n",
    "        \n",
    "        hidden_unsqueezed = hidden_concatenated.unsqueeze(0)\n",
    "        # We are converting the 2-D matrix back to 3-D matrix,\n",
    "        # with the first dimension (or the 0th dimension) == 1.\n",
    "        \n",
    "        if debug: \n",
    "            print(\"-----------Encoder----------:\")\n",
    "            print(\"Input Data shape:\",x.shape)\n",
    "            print(\"After Embedding Layer:\",embedded.shape)\n",
    "            print(\"After Dropout Layer:\",dropped_out.shape)\n",
    "            print(\"Outputs and hidden shape from GRU:\",output.shape, hidden.shape)\n",
    "            print(\"Unsqueezed hidden shape:\", hidden_unsqueezed.shape)\n",
    "            \n",
    "        # Returning both the output and hidden state, output will be needed by Attention module, and \n",
    "        # unsqueezed hidden state will be needed by Decoder.    \n",
    "        return output, hidden_unsqueezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    # At any time step t of decoder, we need a weighted average of hidden state of all tokens of encoder.\n",
    "    # The weights (with which we are multiplying) encoder hidden state must have following property:\n",
    "    #   i. Their sum must be == 1.\n",
    "    #   ii. Their values should also depend on which token we are going to predict in Decoder.\n",
    "    # These weights are basically the attention weights and they are the output of the Attention module,\n",
    "    # not the parameters.\n",
    "    \n",
    "    # Bahadnau's attention calculate the attention weights (alphas) like this:\n",
    "    # alpha = softmax( V * tanh(U * decoder_prev_state + W * encoder_all_hidden_states))\n",
    "    # For one sequence, alpha must be a vector of length (Seq_Length), \n",
    "    # for one batch, alpha = (Batch_Size X Seq_Length)\n",
    "    # U, W and V are the parameters of Attention module.\n",
    "    # At least, one dimension of W has to be of the dimension of encoder's hidden state for matmul compatibility\n",
    "    # At least, one dimension of U has to of dimension of decoder's hidden state for matmul compatibility\n",
    "    \n",
    "    \n",
    "    # encoder_all_hidden_states = (Batch_Size X Seq_Length X 2*Hidden_Dim), since Encoder is Bi-Directional.\n",
    "    # We reshape it to (Batch_Size X 2*Hidden_Dim X Seq_Length) for allowing matrix multiplication with W.\n",
    "    # We chose W = (2*Hidden_Dim X 2*Hidden_Dim), \n",
    "    # W has to be repeated Batch_Size times to multiply with encoder_all_hidden_states.\n",
    "    # torch.matmul() takes care of it by broadcasting W in Batch_Size dimension.\n",
    "    # W * encoder_all_hidden_states  = (Batch_Size X 2*Hidden_Dim X Seq_Length)\n",
    "    \n",
    "    # decoder_prev_state = (1 X Batch_Size X 2*Hidden_Dim) (since, Encoder is Bi-directional)\n",
    "    # We squeeze it to (Batch_Size X 2*Hidden_Dim) and then repeat it Seq_Length times.\n",
    "    # So that, decoder_prev_state = (Batch_Size X 2*Hidden_Dim X Seq_Length)\n",
    "    # We chose U = (2*Hidden_Dim X 2*Hidden_Dim), it will also be broadcasted like W.\n",
    "    # U * decoder_prev_state = (Batch_Size X 2*Hidden_Dim X Seq_Length)\n",
    "    \n",
    "    # V is [1 X 2*Hidden_Dim] vector\n",
    "    # V * tanh(U * decoder_prev_state + W * encoder_all_hidden_states) = (Batch_Size X 1 X Seq_Length)\n",
    "    # We squeeze it to get (Batch_Size X Seq_Length), and apply softmax to get alpha.\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, decoder_hidden_dim, encoder_hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.U = torch.randn(encoder_hidden_dim, decoder_hidden_dim, requires_grad=True)\n",
    "        self.W = torch.randn(encoder_hidden_dim, encoder_hidden_dim, requires_grad=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        # Implements: tanh(U * decoder_prev_state + W * encoder_all_hidden_states)\n",
    "        # U = 2*Hidden_Dim X 2*Hidden_Dim\n",
    "        # W = 2*Hidden_Dim X 2*Hidden_Dim\n",
    "         \n",
    "        self.V = torch.randn(1, encoder_hidden_dim, requires_grad=True)\n",
    "        # Implements: V * tanh(U * decoder_prev_state + W * encoder_all_hidden_states)\n",
    "        # V = 1 X 2*Hidden_Dim\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, op_from_encoder, st_minus_one_from_decoder, debug=False):\n",
    "        # op_from_enoder shape = (Batch_Size X Seq_Length X 2*Hidden_Dim)\n",
    "        # It has to be reshaped to (Batch_Size X 2*Hidden_Dim X Seq_Length)\n",
    "        reshaped_op_from_encoder = op_from_encoder.reshape(\n",
    "            op_from_encoder.shape[0],op_from_encoder.shape[2],op_from_encoder.shape[1]\n",
    "        )\n",
    "        \n",
    "        # st_minus_one_from_decoder shape = (1 X Batch_Size X 2*Hidden_Dim)\n",
    "        # Squeeze it to make (Batch_Size X 2*Hidden_Dim)\n",
    "        # UnSqueeze it to make (Batch_Size X 2*Hidden_Dim X 1)\n",
    "        # And repeat it to Seq_Length times to get (Batch_Size X 2*Hidden_Dim X Seq_Length)\n",
    "        seq_length = reshaped_op_from_encoder.shape[2]\n",
    "        squeezed_st_minus_1 = st_minus_one_from_decoder.squeeze(dim=0)\n",
    "        unsqueezed_st_minus_1 = squeezed_st_minus_1.unsqueeze(dim=2)\n",
    "        reshaped_st_minus_one_from_decoder = unsqueezed_st_minus_1.repeat(1,1,seq_length)\n",
    "        \n",
    "        rnn_op = self.tanh(\n",
    "            torch.matmul(self.U,reshaped_st_minus_one_from_decoder) + torch.matmul(self.W, reshaped_op_from_encoder)\n",
    "        )\n",
    "        # rnn_op = (Batch_Size X Hidden_Dim X Seq_Length)\n",
    "        batch_size = op_from_encoder.shape[0]\n",
    "        linear_op = torch.matmul(self.V, rnn_op) # (Batch_Size X 1 X Seq_Length)\n",
    "        squeezed_linear_op = linear_op.squeeze(dim=1) # squeezing along dim = 1 will give (Batch_Size X Seq_Length)\n",
    "        softmax_op = self.softmax(squeezed_linear_op) # Applies softmax along dim = 1, which means along Seq_Length\n",
    "        \n",
    "        if debug: \n",
    "            print(\"-----------Attention----------:\")\n",
    "            print(\"Encoder All Hidden State Shape:\",op_from_encoder.shape)\n",
    "            print(\"Reshaped Encoder All Hidden State Shape:\",reshaped_op_from_encoder.shape)\n",
    "            print(\"Decoder Previous Hidden State Shape:\",st_minus_one_from_decoder.shape)\n",
    "            print(\"Decoder Previous Hidden State Squeezed Shape:\",squeezed_st_minus_1.shape)\n",
    "            print(\"Decoder Previous Hidden State UnSqueezed Shape:\",unsqueezed_st_minus_1.shape)\n",
    "            print(\"Decoder Reshaped Previous Hidden State Shape:\",reshaped_st_minus_one_from_decoder.shape)\n",
    "            print(\"After RNNCell:\",rnn_op.shape)\n",
    "            print(\"After Linear Layer:\",linear_op.shape)\n",
    "            print(\"After Squeezing Linear Layer:\",squeezed_linear_op.shape)\n",
    "            print(\"Final alphas shape, after softmax:\", softmax_op.shape)\n",
    "        \n",
    "        return softmax_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim,  batch_first=True) # Decoders are almost always unidirectional.\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, attn_weighted_encoder_input, prev_hidden_state, debug=False):\n",
    "        # In Decoder, we concat the attn_weighted_encoder_input to x (after embedding).\n",
    "        \n",
    "        # X shape: (Batch_Size X Seq_Length)\n",
    "        # X (decoder) is only required when we do Teacher Forcing.\n",
    "        \n",
    "        seq_length = x.shape[1]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        # Embedded shape: (Batch_Size X Seq_Length X Embedding_Dim)\n",
    "        \n",
    "        attn_weighted_encoder_input_unsq = attn_weighted_encoder_input.unsqueeze(dim=1)\n",
    "        attn_weighted_encoder_input_rep = attn_weighted_encoder_input_unsq.repeat(1,seq_length,1)\n",
    "        # attn_weighted_encoder_input = (Batch_Size X 2* Hidden_Dim)\n",
    "        # We need to make it of size (Batch_Size X Seq_Length X 2* Hidden_Dim) to append to embedded.\n",
    "        # So we first unsqueeze it to make (Batch_Size X 1 X 2* Hidden_Dim), and then repeat Seq_length times\n",
    "        # to make (Batch_Size X Seq_Length X 2* Hidden_Dim)\n",
    "        \n",
    "        concatenated_input = torch.cat((embedded, attn_weighted_encoder_input_rep), axis=2)\n",
    "        # concatenated_input = (Batch_Size X Seq_Length X (Embedding_Dim + 2* Hidden_Dim))\n",
    "        \n",
    "        output, hidden = self.gru(embedded, prev_hidden_state)\n",
    "        # Output shape: (Batch_Size X Seq_Length X D*Hidden_Dim), where D == 2 if Bi-directional, else 1.\n",
    "        # Bi-directional Decoder does not make sense, except for very rare cases.\n",
    "        # Hidden shape: (D*NumLayers X Batch_Size X Hidden_Dim)\n",
    "        # It contains the h_t for only last token of each sequence in the batch.\n",
    "        \n",
    "        reshaped_output = output.reshape(-1,output.shape[2])\n",
    "        # Reshaped_Output shape = (Batch_Size*Seq_Length X Hidden_Dim) [Basically a 2-D Matrix]\n",
    "        # Since we need to calculate loss on each token of the batch, the output has to be \n",
    "        # reshaped into Batch_Size*Seq_Length X Hidden_Dim.\n",
    "        # -1 in the reshape function tells Pytorch to figure out the size of Tensor by itself.\n",
    "        # output.shape[2] is the Hidden_Dim (obvious from Output Shape: Batch_Size X Seq_Length X D*Hidden_Dim)\n",
    "        # So, the reshape operation figures out the size of 0th dimension, \n",
    "        # given that size of 1st dimension ==  Hidden_Dim. \n",
    "        \n",
    "        prediction = self.fc_out(reshaped_output)\n",
    "        # prediction shape: (Batch_Size*Seq_Length X Vocab_Size)\n",
    "        # For each input token, we get the vector of vocab size consisting of logits.\n",
    "        # We take the softmax of each vector to convert vector of logits to vector of probabilities.\n",
    "        # Then we take argmax of each vector to get the predicted token.\n",
    "        \n",
    "        if debug: \n",
    "            print(\"-----------Decoder----------:\")\n",
    "            print(\"Input Data shape, X:\",x.shape, \", Previous step hidden state:\", prev_hidden_state.shape)\n",
    "            print(\"Attention Weighted Enocder Hidden State:\",attn_weighted_encoder_input.shape)\n",
    "            print(\"After Embedding Layer:\",embedded.shape)\n",
    "            print(\"After Concatenating with AttentionWeightedEnocderHidden:\",concatenated_input.shape)\n",
    "            print(\"Outputs and hidden shape from GRU:\",output.shape, hidden.shape)\n",
    "            print(\"Reshaped Output:\", reshaped_output.shape)\n",
    "            print(\"After FC layer:\", prediction.shape)\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, attention, decoder, device, teacher_forcing_ratio, max_seq_len, token_index_mapper):\n",
    "        super(Seq2Seq, self).__init__() \n",
    "        self.encoder = encoder\n",
    "        self.attention = attention\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_index_mapper = token_index_mapper\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input, debug=False): \n",
    "        # Encoder Input: (Batch_Size X Seq_Length)\n",
    "        # Decoder Input: (Batch_Size X Seq_Length), but we have to send input to decoder one token at a time, \n",
    "        # so at each time step, Decoder Input shape is (Batch_Size X 1)\n",
    "        # At first time step, decoder_input is <sos>. So sos_tokens shape = (Batch_Size X 1)\n",
    "        # At later time steps: decoder_input will either be the predicted_token from previous time step or the teacher token.\n",
    "        \n",
    "        batch_size = encoder_input.shape[0]\n",
    "        sos_tokens = [[token_index_mapper.token_to_index[\"<sos>\"]]*batch_size]\n",
    "        sos_tokens = torch.tensor(sos_tokens).reshape(-1,1).to(self.device) # 1st token to decoder is <sos>.\n",
    "        \n",
    "        # the output matrix will have shape (Batch_Size, Seq_Length, Vocab_Size)\n",
    "        outputs = torch.zeros(batch_size,self.max_seq_len,len(token_index_mapper.token_to_index)).to(self.device)\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = self.encoder(encoder_input, debug)\n",
    "        # encoder_outputs: (Batch_Size X Seq_length X 2*Hidden_Dim), since the Encoder is BiDirectional\n",
    "        for i in range(self.max_seq_len):\n",
    "            if i == 0: \n",
    "                attention_weights = self.attention(encoder_outputs, encoder_hidden, debug)\n",
    "                # attention_weights = (Batch_Size X Seq_Length)\n",
    "                unsqueezed_attention_weights = attention_weights.unsqueeze(dim=2) # for torch.mul\n",
    "                # unsqueezed_attention_weights = (Batch_Size X Seq_Length X 1)\n",
    "                weighted_avg = torch.sum(torch.mul(encoder_outputs, unsqueezed_attention_weights),dim=1)\n",
    "                # torch.mul() for 1 sequence of the batch, will multiply each attn_weight to \n",
    "                # its repective encoder hidden state, giving out Seq_Length X 2*Hidden_Dim\n",
    "                # When done across batch, torch.mul() gives (Batch_Size X Seq_Length X 2*Hidden_Dim)\n",
    "                # Now, we have to do sum across Seq_Length (for doing weighted avg)\n",
    "                # So, torch.sum(dim = 1)\n",
    "                # weighted_avg = (Batch_Size X 2*Hidden_Dim)\n",
    "                decoder_op, decoder_hidden = self.decoder(sos_tokens,weighted_avg, encoder_hidden, debug)\n",
    "            else:\n",
    "                teacher_force = torch.rand(1).item() < self.teacher_forcing_ratio\n",
    "                attention_weights = self.attention(encoder_outputs, decoder_hidden, debug)\n",
    "                unsqueezed_attention_weights = attention_weights.unsqueeze(dim=2) # for torch.mul\n",
    "                weighted_avg = torch.sum(torch.mul(encoder_outputs, unsqueezed_attention_weights),dim=1)\n",
    "                if teacher_force:\n",
    "                    # we need the ith token of all sequences in the batch\n",
    "                    # decoder_input[:,i] will be of len (BatchSize)\n",
    "                    # reshape(-1,1), means calculate the 0th dimension given that 1st dimension == 1\n",
    "                    # So, decoder_input[:,i].reshape(-1,1) will be (Batch_Size X 1)\n",
    "                    decoder_op, decoder_hidden = self.decoder(\n",
    "                        decoder_input[:,i].reshape(-1,1),weighted_avg, decoder_hidden, debug\n",
    "                    )\n",
    "                else:\n",
    "                    # decoder_op is (Batch_Size X Vocab_Size)\n",
    "                    # decoder_hidden like encoder_hidden is (1 X Batch_Size X Hidden_Dim)\n",
    "                    softmax_op = torch.softmax(decoder_op,axis=1) # softmax_op is (Batch_Size X Vocab_Size)\n",
    "                    pred_tokens = torch.argmax(softmax_op,axis=1) # pred_tokens is torch.tensor([]) of len (Batch_Size)\n",
    "                    reshaped_pred_tokens = pred_tokens.reshape(-1,1) # reshape to (Batch_Size X 1)\n",
    "                    decoder_op, decoder_hidden = self.decoder(\n",
    "                        reshaped_pred_tokens.to(self.device),weighted_avg, decoder_hidden, debug\n",
    "                    )\n",
    "                    if debug:\n",
    "                        print(\"Seq2Seq softmax_op\",softmax_op.shape)\n",
    "                        print(\"Seq2Seq pred_tokens\",pred_tokens.shape)\n",
    "                        print(\"Seq2Seq reshaped_pred_tokens\",reshaped_pred_tokens.shape)\n",
    "            # We need to put the decoder_op for each token into output matrix.\n",
    "            outputs[:,i,:] = decoder_op\n",
    "        \n",
    "        # Finally we have to return the output in the shape (Batch_Size*Seq_Length, Vocab_Size)\n",
    "        # So, we are again using the same concept of reshape() to return final_matrix.\n",
    "        return outputs.reshape(-1,len(token_index_mapper.token_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length):\n",
    "    model.eval()\n",
    "    sos_token = torch.tensor([[token_index_mapper.token_to_index[\"<sos>\"]]]).to(device) # 1st token as decoder input is <sos>.\n",
    "    unjumbled_sentence = []\n",
    "    with torch.no_grad():\n",
    "        attention_matrix = []\n",
    "        encoder_outputs, encoder_hidden = model.encoder(Xe_b)\n",
    "        for i in range(max_sequence_length):\n",
    "            if i == 0: \n",
    "                # At 1st time step, input to decoder is the index of <sos> token.\n",
    "                # And the hidden state input to decoder is Encoder's hidden state.\n",
    "                attention_weights = model.attention(encoder_outputs, encoder_hidden)\n",
    "                attention_matrix.append(attention_weights.tolist()[0])\n",
    "                unsqueezed_attention_weights = attention_weights.unsqueeze(dim=2) # for torch.mul\n",
    "                weighted_avg = torch.sum(torch.mul(encoder_outputs, unsqueezed_attention_weights),dim=1)\n",
    "                decoder_op, decoder_hidden = model.decoder(sos_token, weighted_avg,encoder_hidden)\n",
    "            else: \n",
    "                # After 1st time step, input to decoder is the predicted token of previous time step.\n",
    "                # and hidden state input to decoder is the hidden state output of decoder of previous time step.\n",
    "                \n",
    "                # To get the predicted token of previous time step:\n",
    "                # first, do the softmax on decoder_op of previous time step\n",
    "                softmax_op = torch.softmax(decoder_op,axis=1) # decoder_op is (1 X Vocab_Size),\n",
    "\n",
    "                # next, take the token with max probability\n",
    "                # (softmax_op is also [1 X Vocab_Size], as we have taken softmax along axis=1, which\n",
    "                # has simply converted the logits to probabilities.)\n",
    "                # torch.argmax() returns a tensor([]). The list will contain as many elements as 0th dimension of softmax_op.\n",
    "                # because we are taking argamx along axis = 1.\n",
    "                # In this case, softmax_op has only 1 token in 0th dimension, so the list has only 1 element.\n",
    "                # torch.tensor([]).tolist() gives out the []\n",
    "                predicted_token = torch.argmax(softmax_op,axis=1).tolist()\n",
    "                attention_weights = model.attention(encoder_outputs, decoder_hidden)\n",
    "                attention_matrix.append(attention_weights.tolist()[0])\n",
    "                unsqueezed_attention_weights = attention_weights.unsqueeze(dim=2) # for torch.mul\n",
    "                weighted_avg = torch.sum(torch.mul(encoder_outputs, unsqueezed_attention_weights),dim=1)\n",
    "                decoder_op, decoder_hidden = model.decoder(\n",
    "                    torch.tensor([predicted_token]).to(device), weighted_avg, decoder_hidden\n",
    "                )\n",
    "                \n",
    "                unjumbled_sentence.append(token_index_mapper.index_to_token[predicted_token[0]])\n",
    "                if predicted_token[0] == token_index_mapper.token_to_index[\"<eos>\"]: break\n",
    "        print(\"_______________________________________\")\n",
    "        print(unjumbled_sentence)\n",
    "        return attention_matrix, unjumbled_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Training And Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(token_index_mapper.token_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(token_index_mapper.token_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 256 \n",
    "DEC_EMB_DIM = 256 \n",
    "HID_DIM = 512 \n",
    "ENC_DROPOUT = 0.5 \n",
    "device = \"cpu\"\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT) \n",
    "attn = Attention(2*HID_DIM, 2*HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, 2*HID_DIM) \n",
    "model = Seq2Seq(enc, attn, dec, device, 0.7, max_sequence_length, token_index_mapper).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters()) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_index_mapper.token_to_index[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 25]) torch.Size([5, 25]) torch.Size([5, 25])\n",
      "-----------Encoder----------:\n",
      "Input Data shape: torch.Size([5, 25])\n",
      "After Embedding Layer: torch.Size([5, 25, 256])\n",
      "After Dropout Layer: torch.Size([5, 25, 256])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 25, 1024]) torch.Size([4, 5, 512])\n",
      "Unsqueezed hidden shape: torch.Size([1, 5, 1024])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n",
      "-----------Attention----------:\n",
      "Encoder All Hidden State Shape: torch.Size([5, 25, 1024])\n",
      "Reshaped Encoder All Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "Decoder Previous Hidden State Shape: torch.Size([1, 5, 1024])\n",
      "Decoder Previous Hidden State Squeezed Shape: torch.Size([5, 1024])\n",
      "Decoder Previous Hidden State UnSqueezed Shape: torch.Size([5, 1024, 1])\n",
      "Decoder Reshaped Previous Hidden State Shape: torch.Size([5, 1024, 25])\n",
      "After RNNCell: torch.Size([5, 1024, 25])\n",
      "After Linear Layer: torch.Size([5, 1, 25])\n",
      "After Squeezing Linear Layer: torch.Size([5, 25])\n",
      "Final alphas shape, after softmax: torch.Size([5, 25])\n",
      "-----------Decoder----------:\n",
      "Input Data shape, X: torch.Size([5, 1]) , Previous step hidden state: torch.Size([1, 5, 1024])\n",
      "Attention Weighted Enocder Hidden State: torch.Size([5, 1024])\n",
      "After Embedding Layer: torch.Size([5, 1, 256])\n",
      "After Concatenating with AttentionWeightedEnocderHidden: torch.Size([5, 1, 1280])\n",
      "Outputs and hidden shape from GRU: torch.Size([5, 1, 1024]) torch.Size([1, 5, 1024])\n",
      "Reshaped Output: torch.Size([5, 1024])\n",
      "After FC layer: torch.Size([5, 5242])\n",
      "Seq2Seq softmax_op torch.Size([5, 5242])\n",
      "Seq2Seq pred_tokens torch.Size([5])\n",
      "Seq2Seq reshaped_pred_tokens torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "data_index = 6\n",
    "batch_size = 5\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "Xe_b = torch.tensor(X_encoder_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Xd_b = torch.tensor(X_decoder_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "Y_b = torch.tensor(Y_indices_tr[data_index:data_index+batch_size]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "output = model(Xe_b, Xd_b, debug=True)\n",
    "loss = criterion(output, Y_b.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Jumbled sentence: ['and', 'catch', 'in', 'for', '.', 'a', 'waiting', 'come', 'him', 'throwing', 'air', 'up', 'the', 'water', 'so', 'him', 'man', 'can', 'in', 'a', 'down', 'boy', 'he', 'little', 'to', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'man', 'in', 'water', 'throwing', 'a', 'little', 'boy', 'up', 'in', 'the', 'air', 'and', 'waiting', 'for', 'him', 'to', 'come', 'down', 'so', 'he', 'can', 'catch', 'him', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'a', 'in', 'in', 'a', 'in', 'in', 'sox', 'range', 'paddles', 'scottish', 'remote', 'dew', 'fences', 'streaks', 'kisses', 'top', 'containing', 'violin', 'motorcycles', 'punches', 'punches', 'easel', 'coverings']\n"
     ]
    }
   ],
   "source": [
    "# Randomly select one sentence from Test Data to Predict.\n",
    "data_index = random.randint(0,100)\n",
    "# Since its only 1 sentence, we need to convert into a 2-D list before sending it to torch.tensor()\n",
    "Xe_b = torch.tensor([X_encoder_indices_test[data_index]]).to(device)\n",
    "print(\"Test Jumbled sentence:\",X_encoder_tokens_test[data_index])\n",
    "print(\"Test Unjumbled sentence:\", Y_tokens_test[data_index]) \n",
    "attention_matrix, _ = predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" #torch.device(\"cuda:0\")\n",
    "batch_size = 100\n",
    "INPUT_DIM = len(token_index_mapper.token_to_index) # Size of source vocabulary \n",
    "OUTPUT_DIM = len(token_index_mapper.token_to_index) # Size of target vocabulary \n",
    "ENC_EMB_DIM = 128 \n",
    "DEC_EMB_DIM = 128 \n",
    "HID_DIM = 400 \n",
    "ENC_DROPOUT = 0.5 \n",
    "DEC_DROPOUT = 0.5\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT) \n",
    "attn = Attention(2*HID_DIM, 2*HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, 2*HID_DIM) \n",
    "model = Seq2Seq(enc, attn, dec, device, 0.9, max_sequence_length, token_index_mapper).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters()) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_index_mapper.token_to_index[\"<pad>\"])\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 Loss: 8.57169246673584\n",
      "Epoch: 0 Batch: 3000 Loss: 4.3253960609436035\n",
      "Epoch: 0 Batch: 6000 Loss: 3.867767333984375\n",
      "Epoch: 0 Batch: 9000 Loss: 3.3064990043640137\n",
      "Epoch: 0 Batch: 12000 Loss: 3.38032865524292\n",
      "Epoch: 0 Batch: 15000 Loss: 3.3024888038635254\n",
      "Epoch: 0 Batch: 18000 Loss: 2.9526124000549316\n",
      "Epoch: 0 Batch: 21000 Loss: 3.2422266006469727\n",
      "Epoch: 0 Batch: 24000 Loss: 2.8822708129882812\n",
      "Epoch: 0 Batch: 27000 Loss: 2.7922346591949463\n",
      "Epoch: 0 Batch: 30000 Loss: 2.482647657394409\n",
      "______________________________________\n",
      "Epoch Loss: 1056.891522884369\n",
      "Test Jumbled sentence: ['running', 'a', 'beach', 'white', 'rocky', 'is', 'down', 'a', '.', 'dog', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'white', 'dog', 'is', 'running', 'down', 'a', 'rocky', 'beach', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'white', 'dog', 'is', 'running', 'a', 'white', 'dog', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 1 Batch: 0 Loss: 2.4957590103149414\n",
      "Epoch: 1 Batch: 3000 Loss: 2.4972219467163086\n",
      "Epoch: 1 Batch: 6000 Loss: 2.2046241760253906\n",
      "Epoch: 1 Batch: 9000 Loss: 2.2834315299987793\n",
      "Epoch: 1 Batch: 12000 Loss: 2.2311055660247803\n",
      "Epoch: 1 Batch: 15000 Loss: 2.162198066711426\n",
      "Epoch: 1 Batch: 18000 Loss: 2.24660587310791\n",
      "Epoch: 1 Batch: 21000 Loss: 2.0371997356414795\n",
      "Epoch: 1 Batch: 24000 Loss: 2.048691987991333\n",
      "Epoch: 1 Batch: 27000 Loss: 2.22346568107605\n",
      "Epoch: 1 Batch: 30000 Loss: 1.6865742206573486\n",
      "______________________________________\n",
      "Epoch Loss: 679.9030491113663\n",
      "Test Jumbled sentence: ['through', 'snow', 'runs', 'the', '.', 'dog', 'a', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'dog', 'runs', 'through', 'the', 'snow', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'dog', 'runs', 'through', 'the', 'snow', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 2 Batch: 0 Loss: 1.876753330230713\n",
      "Epoch: 2 Batch: 3000 Loss: 1.7539210319519043\n",
      "Epoch: 2 Batch: 6000 Loss: 1.6613177061080933\n",
      "Epoch: 2 Batch: 9000 Loss: 1.7097588777542114\n",
      "Epoch: 2 Batch: 12000 Loss: 1.733073115348816\n",
      "Epoch: 2 Batch: 15000 Loss: 1.678222894668579\n",
      "Epoch: 2 Batch: 18000 Loss: 1.5231105089187622\n",
      "Epoch: 2 Batch: 21000 Loss: 1.4970250129699707\n",
      "Epoch: 2 Batch: 24000 Loss: 1.4982926845550537\n",
      "Epoch: 2 Batch: 27000 Loss: 1.6232041120529175\n",
      "Epoch: 2 Batch: 30000 Loss: 1.3094013929367065\n",
      "______________________________________\n",
      "Epoch Loss: 520.1099264621735\n",
      "Test Jumbled sentence: ['boston', 'in', 'is', 'terrier', '.', 'grass', 'the', 'a', 'running', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'boston', 'terrier', 'is', 'running', 'in', 'the', 'grass', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'german', 'shepherd', 'is', 'running', 'in', 'the', 'grass', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 3 Batch: 0 Loss: 1.3248323202133179\n",
      "Epoch: 3 Batch: 3000 Loss: 1.461515188217163\n",
      "Epoch: 3 Batch: 6000 Loss: 1.3809338808059692\n",
      "Epoch: 3 Batch: 9000 Loss: 1.305890440940857\n",
      "Epoch: 3 Batch: 12000 Loss: 1.4334861040115356\n",
      "Epoch: 3 Batch: 15000 Loss: 1.3213317394256592\n",
      "Epoch: 3 Batch: 18000 Loss: 1.5940004587173462\n",
      "Epoch: 3 Batch: 21000 Loss: 1.2163987159729004\n",
      "Epoch: 3 Batch: 24000 Loss: 1.7946573495864868\n",
      "Epoch: 3 Batch: 27000 Loss: 1.315950632095337\n",
      "Epoch: 3 Batch: 30000 Loss: 1.1124712228775024\n",
      "______________________________________\n",
      "Epoch Loss: 423.0197324156761\n",
      "Test Jumbled sentence: ['dog', 'a', 'running', 'near', 'on', 'a', 'garden', 'a', 'brown', 'lawn', 'hose', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'brown', 'dog', 'running', 'on', 'a', 'lawn', 'near', 'a', 'garden', 'hose', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'brown', 'dog', 'running', 'on', 'a', 'grassy', 'lawn', 'near', 'a', 'pond', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 4 Batch: 0 Loss: 1.1714195013046265\n",
      "Epoch: 4 Batch: 3000 Loss: 1.2213112115859985\n",
      "Epoch: 4 Batch: 6000 Loss: 1.125076413154602\n",
      "Epoch: 4 Batch: 9000 Loss: 1.3727221488952637\n",
      "Epoch: 4 Batch: 12000 Loss: 1.2260181903839111\n",
      "Epoch: 4 Batch: 15000 Loss: 1.1315014362335205\n",
      "Epoch: 4 Batch: 18000 Loss: 1.018782615661621\n",
      "Epoch: 4 Batch: 21000 Loss: 0.9693744778633118\n",
      "Epoch: 4 Batch: 24000 Loss: 1.177884817123413\n",
      "Epoch: 4 Batch: 27000 Loss: 0.9922347664833069\n",
      "Epoch: 4 Batch: 30000 Loss: 1.02741539478302\n",
      "______________________________________\n",
      "Epoch Loss: 349.1413167119026\n",
      "Test Jumbled sentence: ['a', 'is', 'blond', 'street', '.', 'taxi', 'on', 'the', 'woman', '<unk>', 'a', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'blond', 'woman', 'is', 'on', 'the', 'street', '<unk>', 'a', 'taxi', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'woman', 'is', 'performing', 'a', '<unk>', 'on', 'the', 'street', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 5 Batch: 0 Loss: 0.9414377808570862\n",
      "Epoch: 5 Batch: 3000 Loss: 0.951230525970459\n",
      "Epoch: 5 Batch: 6000 Loss: 0.8239315748214722\n",
      "Epoch: 5 Batch: 9000 Loss: 0.8590590357780457\n",
      "Epoch: 5 Batch: 12000 Loss: 1.0628600120544434\n",
      "Epoch: 5 Batch: 15000 Loss: 0.7706053853034973\n",
      "Epoch: 5 Batch: 18000 Loss: 1.0784854888916016\n",
      "Epoch: 5 Batch: 21000 Loss: 0.855292797088623\n",
      "Epoch: 5 Batch: 24000 Loss: 0.8991182446479797\n",
      "Epoch: 5 Batch: 27000 Loss: 0.934110164642334\n",
      "Epoch: 5 Batch: 30000 Loss: 0.904064953327179\n",
      "______________________________________\n",
      "Epoch Loss: 287.05991566181183\n",
      "Test Jumbled sentence: ['on', 'a', 'fight', 'a', 'group', '.', 'children', 'young', 'bed', 'pillow', 'of', 'playing', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'group', 'of', 'young', 'children', 'playing', 'pillow', 'fight', 'on', 'a', 'bed', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'group', 'of', 'young', 'children', 'playing', 'on', 'a', 'bed', 'laughing', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 6 Batch: 0 Loss: 0.8194403052330017\n",
      "Epoch: 6 Batch: 3000 Loss: 0.7730044722557068\n",
      "Epoch: 6 Batch: 6000 Loss: 0.7361225485801697\n",
      "Epoch: 6 Batch: 9000 Loss: 0.6843681335449219\n",
      "Epoch: 6 Batch: 12000 Loss: 0.7844181656837463\n",
      "Epoch: 6 Batch: 15000 Loss: 0.6887960433959961\n",
      "Epoch: 6 Batch: 18000 Loss: 0.9029035568237305\n",
      "Epoch: 6 Batch: 21000 Loss: 0.6320393681526184\n",
      "Epoch: 6 Batch: 24000 Loss: 0.7308370471000671\n",
      "Epoch: 6 Batch: 27000 Loss: 0.7510167360305786\n",
      "Epoch: 6 Batch: 30000 Loss: 0.8707401156425476\n",
      "______________________________________\n",
      "Epoch Loss: 240.91255915164948\n",
      "Test Jumbled sentence: ['waters', 'kayak', '<unk>', 'a', 'child', 'and', 'man', '.', 'through', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'man', 'and', 'child', 'kayak', 'through', '<unk>', 'waters', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'man', '<unk>', 'child', 'and', 'child', 'fly', '<unk>', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 7 Batch: 0 Loss: 0.7561707496643066\n",
      "Epoch: 7 Batch: 3000 Loss: 0.657155454158783\n",
      "Epoch: 7 Batch: 6000 Loss: 0.6479390263557434\n",
      "Epoch: 7 Batch: 9000 Loss: 0.7868594527244568\n",
      "Epoch: 7 Batch: 12000 Loss: 0.8016390204429626\n",
      "Epoch: 7 Batch: 15000 Loss: 0.49800610542297363\n",
      "Epoch: 7 Batch: 18000 Loss: 0.5753823518753052\n",
      "Epoch: 7 Batch: 21000 Loss: 0.5387105941772461\n",
      "Epoch: 7 Batch: 24000 Loss: 0.727409839630127\n",
      "Epoch: 7 Batch: 27000 Loss: 0.6892474293708801\n",
      "Epoch: 7 Batch: 30000 Loss: 0.5002257227897644\n",
      "______________________________________\n",
      "Epoch Loss: 202.94664844870567\n",
      "Test Jumbled sentence: ['front', 'in', 'standing', 'man', 'skyscraper', 'is', 'of', 'a', 'a', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'man', 'is', 'standing', 'in', 'front', 'of', 'a', 'skyscraper', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'man', 'is', 'standing', 'in', 'front', 'of', 'a', 'garage', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 8 Batch: 0 Loss: 0.4856874644756317\n",
      "Epoch: 8 Batch: 3000 Loss: 0.5228656530380249\n",
      "Epoch: 8 Batch: 6000 Loss: 0.47808924317359924\n",
      "Epoch: 8 Batch: 9000 Loss: 0.6830153465270996\n",
      "Epoch: 8 Batch: 12000 Loss: 0.6402415037155151\n",
      "Epoch: 8 Batch: 15000 Loss: 0.606595516204834\n",
      "Epoch: 8 Batch: 18000 Loss: 0.5942725539207458\n",
      "Epoch: 8 Batch: 21000 Loss: 0.5218948721885681\n",
      "Epoch: 8 Batch: 24000 Loss: 0.44014447927474976\n",
      "Epoch: 8 Batch: 27000 Loss: 0.4869060516357422\n",
      "Epoch: 8 Batch: 30000 Loss: 0.41623935103416443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________\n",
      "Epoch Loss: 167.81465411186218\n",
      "Test Jumbled sentence: ['the', 'black', 'runs', 'dog', 'the', '.', 'through', 'water', '', '<eos>']\n",
      "Test Unjumbled sentence: ['the', 'black', 'dog', 'runs', 'through', 'the', 'water', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['the', 'black', 'dog', 'runs', 'through', 'the', 'water', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "Epoch: 9 Batch: 0 Loss: 0.402446985244751\n",
      "Epoch: 9 Batch: 3000 Loss: 0.5237300395965576\n",
      "Epoch: 9 Batch: 6000 Loss: 0.3517068326473236\n",
      "Epoch: 9 Batch: 9000 Loss: 0.42072418332099915\n",
      "Epoch: 9 Batch: 12000 Loss: 0.43324390053749084\n",
      "Epoch: 9 Batch: 15000 Loss: 0.35038942098617554\n",
      "Epoch: 9 Batch: 18000 Loss: 0.40581122040748596\n",
      "Epoch: 9 Batch: 21000 Loss: 0.395102322101593\n",
      "Epoch: 9 Batch: 24000 Loss: 0.45425039529800415\n",
      "Epoch: 9 Batch: 27000 Loss: 0.5414351224899292\n",
      "Epoch: 9 Batch: 30000 Loss: 0.496710866689682\n",
      "______________________________________\n",
      "Epoch Loss: 143.49759796261787\n",
      "Test Jumbled sentence: ['day', 'a', 'on', 'a', 'on', 'tractor', 'playing', 'a', 'family', 'beautiful', '', '<eos>']\n",
      "Test Unjumbled sentence: ['a', 'family', 'playing', 'on', 'a', 'tractor', 'on', 'a', 'beautiful', 'day', '', '<eos>']\n",
      "_______________________________________\n",
      "['a', 'family', 'playing', 'a', 'bagpipe', 'on', 'a', 'muddy', 'day', '', '<eos>']\n",
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for j in range(0,len(X_encoder_indices_tr),batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        Xe_b = torch.tensor(X_encoder_indices_tr[j:j+batch_size]).to(device)\n",
    "        Xd_b = torch.tensor(X_decoder_indices_tr[j:j+batch_size]).to(device)\n",
    "        Y_b = torch.tensor(Y_indices_tr[j:j+batch_size]).to(device)\n",
    "        op = model(Xe_b,Xd_b)\n",
    "        loss = criterion(op,Y_b.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        if j%3000 == 0: print(\"Epoch:\",i,\"Batch:\",j,\"Loss:\",batch_loss)\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Epoch Loss:\",epoch_loss)\n",
    "    \n",
    "    # Randomly select one sentence from Test Data to Predict.\n",
    "    data_index = random.randint(0,100)\n",
    "    # Since its only 1 sentence, we need to convert into a 2-D list before sending it to torch.tensor()\n",
    "    Xe_b = torch.tensor([X_encoder_indices_test[data_index]]).to(device)\n",
    "    print(\"Test Jumbled sentence:\",X_encoder_tokens_test[data_index])\n",
    "    print(\"Test Unjumbled sentence:\", Y_tokens_test[data_index]) \n",
    "    attention_matrix, _ = predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length)\n",
    "    print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Jumbled sentence: ['people', 'of', '.', 'beach', 'crowd', 'the', 'at', '', '<eos>']\n",
      "Test Unjumbled sentence: ['crowd', 'of', 'people', 'at', 'the', 'beach', '.', '', '<eos>']\n",
      "_______________________________________\n",
      "['crowd', 'of', 'people', 'at', 'the', 'beach', '.', '', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "data_index = random.randint(0,100)\n",
    "# Since its only 1 sentence, we need to convert into a 2-D list before sending it to torch.tensor()\n",
    "Xe_b = torch.tensor([X_encoder_indices_test[data_index]]).to(device)\n",
    "print(\"Test Jumbled sentence:\",X_encoder_tokens_test[data_index])\n",
    "print(\"Test Unjumbled sentence:\", Y_tokens_test[data_index]) \n",
    "attention_matrix, predicted_sentence = predict_on_1_input(model, Xe_b, token_index_mapper, device, max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niprakash\\AppData\\Local\\Temp\\ipykernel_45820\\3811298408.py:4: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + X_encoder_tokens_test[data_index], rotation=90)\n",
      "C:\\Users\\niprakash\\AppData\\Local\\Temp\\ipykernel_45820\\3811298408.py:5: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + Y_tokens_test[data_index])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAADxCAYAAAAp+dtsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgKElEQVR4nO3deZxcZZ3v8c83YYnsYFCBoCziaETWDLKIIOoAMwp6RVnd4GWuIorD6JWZ63aZq9fluqG4REUQFAQcnYzKRAclIIISIAQCohFBgowMW4ggkHR/549zCoq2u+t0UlWnlu/79TqvrnPqqVO/k1TXr5/lPI9sExERw21a3QFERET9kgwiIiLJICIikgwiIoIkg4iIIMkgIiJIMoiI6CuSzpR0t6QbJ3hekk6XtEzSEkl7VDlvkkFERH85CzhkkucPBXYqt7nAF6ucNMkgIqKP2L4MuG+SIocD33DhKmAzSVu1Ou867QowIiLGd/BLNvS9941UKnvNkkeXAo80HZpne94U3m4b4I6m/eXlsbsme1GSQUREh91z3wi/WDCrUtl1t/rtI7bndDikv5BkEBHRcWbEo916szuBbZv2Z5XHJpU+g4iIDjMwiittbTAfeEM5qmhvYIXtSZuIIDWDiIiuGKU9NQNJ5wEHAjMlLQc+CKwLYPtLwA+BvwWWAQ8Db65y3iSDiIgOM2ZVm5qJbB/d4nkDb5/qeZMMIiI6zMBIe5qAOibJICKiC9rUH9AxSQYRER1mYKTHV5VMMoiI6IKuDSxdQ0kGEREdZpw+g4iIYWfDqt7OBUkGERGdJ0ZQ3UFMKskgIqLDDIymZhAREakZREQMueKmsySDiIihZmCVe3te0CSDkqQNbD9cdxwRMXiMGOnxSaJ7O7oukLSvpJuAX5X7u0r6Qs1hRcSAGbUqbXUZ+mQAfBo4GLgXwPb1wItrjSgiBkqjz6DKVpc0EwG275Ce9J9QbbHSiIhKxEj6DHreHZL2BSxpXeBk4OaaY4ohIml94DXAdjT9Tto+ra6Yor2Klc6SDHrdW4HPAttQrBP6I9ZgYYiItfCvwArgGuDRmmNZK5JWwsST8NjepIvh9AxbPObpdYcxqaFPBrbvAY6tO44YarNsH1J3EO1ge2MASf8M3AWcA4jid2yrGkOr3WjuM+hNkj7H5H/BvLOL4cRw+7mkF9i+oe5A2ugw27s27X9R0vXAB+oKqE5FB3KaiXrVoroDiOEm6QaK74l1gDdLupWimUgUS9nuUmd8a+khSccC51Nc49HAQ/WGVKd0IPcs22c370vapDjslTWFFMPnFXUH0EHHUPTFfZYiGVxRHhtK6UDuA5LmAF8HNi529QBwvO1rag0sBp7t2wEk7Q0sbfwhUv5h8jzg9hrDW1v/afvwuoPoJSM13lBWxdAnA+BM4ETblwNIehFFcujnKnr0ly8CezTt/2mcY/3mRkl/BC4vt5/ZXlFzTLUxYpV7++u2t+st3THSSAQAtn8GrK4xnhg+sp9YLd32KH3+h5rtZ1P0E9wA/B1wvaTFtQZVo0YHcpWtLn39gWuThZK+DJxH8X92JHCppD0AbF9bZ3AxFG6V9E6K2gDAicCtNcaz1iTNAvYD9gd2BZYCP6s1qBoZpZmoDzSGv31wzPHdKZLDQd0NJ4bQW4HTgfdRfOYuAebWGtHa+z1wNfAR22+tO5hekA7kHmf7JXXHEMPN9t3AUXXH0Wa7Ay8CjpF0KvAbYKHtr9UbVj1sMrS010nalKJW0JipdCFw2jB3dkV3SZoBnAA8H5jROG77+NqCWku2r5f0W+C3FE1FxwEHAMOZDBCrenw6it5OVd1xJrASeF25PUgxmiiiW84BnkExlfpCYBbFZ7JvSVoEXAm8mmLixxfbfla9UdUrHci9b0fbr2na/z/DPOohavFs26+VdLjtsyV9i2I4Zj871PZ/1R1ErzD1LlxTRWoG8Ofy3gIAJO0H/LnGeGL4rCp/PiBpZ2BT4Gk1xtMOj0n6lKRF5fbJskl2aKVm0PveBpzd9EG9H3hTfeHEEJonaXPg/cB8YCP6f0K3M4EbKZpeAV5P0fz6P2qLqEYGRtOB3NtsLwZ2LacAwPaD9UYUw8b2V8uHC4Ed6oyljdL8+iT1LmlZRW+nqi6Q9HRJXwO+bftBSbMlnVB3XDE8Gp9BSReX+4PwGUzzaxMDqzy90laXoU8GwFnAAmDrcv/XwLvqCiaG0lkM3mfwrcAZkm6TdBvweeB/1htSfWwx6mmVtrokGcBM2xcAowC2VwMj9Ya0ZiSdU/48ue5YYkoG5jMIIGk68PpycZtdgF1s7257Sc2h1WrE0yptVUg6RNItkpaVN/WNff6Zkn4q6TpJSyT9batzJhkUi3A8lXLVs3I64X694WxPSVsDx0vaXNIWzVvdwcWEBukziO0RiruPsf1g+uEa6xmo0tZKmWzPAA4FZgNHS5o9ptj7gAts705xd/sXWp136DuQgVMoRnDsIOkKYEvgiHpDWmNfopjXZgeKxdUbRPF5HJTOyUEzSJ/BhuskzQcupGmFM9v/Ul9IdWrrSmd7Acts3wog6XzgcOCmpjIGNikfbwr8odVJkwyKf8DvAg9T3PX5PYo2275j+3TgdElfpEgMjSk2LrN9fX2RRQsD8xlsMgO4lydP9GhgKJNBMbS08miimeUd3A3zbM9r2t8GuKNpfznwwjHn+BDwI0nvADYEXtbqTZMM4BsUU1B8pNw/hmJ6gNfWFtHa+xVwLsUvnoBzJH3F9ufqDSsmMIifwWnAybYfACjvo/hkrRHVaIpzE91je85avuXRwFm2PylpH4rvgJ3LtTLGlWQAO9tubm/7qaSbJizdH04A9rb9EICkj1HME5Nk0JsG8TO4SyMRANi+X9LuNcZTuzZOYX0nsG3T/qzyWLMTgEMAbF9ZToY4E7h7opOmAxmuLTvsAJD0QmDRJOX7gXjyaJSR8lj0pkH8DE4rawMAlAMYhvaPz2IKa1XaKrga2EnS9pLWo+ggnj+mzO+BlwJIeh5Fs92kc0UN7X9Okz2Bn0v6fbn/TOAWSTcAtt2PayF/HfiFpO+W+69iQKcOlvQM2/9ZdxxrovEZA9blic+ggWdRNPX1s08CV0q6sNx/LfDhGuOpXbsmqrO9WtJJFPemTAfOtL1U0mnAItvzgX8AviLp7yk+U29qXlp1PGrx/MCTNOm0urZv71Ys7VQu29m4A/Ry29fVGU+nSPqB7b+rO441MaifvYZyuGOjA/kntvu96WuNPW32U33kuYdUKvv5Pb91TRv6DKZs6GsG/f4LN5Fy7eaBX7+5XxMBDO5nr6H88h/aBNCsmI6it1vlhz4ZRER0njJraUREUOnu4jr1dqrqMklz646hEwbxugbxmmAwr2sQr2mq2jyaqCOSDJ5sUD+0g3hdg3hNMJjXNYjXNGW9PmtpmokiIjqsH9ZAHtpksJ7W9ww2fNKxGWzAJtpi4MbaDuJ1DeI1wfjX9ZxdHq78+l8v2aDtMa2tQfi/Wsn999jeck1fb2B1OpB70ww25IV6ad1hRLS0YMHiymUP3nq3jsUxzP7DF631MOCMJuoySX+yvVHdcUREPM5pJqpE0jrl6k4REQOnsbhNL+taMpD0BuDdFP8uSygmT3sE2B24QtI3KObg3wD4LXA8xZwtF9veU9KuwGLgWbZ/L+m3wAuApwPfAjYC/rVb1xMRMRW9XjPoSiOWpOdTLMN2ULkuamON3lnAvrZPoZjT/b3lxHA3AB+0fTcwQ9ImwP4UMznuX87pcrfth4HPAl+0/QLgrhZxzJW0SNKiVTzagSuNiPhLjcVtqmx16VaPxkHAhbbvAbB9X3n8QtsjkjYFNrO9sDx+Nk+s0vVzYL9y/yPlz/2By8vn9wPOKx+fM1kQtufZnmN7zrqs34bLiohozYjVo9MqbXWpu3v7odZFuIziy/9ZFM1Au1LMxnl5U5m+HrYWEYNv7ML3E2116VYy+AnwWklPhccXunic7RXA/ZL2Lw+9HmjUEi4HjgN+Uy7Zdh/wt8DPyuevoFjcAeDYjl1BRMSacu83E3WlA7lceOHDwEJJI8B4c+u/EfiSpA2AW4E3l6+9TZIoaghQJIFZtu8v908GviXpvaQDOSJ6UKPPoJd1bTSR7bMp+gImen4xsPcEz23b9PgjPLFwOLZ/B+zTVPx9axtrRC/p1I1kC/6wuPYYhkmSQUTEkDNipMbO4SqSDCIiuiA3nUVEDDm795uJWtZbJG0g6f2SvlLu7yTpFZ0PLSJicNiqtNWlSiPW14FHeaKT9k7g/3YsooiIgVNtWGmv34G8o+2PA6sAyikgeru+ExHRY3q9ZlClz+AxSU+hvMtX0o6QiX0iIqqyYWS0t/+GrpIMPgj8O7CtpG9SzAX0pk4GFRExaPp+NJHtH0u6luKGMAEnNyaci4iI1gy1NgFVUWU00auB1bZ/YPv7wGpJr+p4ZBERA6P3O5ArNRPZ/m5jx/YDkj4IfK9jUUXE43a+pvqdqzfuOVq5bD9NMfHYwXMql11vwaIORrLm3ONzK1dJBuN9EnOzWkTEFPR9MxGwSNKnJO1Ybp8Crul0YGtK0jsl3Vx2dkdE1K4YTTSt0laXKu/8DuAx4Nvl9ijw9k4GtZZOBF5uO2sbRETPsKttdakymugh4NQuxDJlkk4Bji93vwo8F9gBuFjSmbY/XVtwERFNer2ZqGUykPQc4N3Ads3lbR/UubBak7QnxQI4L6QY8voLihXRDgFeMt7wV0lzgbkAM9ige8FGxFAz9d5dXEWVjuALgS9R/OU90tlwpuRFwHfLmguS/oVireQJ2Z4HzAPYRFv0eN9+RAySXv/CqZIMVtv+YscjiYgYVAa3cToKSYcAnwWmA1+1/dFxyrwO+FDx7lxv+5jJzlmlA/nfJJ0oaStJWzS2qYffdpcDryqn2N4QeHV5LCKi57RrojpJ04EzgEOB2cDRkmaPKbMT8I/AfrafD7yr1Xmr1AzeWP58T/N1UXTU1sb2tZLOAn5ZHvqq7euk3m6Xi4jh1MaRQnsBy2zfCiDpfOBw4KamMm8BzrB9f/HevrvVSauMJtp+jcLtAtufAj415th29UQTU/Gd5VdVLvuaWXt3JAatv36lcn603kl6p3JX8aDq1buKq5ri3EQzJTVf8Lyyv7NhG+COpv3lFANpmj0HQNIVFE1JH7L975O9aZXRRBsApwDPtD23rH78VTlPUUREtGKgejK4x3b1+TfGtw6wE3AgMAu4TNILbD8w0QuqrnT2GLBvuZ+VziIipqiNN53dCWzbtD+rPNZsOTDf9irbvwN+TZEcJpSVziIiOk54tNpWwdXATpK2l7QecBQwf0yZ71HUCpA0k6LZ6NbJTlolGWSls4iIteWKW6vT2KuBk4AFwM3ABbaXSjpN0mFlsQXAvZJuAn4KvMf2vZOdNyudRUR0mts7HYXtHwI/HHPsA02PTdHXe0rVc2als4iIbujxW5CrjCZ6cflwZflztiRsX9a5sCIiBk1vd7VWaSZqvtlsBsUND9cAtU5UFxHRV3r8dpEqzUSvbN6XtC3wmU4FFBExcKZ2n0Et1mT5yuXA89odSETEIOv7NZAlfY4nuj6mAbsB13YwpuiGKczhNK3itA0Ao488UqncETseUPmcnRrJfOf5O1Yqt/Wrb2pdqIO0TvW/2bx6dQcjqc+K46pPSbLpudWnOumqfk8GQPMcGauB82xf0aF4IiIGU783E9k+uxuBREQMMvV7zUDSDYxfwRHFvQ27tD2qCiS9CZhj+6Q63j8iojIL2ri4TSdUaSa6uPx5Tvnz2PJnVj+LiKiqx2sGVeYmernt/2X7hnI7Ffgb27fbvn2iF0naTtKvJH1T0s2SLipXJdtT0kJJ10haIGmrsvxukq6StETSdyVtXh6/VNJnJS2WdKOkvcZ5ry0lfUfS1eW235r+g0REdESb5ibqlCrJQM1frpL2rfg6gL8CvmD7ecCDwNuBzwFH2N4TOBP4cFn2G8B7y2anGyjmRGrYwPZuwInla8b6LPBp238NvAb46gQXMlfSIkmLVmWuvYjoph5PBlWaiU4AzpS0abn/AHB8xfPf0TTy6Fzgn4CdgR+Xy1NOB+4qz72Z7YVl2bOBC5vOcx6A7cskbSJpszHv8zLKaTJKm0jayPafmguVqwXNKwps0eOVtogYGINw05nta4BdG8nA9oopnH/sF+5KYKntfZoPNiWaqucZuz8N2Nt2tUHuERFd1uujiVo290h6uqSvAefbXiFptqQTKp7/mZIaX/zHAFcBWzaOSVpX0vPLBHO/pP3Lsq8HFjad58iy/IuAFeMkpB8B72iKebeK8UVEdEePNxNVafs/i2KhhK3L/V8D76p4/luAt0u6Gdicsr8A+Jik64HFPLGc5huBT0haQnGX82lN53lE0nXAlyiarcZ6JzCn7Hy+CXhrxfgiIrpCrrbVpUqfwUzbF0j6RyhW2ZE0UvH8q20fN+bYYuDFYwvaXkyxZsJ4zrX9rjHlz6JIVJTrKxxZMaaAKU2UUnWKiSm9/aP1d+DXPc1EVYM6xcRUXPXxL1Uue/C5u3UukLXR730GwEOSnsoTy17uDUyl3yAiYrjV3ARURZVkcArFYss7SroC2JKiqWdStm+jGDm0VmwfuLbniIioXb8nA9vXSjqA4p4BAbfYXtXxyCIiBoh6fHGbCTuQJf21pGdA0U8A7Elxg9gnJW3RpfgiIgZDH48m+jLwGDy+DvJHKe4SXkF541ZERLRWdSRRr44mmm77vvLxkcA8298BviNpcccji4gYJD0+mmiymsF0SY1k8VLgJ03PrclymRERw6vHm4km+1I/D1go6R7gz8DlAJKeTYaWRkRMSa9PRzFhMrD9YUmXAFsBP7Ifv0tpGk1TP0RERAvu/dFEkzb32P6LlaVt/7pz4UREDKh+rRlEf/rDe/ZtXQjY+hM/73AkMYx+d371VXC3P2pJ5bIHb73bGkTTY5IMIiKi1/sMqkxh/bEqx3qBpH+qO4aIiH5UaQ3kcY4d2u5A2iTJICJ6U78OLZX0Noo1h3co1xho2Bi4YvxXdY+k7wHbAjMo1kDeAXhKeUPcUtvH1hddRESTPh9N9C3gYuD/Aac2HV/ZdGdynY63fZ+kpwBXAwcAJ9nebaIXSJoLzAWYwQZdCTIiAuj5DuQJm4lsr7B9m+2jKf4CP8j27cA0Sdt3LcKJvbNcLe0qivh2avUC2/Nsz7E9Z13W73iAERFQTPfcz3MTASDpg8Aciimsvw6sB5wL7NfZ0CaN6UDgZcA+th+WdClFc1FERG/q15pBk1cDhwEPAdj+A0W/QZ02Be4vE8FzeWK5zFWS1q0xroiIv9TmWUslHSLpFknLJJ06SbnXSLKkOa3OWSUZPFZORdFY9nLDauF21L8D60i6mWJq7cad0vOAJZK+WVtkERHjGa24tSBpOnAGxajO2cDRkmaPU25j4GTgF1XCq3LT2QWSvgxsJuktwPHAV6qcvFNsP8r4w1svBd7b3WgiIlprY3/AXsAy27cCSDofOBy4aUy5fwY+BrynykmrLHv5/yW9HHiQot/gA7Z/PIXAo4v6ZZqJaRtUH801+vDDHYykvc6/o/q//1HbVps6pJ9MZYqJoVM9GcyUtKhpf57t5gXFtgHuaNpfDryw+QSS9gC2tf0DSe1JBgDll38SQETEmpjaDWX32G7Zxj8RSdOATwFvmsrrqowmWslfXsYKYBHwD42qSkRETKyNzUR3Ugynb5hVHmvYGNgZuFQSwDOA+ZIOs91c43iSKjWDz1BUQ75FMVz2KGBH4FrgTODAqlcQETG02pcMrgZ2Ku/3upPiO/mYx9/GXgHMbOyXQ+/fPVkigGqjiQ6z/WXbK20/WLZdHWz728DmU7+OiIjho9FqWyu2VwMnAQuAm4ELbC+VdJqkw9Y0vio1g4clvQ64qNw/AnikEdeavnFExNBo8yR0tn8I/HDMsQ9MUPbAKuesUjM4Fng9cDfwx/LxceWcQCdVeZOIiGGmKWx1mbRmUN7ccKLtV05Q5GftDykiYgD1eDtKqzWQRyS9qFvBREQMql5f6axKn8F1kuYDF1LOTwRg+186FlVExKAZgGQwA7gXOKjpmIEkg1hjFy+rfqduPy2GPoh3FUcb9PniNgDYfnM3AomIGGj9XjOQNAM4AXg+TWsG2D6+g3E13nsz4BjbXyjXMHi37Vd0+n0jItqt1/sMqgwtPYfiduaDgYUUtz6v7GRQTTajWIc5IqK/jV34fqKtJhMmA0mNWsOzbb8feMj22cDfMWaGvA76KLBjucj9J4CNJF0k6VeSvqly4g1Je0paKOkaSQskbdWl+CIiKun1ZS8nqxn8svy5qvz5gKSdKVYZe1pHo3rCqcBvy0Xu3wPsDryLYkGHHYD9ypXNPgccYXtPivmSPtyl+CIiWjNtW9ymU6qMJponaXPgfcB8YCPg/R2NamK/tL0coKwtbAc8QDFD34/LisJ04K7xXixpLjAXYAbV59OPiFgbovf7DCZLBk+TdEr5uDGi6IzyZ11LXz7a9HiEIn4BS23v0+rF5SR78wA20RY9/l8TEQOlx79xJksG0ylqAeNNl9Gty1pJMTf3ZG4BtpS0j+0ry2aj59he2vnwIiKqkXs7G0yWDO6yfVrXIhmH7XslXSHpRuDPFBPljS3zmKQjgNMlbUpxTZ8BkgwiojfUPFKoismSQZ0T6D3O9jETHD+p6fFi4MXdiikiYqr6uc/gpV2LIoZOP00xMRUL/rC4ctlB/TeI8fXtdBS27+tmIBERA62PawYREdEONd9QVkWSQURENyQZREQMt36/6SwiItpEo72dDZIMIiI6rc/vM4iIiDbp26GlERHRRqkZREREOpAjIoadgT6eqG6greT+e/7DF90+5vBM4J464umwQbyunrym6VNaY2/ZeAd78rrW0iBc07PW9gTpM+hRtrcce0zSIttz6oinkwbxugbxmmAwr2sQr2mqcp9BREQUTURpJoqIiNQM+su8ugPokEG8rkG8JhjM6xrEa5q6JIP+Ua6RPHAG8boG8ZpgMK9rEK9pTaRmEBEx7AyM9HY2SDKIiOiCXq8ZTKs7gBhOkv7UgXNuJ2ncNbMlTZN0uqQbJd0g6WpJ27c7hogJNUYUtdoqkHSIpFskLZN06jjPnyLpJklLJF0iqeV9EkkGMUi2A8ZNBsCRwNbALrZfALwaeKA7YUUUNYMqW8vzSNOBM4BDgdnA0ZJmjyl2HTDH9i7ARcDHW503ySBqJelASZdKukjSryR9U5LK526T9PHyL/lfSnp2efwsSUc0naNRy/gosL+kxZL+fsxbbQXcZXsUwPZy2/eXr/8bSVdKulbShZI2Ko8fUsZ0bVmr+H55/EOS3t30/jdK2q58fFwZ62JJXy5/cZH0J0kflnS9pKskPb08/nRJ3y2PXy9p38nOE33KU9ha2wtYZvtW248B5wOHP+nt7J/afrjcvQqY1eqkSQbRC3YH3kXxV84OwH5Nz60o/5L/PPCZFuc5Fbjc9m62Pz3muQuAV5Zfrp+UtDuApJnA+4CX2d4DWAScImkG8BXglcCewDNaXYSk51HUQPazvRswAhxbPr0hcJXtXYHLgLeUx08HFpbH9wCWtjhP9CEBGnGlDZgpaVHTNnfM6bYB7mjaX14em8gJwMWtYkwHcvSCX9peDiBpMUVzz8/K585r+jn2C74y28sl/RVwULldIum1wFMoktAVZYVkPeBK4LnA72z/pozrXGDsL+VYL6VIHFeX53oKcHf53GPA98vH1wAvLx8fBLyhjHEEWCHp9ZOcJ/qUqt+BfE+7pu+QdBwwBzigVdkkg+gFjzY9HuHJn0uP83g1Za1W0jSKL/CWbD9K8RfSxZL+CLwK+BHwY9tHN5eVtNskp3r8/UszGi8Dzrb9j+O8ZpX9+LfB2Gsca7LzRD9q70pndwLbNu3PKo89iaSXAf8bOKD87E8qzUTR645s+nll+fg2ir+cAQ4D1i0frwQ2Hu8kkvaQtHX5eBqwC3A7RXvqfk39ERtKeg7wK2A7STuWp2hOFrdRNOkgaQ+gMSrpEuAISU8rn9uiwiiOS4C3leWnS9p0Dc8TPa3iSKJqtYergZ0kbS9pPeAoYH5zgbIZ9MvAYbYr1SqTDKLXbS5pCXAy0OgU/gpwgKTrgX2Ah8rjS4CRsiN2bAfy04B/k3RjWW418Hnb/wW8CTivfJ8rgefafoSiWegHkq7lyc003wG2kLQUOAn4NYDtmyj6H35UnuvHFB3XkzkZeImkGyiaj2av4Xmix7VrNJHt1RSfuwXAzcAFtpdKOk3SYWWxTwAbAReW/WTzJzhdU3w9PpNeDC9Jt1EMj6t9LnxJBwLvtv2KmkOJPrTJxtt4r91PrFT2ksvfd00dU36nzyAiotNMY6RQz0oyiJ5le7u6Y2iwfSlwac1hRD/r7VyQZBAR0Q1TGFpaiySDiIhuSDKIiBhyBkbrDmJySQYRER0mnGaiiIgARnu7apBkEBHRaWkmiogIyGiiiIiAjCaKiIjqS1rWJckgIqLTDGQ6ioiISJ9BRESkmSgiYugZGE0yiIgYculAjogISDKIiBh6BkZ6+xbkJIOIiI4zOMkgIiLSTBQRMeQymigiIoDUDCIigiSDiIihZ8PISN1RTCrJICKiG1IziIiIJIOIiKHnjCaKiBh6Buems4iIyHQUERHDzobRJIOIiEgHckREODWDiIhhl8VtIiIiE9VFRIQB9/h0FNPqDiAiYuC5XNymylaBpEMk3SJpmaRTx3l+fUnfLp//haTtWp0zySAiogs86kpbK5KmA2cAhwKzgaMlzR5T7ATgftvPBj4NfKzVeZMMIiK6oX01g72AZbZvtf0YcD5w+JgyhwNnl48vAl4qSZOdNH0GEREdtpL7F/yHL5pZsfgMSYua9ufZnte0vw1wR9P+cuCFY87xeBnbqyWtAJ4K3DPRmyYZRER0mO1D6o6hlTQTRUT0lzuBbZv2Z5XHxi0jaR1gU+DeyU6aZBAR0V+uBnaStL2k9YCjgPljyswH3lg+PgL4iT35XW9pJoqI6CNlH8BJwAJgOnCm7aWSTgMW2Z4PfA04R9Iy4D6KhDEptUgWERExBNJMFBERSQYREZFkEBERJBlERARJBhERQZJBRESQZBAREcB/Aw/3UMWXV4YjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots() \n",
    "cax = ax.matshow(attention_matrix, cmap='viridis') \n",
    "# Set axis labels \n",
    "ax.set_xticklabels([''] + X_encoder_tokens_test[data_index], rotation=90) \n",
    "ax.set_yticklabels([''] + Y_tokens_test[data_index]) \n",
    "# Add colorbar \n",
    "plt.colorbar(cax) \n",
    "plt.xlabel('Input Sequence') \n",
    "plt.ylabel('Target Sequence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(attention_matrix), len(attention_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
