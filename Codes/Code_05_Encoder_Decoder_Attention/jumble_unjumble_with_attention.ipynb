{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> In this notebook, we will try to unjumble a sentence using Encoder-Decoder + Attention Architecture built using <br><br>\n",
    " Recurrent Networks like GRU, LSTM and Bi-directional LSTMs.</h6>\n",
    "<h6> The Data is located here: ../../Datasets/Jumble_Unjumble/ </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 2) (130, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumbled_sentences</th>\n",
       "      <th>unjumbled_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Climate change a issue requiring and global im...</td>\n",
       "      <td>Climate change is a pressing global issue requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The rise. temperatures is gases causing to gre...</td>\n",
       "      <td>The increase in greenhouse gases is causing gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>significantly the to levels. contributes dioxi...</td>\n",
       "      <td>Deforestation contributes significantly to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vital carbon for are Renewable energy emission...</td>\n",
       "      <td>Renewable energy sources are vital for reducin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of power energy. Solar renewable wind and sour...</td>\n",
       "      <td>Solar and wind power are clean, renewable sour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   jumbled_sentences  \\\n",
       "0  Climate change a issue requiring and global im...   \n",
       "1  The rise. temperatures is gases causing to gre...   \n",
       "2  significantly the to levels. contributes dioxi...   \n",
       "3  vital carbon for are Renewable energy emission...   \n",
       "4  of power energy. Solar renewable wind and sour...   \n",
       "\n",
       "                                 unjumbled_sentences  \n",
       "0  Climate change is a pressing global issue requ...  \n",
       "1  The increase in greenhouse gases is causing gl...  \n",
       "2  Deforestation contributes significantly to the...  \n",
       "3  Renewable energy sources are vital for reducin...  \n",
       "4  Solar and wind power are clean, renewable sour...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/Train_400.tsv\",sep=\"\\t\")\n",
    "test_df = pd.read_csv(\"../../Datasets/Jumble_Unjumble/Test_100.tsv\",sep=\"\\t\")\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocab using jumbled_sentences of Train + Test dataset. Ideally only Train dataset should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabBuilder:\n",
    "    def __init__(self,text_corpus,unknown_token=None,pad_token=None,sos_token=None,eos_token=None):\n",
    "        '''\n",
    "        text_corpus = \"This is first sentence. This is second sentence. This is another sentence\"\n",
    "        '''\n",
    "        self.text_corpus = text_corpus\n",
    "        self.unknown_token = unknown_token or \"<unk>\"\n",
    "        self.pad_token = pad_token or \"<pad>\"\n",
    "        self.sos_token = sos_token or \"<sos>\"\n",
    "        self.eos_token = eos_token or \"<eos>\"\n",
    "        self.word_to_index, self.index_to_word = self.get_vocabs()\n",
    "                        \n",
    "    def get_vocabs(self):\n",
    "        word_to_index = {}\n",
    "        index_count = 0\n",
    "        all_unique_words = set(self.text_corpus.split(\" \"))\n",
    "        for index, word in enumerate(all_unique_words):\n",
    "            word_to_index[word] = index\n",
    "        word_to_index[self.pad_token] = index + 1\n",
    "        word_to_index[self.unknown_token] = index + 2\n",
    "        word_to_index[self.sos_token] = index + 3\n",
    "        word_to_index[self.eos_token] = index + 4\n",
    "        index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "        return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate change a issue requiring and global immediate is action. sustained pressing The rise. temperatures is gases causing to greenhouse increase global in significantly the to levels. contributes di\n"
     ]
    }
   ],
   "source": [
    "text_corpus_1 = \" \".join(train_df[\"jumbled_sentences\"].tolist())\n",
    "text_corpus_2 = \" \".join(test_df[\"jumbled_sentences\"].tolist())\n",
    "text_corpus = text_corpus_1 + \" \" + text_corpus_2\n",
    "print(text_corpus[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordToIndex Dict length: 1521\n",
      "IndexToWord Dict length: 1521\n"
     ]
    }
   ],
   "source": [
    "unknown_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "vocab_builder = VocabBuilder(text_corpus,unknown_token,pad_token,sos_token,eos_token)\n",
    "print(\"WordToIndex Dict length:\",len(vocab_builder.word_to_index))\n",
    "print(\"IndexToWord Dict length:\",len(vocab_builder.index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X_Encoder, X_Decoder and Y\n",
    "X_encoder is the matrix of words in jumbled_sentences, each sentence suffixed by \"eos\" token <br>\n",
    "X_decoder is the matrix of unjumbled_sentences, each sentence prefixed by \"sos\" token <br>\n",
    "Y is the matrix of unjumbled_sentences, each sentence suffixed by \"eos\" token <br>\n",
    "\n",
    "Do this for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 400\n",
      "X Encoder test length: 130\n",
      "Sample X_encoder_train: ['Climate', 'change', 'a', 'issue', 'requiring', 'and', 'global', 'immediate', 'is', 'action.', 'sustained', 'pressing', '<eos>']\n",
      "Sample X_decoder_train: ['<sos>', 'Climate', 'change', 'is', 'a', 'pressing', 'global', 'issue', 'requiring', 'immediate', 'and', 'sustained', 'action.']\n",
      "Sample Y_train: ['Climate', 'change', 'is', 'a', 'pressing', 'global', 'issue', 'requiring', 'immediate', 'and', 'sustained', 'action.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "def get_Xe_Xd_Y(dataframe, sos_token, eos_token):\n",
    "    jumbled_sentences = dataframe[\"jumbled_sentences\"].tolist()\n",
    "    unjumbled_sentences = dataframe[\"unjumbled_sentences\"].tolist()\n",
    "    X_encoder_words = [el.split(\" \") + [eos_token] for el in jumbled_sentences]\n",
    "    X_decoder_words = [[sos_token] + el.split(\" \") for el in unjumbled_sentences]\n",
    "    Y_words = [el.split(\" \") + [eos_token] for el in unjumbled_sentences]\n",
    "    return X_encoder_words, X_decoder_words, Y_words\n",
    "\n",
    "X_encoder_words_tr, X_decoder_words_tr, Y_words_tr = get_Xe_Xd_Y(train_df, sos_token, eos_token)\n",
    "X_encoder_words_test, X_decoder_words_test, Y_words_test = get_Xe_Xd_Y(test_df, sos_token, eos_token)\n",
    "print(\"X Encoder train length:\",len(X_encoder_words_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_words_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_words_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_words_tr[0])\n",
    "print(\"Sample Y_train:\",Y_words_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map X_encoder, X_decoder and Y using Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Index_Mapper:\n",
    "    def __init__(self,word_to_index,index_to_word, unknown_token):\n",
    "        self.word_to_index = word_to_index\n",
    "        self.index_to_word = index_to_word\n",
    "        self.unknown_token = unknown_token\n",
    "    \n",
    "    def get_encoding(self,sentence):\n",
    "        '''\n",
    "        sentence must be a list of words.\n",
    "        Ex: [\"Climate\",\"change\",\"is\",\"a\",\"pressing\",\"global\",\"issue\"]\n",
    "        '''\n",
    "        encoded_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in self.word_to_index: encoded_sentence.append(self.word_to_index[word])\n",
    "            else: encoded_sentence.append(self.word_to_index[self.unknown_token])\n",
    "        return encoded_sentence\n",
    "    \n",
    "    def get_decoding(self,encoded_sentence):\n",
    "        '''\n",
    "        encoded_sentence must be a list of vocab indices.\n",
    "        Ex: encoded_sentence = [24,21,4,1,..] \n",
    "        '''\n",
    "        sentence = [self.index_to_word[index] for index in encoded_sentence]\n",
    "        return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_words_to_indices(word_index_mapper, max_sequence_length, word_matrix):\n",
    "    index_matrix = []\n",
    "    for el in word_matrix:\n",
    "        el = el[:max_sequence_length]\n",
    "        if len(el) < max_sequence_length:\n",
    "            pad_tokens_to_append = max_sequence_length - len(el)\n",
    "            el = el + [pad_token]*pad_tokens_to_append\n",
    "        index_matrix.append(word_index_mapper.get_encoding(el))\n",
    "    return index_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoder train length: 400\n",
      "X Encoder test length: 130\n",
      "Sample X_encoder_train: [490, 1269, 668, 400, 338, 952, 164, 794, 781, 838, 179, 43, 1520, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517]\n",
      "Sample X_decoder_train: [1519, 490, 1269, 781, 668, 43, 164, 400, 338, 794, 952, 179, 838, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517]\n",
      "Sample Y_train: [490, 1269, 781, 668, 43, 164, 400, 338, 794, 952, 179, 838, 1520, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517]\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 25\n",
    "word_index_mapper = Word_Index_Mapper(vocab_builder.word_to_index, vocab_builder.index_to_word, unknown_token)\n",
    "X_encoder_indices_tr = map_words_to_indices(word_index_mapper, max_sequence_length, X_encoder_words_tr)\n",
    "X_decoder_indices_tr = map_words_to_indices(word_index_mapper, max_sequence_length, X_decoder_words_tr)\n",
    "Y_indices_tr = map_words_to_indices(word_index_mapper, max_sequence_length, Y_words_tr)\n",
    "X_encoder_indices_test = map_words_to_indices(word_index_mapper, max_sequence_length, X_encoder_words_test)\n",
    "X_decoder_indices_test = map_words_to_indices(word_index_mapper, max_sequence_length, X_decoder_words_test)\n",
    "Y_indices_test = map_words_to_indices(word_index_mapper, max_sequence_length, Y_words_test)\n",
    "print(\"X Encoder train length:\",len(X_encoder_indices_tr))\n",
    "print(\"X Encoder test length:\",len(X_encoder_indices_test))\n",
    "print(\"Sample X_encoder_train:\",X_encoder_indices_tr[0])\n",
    "print(\"Sample X_decoder_train:\",X_decoder_indices_tr[0])\n",
    "print(\"Sample Y_train:\",Y_indices_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unknown token statistics - must be 0, as both Train and Test dataset has been used for vocab creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<sos> The youth are increasingly active in climate advocacy. <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "X_test_temp = []\n",
    "unknown_token_counts = 0\n",
    "for el in X_decoder_indices_test:\n",
    "    temp_list = word_index_mapper.get_decoding(el)\n",
    "    unknown_token_counts += temp_list.count(unknown_token)\n",
    "    X_test_temp.append(temp_list)\n",
    "print(unknown_token_counts)\n",
    "print(X_test_temp[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnjumbleEncoderModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,num_lstm_layers,hidden_size,make_bidirectional,debug):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.bidirectional = make_bidirectional\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gru = nn.GRU(input_size=embedding_dim,hidden_size=hidden_size,dropout=0.5,\n",
    "                            num_layers=num_lstm_layers,bidirectional=make_bidirectional,batch_first=True)\n",
    "        \n",
    "    def forward(self,x,h):\n",
    "        if self.debug: \n",
    "            print(\"_______________________________\")\n",
    "            print(\"\\t\\tEncoder\\t\\t\")\n",
    "            print(\"_______________________________\")\n",
    "        if self.debug: print(\"Before starting: x Shape:\",x.shape,\"Prev State Shape\",h.shape)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.relu(x)\n",
    "        if self.debug: print(\"Embedding, x Shape:\",x.shape)\n",
    "        \n",
    "        op,ht = self.gru(x,h)\n",
    "        if self.debug: print(\"GRU, op Shape:\",op.shape,\"ht shape\",ht.shape)\n",
    "        \n",
    "        if self.bidirectional: \n",
    "            ht_for_decoder = torch.cat((ht[-1],ht[-2]),axis=1)\n",
    "            ht_for_decoder = ht_for_decoder.unsqueeze(0)\n",
    "        else: ht_for_decoder = ht[-1].unsqueeze(0)\n",
    "        if self.debug: print(\"ht for decoder shape\",ht_for_decoder.shape)\n",
    "            \n",
    "        return op,ht,ht_for_decoder\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        first_param = self.num_lstm_layers\n",
    "        if self.bidirectional: first_param *= 2\n",
    "        return torch.zeros(first_param, 1, self.hidden_size)\n",
    "\n",
    "class UnjumbleBahadnauAttention(nn.Module):\n",
    "    def __init__(self,attention_neurons,debug):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.rnn = nn.RNNCell(input_size=attention_neurons,hidden_size=attention_neurons,bias=False)\n",
    "        self.linear = nn.Linear(in_features = attention_neurons, out_features = 1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "    \n",
    "    def forward(self,op_from_enoder,st_minus_one_from_decoder):\n",
    "        \n",
    "        # Reshape the op_from_enoder from (batch_size,seq_length,lstm_neurons) to (batch_size*seq_length,lstm_neurons)\n",
    "        # And reshape st_minus_one_from_decoder from (1,batch_size,lstm_neurons) to (batch_size,lstm_neurons)\n",
    "        # And repeat st_minus_one_from_decoder to seq_length times to get (batch_size*seq_length,lstm_neurons)\n",
    "        if self.debug: \n",
    "            print(\"_______________________________\")\n",
    "            print(\"\\t\\tAttention\\t\\t\")\n",
    "            print(\"_______________________________\")\n",
    "        seq_length = op_from_enoder.shape[1]\n",
    "        op_from_enoder = op_from_enoder.reshape(-1,op_from_enoder.shape[2])\n",
    "        st_minus_one_from_decoder = st_minus_one_from_decoder[-1]\n",
    "        st_minus_one_from_decoder = st_minus_one_from_decoder.repeat(seq_length,1)\n",
    "        if self.debug: print(\"Shape of op_from_encoder:\",op_from_enoder.shape,\n",
    "                             \"Shape of st_minus_one_from_decoder:\",st_minus_one_from_decoder.shape)\n",
    "            \n",
    "        rnn_op = self.rnn(op_from_enoder,st_minus_one_from_decoder)\n",
    "        if self.debug: print(\"RNN Cell Op:\",rnn_op.shape)\n",
    "            \n",
    "        linear_op = self.linear(rnn_op)\n",
    "        if self.debug: print(\"Linear Op:\",linear_op.shape)\n",
    "            \n",
    "        softmax_op = self.softmax(linear_op)\n",
    "        if self.debug: print(\"Softmax Op:\",softmax_op.shape)\n",
    "            \n",
    "        ct = torch.sum(torch.mul(op_from_enoder,softmax_op),dim=0).unsqueeze(0)\n",
    "        if self.debug: print(\"Weighted Averaged h vectors:\",ct.shape)\n",
    "        \n",
    "        return ct,softmax_op\n",
    "        \n",
    "\n",
    "class UnjumbleDecoderModel(nn.Module):\n",
    "    def __init__(self,model_attention,vocab_size,embedding_dim,num_lstm_layers,\n",
    "                 hidden_size,make_bidirectional,debug):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.model_attention = model_attention\n",
    "        self.bidirectional = make_bidirectional\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gru_input_size = embedding_dim  + hidden_size\n",
    "        self.gru = nn.GRU(input_size=self.gru_input_size,hidden_size=hidden_size,\n",
    "                            num_layers=num_lstm_layers,bidirectional=make_bidirectional,batch_first=True)\n",
    "        self.in_features = hidden_size*2 if make_bidirectional else hidden_size\n",
    "        self.linear = nn.Linear(in_features=self.in_features, out_features=vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self,x,s0_from_encoder,op_from_encoder_for_attn):\n",
    "        if self.debug: \n",
    "            print(\"_______________________________\")\n",
    "            print(\"\\t\\tDecoder\\t\\t\")\n",
    "            print(\"_______________________________\")\n",
    "        if self.debug: print(\"Before starting: x Shape:\",x.shape,\" s0_from_encoder Shape:\",s0_from_encoder.shape)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.relu(x)\n",
    "        if self.debug: print(\"Embedding, x Shape:\",x.shape)\n",
    "        \n",
    "        \n",
    "        seq_length = x.shape[1]\n",
    "        if self.debug: print(\"Sequence Length:\",seq_length)\n",
    "        \n",
    "        all_timestep_op = []\n",
    "        for i in range(seq_length):\n",
    "            if i == 0: \n",
    "                ct,softmax_op = self.model_attention(op_from_encoder_for_attn,s0_from_encoder)\n",
    "                concatenated_x = torch.cat((x[0][i].unsqueeze(0),ct),axis=1)\n",
    "                if self.debug: print(\"concatenated_x shape:\",concatenated_x.shape)\n",
    "                gru_op,ht = self.gru(concatenated_x.unsqueeze(0),s0_from_encoder)\n",
    "                \n",
    "            else: \n",
    "                ct,softmax_op = self.model_attention(op_from_encoder_for_attn,ht)\n",
    "                concatenated_x = torch.cat((x[0][i].unsqueeze(0),ct),axis=1)\n",
    "                if self.debug: print(\"concatenated_x shape:\",concatenated_x.shape)\n",
    "                gru_op,ht = self.gru(concatenated_x.unsqueeze(0),ht)\n",
    "            \n",
    "            all_timestep_op.append(gru_op)\n",
    "            if self.debug:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"GRU_OP:\",gru_op.shape,\"Ht:\",ht.shape)\n",
    "                print(\"---------------------------------\")\n",
    "        \n",
    "        gru_final_op = torch.cat(all_timestep_op,axis=1)\n",
    "        if self.debug: print(\"GRU, Final Shape:\",gru_final_op.shape,\"ht shape\",ht.shape)\n",
    "            \n",
    "        # Resizing caption for Linear Layer\n",
    "        gru_final_op = gru_final_op.reshape(-1,gru_final_op.shape[2])\n",
    "        if self.debug: print(\"Reshaping gru_final_op Shape:\",gru_final_op.shape)\n",
    "        \n",
    "        linear_op = self.linear(gru_final_op)\n",
    "        if self.debug: print(\"Linear linear_op Shape:\",linear_op.shape)\n",
    "        \n",
    "        op = self.log_softmax(linear_op)\n",
    "        if self.debug: print(\"log_softmax op Shape:\",op.shape)\n",
    "            \n",
    "        if self.debug:print(\"_______________________________\\n\\n\")\n",
    "            \n",
    "        return op,ht,softmax_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" #torch.device(\"cuda:0\")\n",
    "hidden_size_encoder = 400\n",
    "hidden_size_decoder = hidden_size_encoder\n",
    "model_encoder = UnjumbleEncoderModel(\n",
    "    vocab_size=len(word_index_mapper.word_to_index),embedding_dim=300,num_lstm_layers=2,\n",
    "    hidden_size=hidden_size_encoder,make_bidirectional=True,debug=True\n",
    ").to(device)\n",
    "if model_encoder.bidirectional: hidden_size_decoder = 2*hidden_size_encoder\n",
    "model_attention = UnjumbleBahadnauAttention(hidden_size_decoder,debug=True).to(device)\n",
    "model_decoder = UnjumbleDecoderModel(\n",
    "    model_attention = model_attention,\n",
    "    vocab_size=len(word_index_mapper.word_to_index),embedding_dim=300,num_lstm_layers=1,\n",
    "    hidden_size=hidden_size_decoder,make_bidirectional=False,debug=True\n",
    ").to(device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer_encoder = torch.optim.Adam(model_encoder.parameters(),lr=0.003)\n",
    "optimizer_decoder = torch.optim.Adam(model_decoder.parameters(),lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25]) torch.Size([1, 25]) torch.Size([1, 25])\n",
      "_______________________________\n",
      "\t\tEncoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 25]) Prev State Shape torch.Size([4, 1, 400])\n",
      "Embedding, x Shape: torch.Size([1, 25, 300])\n",
      "GRU, op Shape: torch.Size([1, 25, 800]) ht shape torch.Size([4, 1, 400])\n",
      "ht for decoder shape torch.Size([1, 1, 800])\n",
      "_______________________________\n",
      "\t\tDecoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 25])  s0_from_encoder Shape: torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 25, 300])\n",
      "Sequence Length: 25\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "GRU, Final Shape: torch.Size([1, 25, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping gru_final_op Shape: torch.Size([25, 800])\n",
      "Linear linear_op Shape: torch.Size([25, 1521])\n",
      "log_softmax op Shape: torch.Size([25, 1521])\n",
      "_______________________________\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "torch.Size([1, 25]) torch.Size([1, 25]) torch.Size([1, 25])\n",
      "_______________________________\n",
      "\t\tEncoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 25]) Prev State Shape torch.Size([4, 1, 400])\n",
      "Embedding, x Shape: torch.Size([1, 25, 300])\n",
      "GRU, op Shape: torch.Size([1, 25, 800]) ht shape torch.Size([4, 1, 400])\n",
      "ht for decoder shape torch.Size([1, 1, 800])\n",
      "_______________________________\n",
      "\t\tDecoder\t\t\n",
      "_______________________________\n",
      "Before starting: x Shape: torch.Size([1, 25])  s0_from_encoder Shape: torch.Size([1, 1, 800])\n",
      "Embedding, x Shape: torch.Size([1, 25, 300])\n",
      "Sequence Length: 25\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "_______________________________\n",
      "\t\tAttention\t\t\n",
      "_______________________________\n",
      "Shape of op_from_encoder: torch.Size([25, 800]) Shape of st_minus_one_from_decoder: torch.Size([25, 800])\n",
      "RNN Cell Op: torch.Size([25, 800])\n",
      "Linear Op: torch.Size([25, 1])\n",
      "Softmax Op: torch.Size([25, 1])\n",
      "Weighted Averaged h vectors: torch.Size([1, 800])\n",
      "concatenated_x shape: torch.Size([1, 1100])\n",
      "---------------------------------\n",
      "GRU_OP: torch.Size([1, 1, 800]) Ht: torch.Size([1, 1, 800])\n",
      "---------------------------------\n",
      "GRU, Final Shape: torch.Size([1, 25, 800]) ht shape torch.Size([1, 1, 800])\n",
      "Reshaping gru_final_op Shape: torch.Size([25, 800])\n",
      "Linear linear_op Shape: torch.Size([25, 1521])\n",
      "log_softmax op Shape: torch.Size([25, 1521])\n",
      "_______________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_index = 6\n",
    "\n",
    "init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "model_encoder.train()\n",
    "model_decoder.train()\n",
    "\n",
    "optimizer_encoder.zero_grad()\n",
    "optimizer_decoder.zero_grad()\n",
    "Xe_b = torch.tensor([X_encoder_indices_tr[data_index]]).to(device)\n",
    "Xd_b = torch.tensor([X_decoder_indices_tr[data_index]]).to(device)\n",
    "Y_b = torch.tensor([Y_indices_tr[data_index]]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "\n",
    "op_from_encoder,ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "op,_,_ = model_decoder(Xd_b,ht_for_decoder,op_from_encoder)\n",
    "ht = ht.detach()\n",
    "loss = loss_fn(op,Y_b.reshape(-1))\n",
    "loss.backward()\n",
    "optimizer_encoder.step()\n",
    "optimizer_decoder.step()\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "optimizer_encoder.zero_grad()\n",
    "optimizer_decoder.zero_grad()\n",
    "Xe_b = torch.tensor([X_encoder_indices_tr[data_index+1]]).to(device)\n",
    "Xd_b = torch.tensor([X_decoder_indices_tr[data_index+1]]).to(device)\n",
    "Y_b = torch.tensor([Y_indices_tr[data_index+1]]).to(device)\n",
    "print(Xe_b.shape,Xd_b.shape,Y_b.shape)\n",
    "op_from_encoder,ht,ht_for_decoder = model_encoder(Xe_b,ht)\n",
    "op,_,_ = model_decoder(Xd_b,ht_for_decoder,op_from_encoder)\n",
    "ht = ht.detach()\n",
    "loss = loss_fn(op,Y_b.reshape(-1))\n",
    "loss.backward()\n",
    "optimizer_encoder.step()\n",
    "optimizer_decoder.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_encoder,model_decoder,\n",
    "            X_encoder_indices_test,X_decoder_indices_test, X_encoder_words_test, X_decoder_words_test, \n",
    "            word_index_mapper, device):\n",
    "    data_index = random.randint(0,100)\n",
    "    Xe_b = torch.tensor([X_encoder_indices_test[data_index]]).to(device)\n",
    "    print(X_encoder_words_test[data_index],Xe_b)\n",
    "    print(X_decoder_words_test[data_index])\n",
    "    \n",
    "    model_encoder.eval()\n",
    "    model_decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        softmax_ops = []\n",
    "        init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "        op_from_encoder,ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "        sos_word = torch.tensor([[word_index_mapper.word_to_index[\"<sos>\"]]]).to(device)\n",
    "        op,ht,softmax_op = model_decoder(sos_word,ht_for_decoder,op_from_encoder)\n",
    "        softmax_ops.append([round(float(el),3) for el in softmax_op.cpu()])\n",
    "        unjumbled_sentence = []\n",
    "        for i in range(25):\n",
    "            predicted_word = torch.argmax(op,axis=1).tolist()\n",
    "#             print(\"Predicted .....................\",predicted_word)\n",
    "            unjumbled_sentence.append(word_index_mapper.index_to_word[predicted_word[0]])\n",
    "            if predicted_word[0] == word_index_mapper.word_to_index[\"<eos>\"]: break\n",
    "            op,ht,softmax_op = model_decoder(torch.tensor([predicted_word]).to(device),ht,op_from_encoder)\n",
    "            softmax_ops.append([round(float(el),3) for el in softmax_op.cpu()])\n",
    "        print(\"_______________________________________\")\n",
    "        print(unjumbled_sentence)\n",
    "        print(\"attention weights\")\n",
    "#         df = pd.DataFrame(softmax_ops)\n",
    "#         if df.shape[0] == len(unjumbled_sentence):\n",
    "#             jumbled_words = X_encoder_words_test[data_index]\n",
    "#             df.columns = jumbled_words\n",
    "#             df.index = unjumbled_sentence\n",
    "#             plt.figure(figsize=(16,4))\n",
    "#             sns.heatmap(df, annot=True)\n",
    "#             plt.show()\n",
    "        print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dating', 'connects', 'people', 'for', 'relationships.', 'Online', 'looking', '<eos>'] tensor([[ 800,  348,  686,  339, 1044, 1435,  438, 1520, 1517, 1517, 1517, 1517,\n",
      "         1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n",
      "         1517]])\n",
      "['<sos>', 'Online', 'dating', 'connects', 'people', 'looking', 'for', 'relationships.']\n",
      "_______________________________________\n",
      "['Individuals', 'can', 'help', 'by', 'reducing', 'their', 'carbon', 'footprints.', '<eos>']\n",
      "attention weights\n",
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "predict(model_encoder,model_decoder,X_encoder_indices_test,X_decoder_indices_test, X_encoder_words_test, X_decoder_words_test, \n",
    "            word_index_mapper, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # torch.device(\"cuda:0\")\n",
    "hidden_size_encoder = 400\n",
    "hidden_size_decoder = hidden_size_encoder\n",
    "model_encoder = UnjumbleEncoderModel(\n",
    "    vocab_size=len(word_index_mapper.word_to_index),embedding_dim=300,num_lstm_layers=2,\n",
    "    hidden_size=hidden_size_encoder,make_bidirectional=True,debug=False\n",
    ").to(device)\n",
    "if model_encoder.bidirectional: hidden_size_decoder = 2*hidden_size_encoder\n",
    "model_attention = UnjumbleBahadnauAttention(hidden_size_decoder,debug=False).to(device)\n",
    "model_decoder = UnjumbleDecoderModel(\n",
    "    model_attention = model_attention,\n",
    "    vocab_size=len(word_index_mapper.word_to_index),embedding_dim=300,num_lstm_layers=1,\n",
    "    hidden_size=hidden_size_decoder,make_bidirectional=False,debug=False\n",
    ").to(device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer_encoder = torch.optim.Adam(model_encoder.parameters(),lr=0.0003)\n",
    "optimizer_decoder = torch.optim.Adam(model_decoder.parameters(),lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 Loss: 7.331637382507324\n",
      "Epoch: 0 Batch: 20 Loss: 3.2774887084960938\n",
      "Epoch: 0 Batch: 40 Loss: 2.9809961318969727\n",
      "Epoch: 0 Batch: 60 Loss: 2.643310308456421\n",
      "Epoch: 0 Batch: 80 Loss: 3.0020956993103027\n",
      "Epoch: 0 Batch: 100 Loss: 2.5291919708251953\n",
      "Epoch: 0 Batch: 120 Loss: 2.2877607345581055\n",
      "Epoch: 0 Batch: 140 Loss: 2.2846059799194336\n",
      "Epoch: 0 Batch: 160 Loss: 2.7650551795959473\n",
      "Epoch: 0 Batch: 180 Loss: 1.7087280750274658\n",
      "Epoch: 0 Batch: 200 Loss: 2.5276002883911133\n",
      "Epoch: 0 Batch: 220 Loss: 2.230926752090454\n",
      "Epoch: 0 Batch: 240 Loss: 2.3627145290374756\n",
      "Epoch: 0 Batch: 260 Loss: 1.9801985025405884\n",
      "Epoch: 0 Batch: 280 Loss: 2.3858814239501953\n",
      "Epoch: 0 Batch: 300 Loss: 2.4429879188537598\n",
      "Epoch: 0 Batch: 320 Loss: 2.580000638961792\n",
      "Epoch: 0 Batch: 340 Loss: 2.4296698570251465\n",
      "Epoch: 0 Batch: 360 Loss: 2.132462501525879\n",
      "Epoch: 0 Batch: 380 Loss: 1.4336600303649902\n",
      "______________________________________\n",
      "Epoch Loss: 886.1695392131805\n",
      "['and', 'diagnosis', 'Telemedicine', 'remote', 'allows', 'treatment.', '<eos>'] tensor([[ 952, 1267, 1282, 1255,   40,  409, 1520, 1517, 1517, 1517, 1517, 1517,\n",
      "         1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n",
      "         1517]])\n",
      "['<sos>', 'Telemedicine', 'allows', 'remote', 'diagnosis', 'and', 'treatment.']\n",
      "_______________________________________\n",
      "['Exercise', 'can', 'can', 'a', 'your', '<eos>']\n",
      "attention weights\n",
      "_______________________________________\n",
      "_______________________________________\n",
      "Epoch: 1 Batch: 0 Loss: 3.2667694091796875\n",
      "Epoch: 1 Batch: 20 Loss: 2.2722904682159424\n",
      "Epoch: 1 Batch: 40 Loss: 2.278247594833374\n",
      "Epoch: 1 Batch: 60 Loss: 2.1909961700439453\n",
      "Epoch: 1 Batch: 80 Loss: 2.2694060802459717\n",
      "Epoch: 1 Batch: 100 Loss: 1.9732253551483154\n",
      "Epoch: 1 Batch: 120 Loss: 1.7226299047470093\n",
      "Epoch: 1 Batch: 140 Loss: 1.8223282098770142\n",
      "Epoch: 1 Batch: 160 Loss: 2.0852162837982178\n",
      "Epoch: 1 Batch: 180 Loss: 1.5122264623641968\n",
      "Epoch: 1 Batch: 200 Loss: 2.24808931350708\n",
      "Epoch: 1 Batch: 220 Loss: 1.7460497617721558\n",
      "Epoch: 1 Batch: 240 Loss: 1.915318250656128\n",
      "Epoch: 1 Batch: 260 Loss: 1.5548919439315796\n",
      "Epoch: 1 Batch: 280 Loss: 1.874466061592102\n",
      "Epoch: 1 Batch: 300 Loss: 2.0009915828704834\n",
      "Epoch: 1 Batch: 320 Loss: 2.0163774490356445\n",
      "Epoch: 1 Batch: 340 Loss: 1.78271484375\n",
      "Epoch: 1 Batch: 360 Loss: 1.7550817728042603\n",
      "Epoch: 1 Batch: 380 Loss: 0.9735603332519531\n",
      "______________________________________\n",
      "Epoch Loss: 688.1805456280708\n",
      "['new', 'can', 'Embracing', 'technology', 'to', 'opportunities.', 'lead', '<eos>'] tensor([[  27,  903,  168,  543, 1333,  956,  461, 1520, 1517, 1517, 1517, 1517,\n",
      "         1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n",
      "         1517]])\n",
      "['<sos>', 'Embracing', 'technology', 'can', 'lead', 'to', 'new', 'opportunities.']\n",
      "_______________________________________\n",
      "['Exercise', 'can', 'be', 'a', 'your', '<eos>']\n",
      "attention weights\n",
      "_______________________________________\n",
      "_______________________________________\n",
      "Epoch: 2 Batch: 0 Loss: 2.5915300846099854\n",
      "Epoch: 2 Batch: 20 Loss: 2.0255508422851562\n",
      "Epoch: 2 Batch: 40 Loss: 1.8911335468292236\n",
      "Epoch: 2 Batch: 60 Loss: 1.909470558166504\n",
      "Epoch: 2 Batch: 80 Loss: 1.917380690574646\n",
      "Epoch: 2 Batch: 100 Loss: 1.7403688430786133\n",
      "Epoch: 2 Batch: 120 Loss: 1.3697720766067505\n",
      "Epoch: 2 Batch: 140 Loss: 1.5013917684555054\n",
      "Epoch: 2 Batch: 160 Loss: 1.7018376588821411\n",
      "Epoch: 2 Batch: 180 Loss: 1.37742280960083\n",
      "Epoch: 2 Batch: 200 Loss: 1.8967831134796143\n",
      "Epoch: 2 Batch: 220 Loss: 1.4303627014160156\n",
      "Epoch: 2 Batch: 240 Loss: 1.6131486892700195\n",
      "Epoch: 2 Batch: 260 Loss: 1.3434267044067383\n",
      "Epoch: 2 Batch: 280 Loss: 1.6316790580749512\n",
      "Epoch: 2 Batch: 300 Loss: 1.762351632118225\n",
      "Epoch: 2 Batch: 320 Loss: 1.6282137632369995\n",
      "Epoch: 2 Batch: 340 Loss: 1.6773953437805176\n",
      "Epoch: 2 Batch: 360 Loss: 1.4410929679870605\n",
      "Epoch: 2 Batch: 380 Loss: 0.8737024664878845\n",
      "______________________________________\n",
      "Epoch Loss: 577.1718940138817\n",
      "['learning', 'rely', 'machine', 'data.', 'Artificial', 'and', 'internet', 'on', 'intelligence', '<eos>'] tensor([[  29,  107,  308, 1448,  721,  952, 1400,  951,  315, 1520, 1517, 1517,\n",
      "         1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n",
      "         1517]])\n",
      "['<sos>', 'Artificial', 'intelligence', 'and', 'machine', 'learning', 'rely', 'on', 'internet', 'data.']\n",
      "_______________________________________\n",
      "['Exercise', 'can', 'be', 'a', 'healthy', 'your', '<eos>']\n",
      "attention weights\n",
      "_______________________________________\n",
      "_______________________________________\n",
      "Epoch: 3 Batch: 0 Loss: 2.0249812602996826\n",
      "Epoch: 3 Batch: 20 Loss: 1.7608428001403809\n",
      "Epoch: 3 Batch: 40 Loss: 1.783423662185669\n",
      "Epoch: 3 Batch: 60 Loss: 1.7425107955932617\n",
      "Epoch: 3 Batch: 80 Loss: 1.639432668685913\n",
      "Epoch: 3 Batch: 100 Loss: 1.5298439264297485\n",
      "Epoch: 3 Batch: 120 Loss: 1.1388541460037231\n",
      "Epoch: 3 Batch: 140 Loss: 1.2869642972946167\n",
      "Epoch: 3 Batch: 160 Loss: 1.2817909717559814\n",
      "Epoch: 3 Batch: 180 Loss: 1.1965259313583374\n",
      "Epoch: 3 Batch: 200 Loss: 1.5019336938858032\n",
      "Epoch: 3 Batch: 220 Loss: 1.1640424728393555\n",
      "Epoch: 3 Batch: 240 Loss: 1.3001599311828613\n",
      "Epoch: 3 Batch: 260 Loss: 1.043420672416687\n",
      "Epoch: 3 Batch: 280 Loss: 1.2890974283218384\n",
      "Epoch: 3 Batch: 300 Loss: 1.5258574485778809\n",
      "Epoch: 3 Batch: 320 Loss: 1.470610499382019\n",
      "Epoch: 3 Batch: 340 Loss: 1.3682658672332764\n",
      "Epoch: 3 Batch: 360 Loss: 0.9706972241401672\n",
      "Epoch: 3 Batch: 380 Loss: 0.896598219871521\n",
      "______________________________________\n",
      "Epoch Loss: 480.2448835372925\n",
      "['a', 'The', 'plays', 'response', 'vital', 'recovery.', 'and', 'disaster', 'in', 'internet', 'role', '<eos>'] tensor([[ 668,  747,  947,   18,  895,  141,  952, 1386,   51, 1400, 1018, 1520,\n",
      "         1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n",
      "         1517]])\n",
      "['<sos>', 'The', 'internet', 'plays', 'a', 'vital', 'role', 'in', 'disaster', 'response', 'and', 'recovery.']\n",
      "_______________________________________\n",
      "['Exercise', 'can', 'be', 'a', 'great', 'your', '<eos>']\n",
      "attention weights\n",
      "_______________________________________\n",
      "_______________________________________\n",
      "Epoch: 4 Batch: 0 Loss: 1.6601133346557617\n",
      "Epoch: 4 Batch: 20 Loss: 1.4654217958450317\n",
      "Epoch: 4 Batch: 40 Loss: 1.4954757690429688\n",
      "Epoch: 4 Batch: 60 Loss: 1.5020997524261475\n",
      "Epoch: 4 Batch: 80 Loss: 1.3251850605010986\n",
      "Epoch: 4 Batch: 100 Loss: 1.3271875381469727\n",
      "Epoch: 4 Batch: 120 Loss: 0.9458064436912537\n",
      "Epoch: 4 Batch: 140 Loss: 1.1143131256103516\n",
      "Epoch: 4 Batch: 160 Loss: 1.0189005136489868\n",
      "Epoch: 4 Batch: 180 Loss: 1.038684606552124\n",
      "Epoch: 4 Batch: 200 Loss: 1.049808382987976\n",
      "Epoch: 4 Batch: 220 Loss: 1.1899648904800415\n",
      "Epoch: 4 Batch: 240 Loss: 1.0427829027175903\n",
      "Epoch: 4 Batch: 260 Loss: 0.8179027438163757\n",
      "Epoch: 4 Batch: 280 Loss: 0.9089245796203613\n",
      "Epoch: 4 Batch: 300 Loss: 1.265986442565918\n",
      "Epoch: 4 Batch: 320 Loss: 1.1867070198059082\n",
      "Epoch: 4 Batch: 340 Loss: 1.1126160621643066\n",
      "Epoch: 4 Batch: 360 Loss: 0.7752581238746643\n",
      "Epoch: 4 Batch: 380 Loss: 0.7629274129867554\n",
      "______________________________________\n",
      "Epoch Loss: 406.6635718047619\n",
      "['shape', 'Future', 'will', 'to', 'the', 'continue', \"internet's\", 'advancements', 'evolution.', '<eos>'] tensor([[ 688, 1090, 1292, 1333,  900, 1495,  789,   30, 1469, 1520, 1517, 1517,\n",
      "         1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n",
      "         1517]])\n",
      "['<sos>', 'Future', 'advancements', 'will', 'continue', 'to', 'shape', 'the', \"internet's\", 'evolution.']\n",
      "_______________________________________\n",
      "['Exercise', 'can', 'be', 'a', 'great', 'way', 'of', 'your', 'daily', 'routine.', '<eos>']\n",
      "attention weights\n",
      "_______________________________________\n",
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "# init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "for i in range(epochs):\n",
    "    model_encoder.train()\n",
    "    model_decoder.train()\n",
    "    epoch_loss = 0\n",
    "    for j in range(0,len(X_encoder_indices_tr)):\n",
    "        optimizer_encoder.zero_grad()\n",
    "        optimizer_decoder.zero_grad()\n",
    "        Xe_b = torch.tensor([X_encoder_indices_tr[j]]).to(device)\n",
    "        Xd_b = torch.tensor([X_decoder_indices_tr[j]]).to(device)\n",
    "        Y_b = torch.tensor([Y_indices_tr[j]]).to(device)\n",
    "        init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "        op_from_encoder,ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "        op,_,_ = model_decoder(Xd_b,ht_for_decoder,op_from_encoder)\n",
    "        ht = ht.detach()\n",
    "#         init_ht_for_encoder = ht\n",
    "        loss = loss_fn(op,Y_b.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer_encoder.step()\n",
    "        optimizer_decoder.step()\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        if j%20 == 0: print(\"Epoch:\",i,\"Batch:\",j,\"Loss:\",batch_loss)\n",
    "#     init_ht_for_encoder = ht\n",
    "    print(\"______________________________________\")\n",
    "    print(\"Epoch Loss:\",epoch_loss)\n",
    "    predict(model_encoder,model_decoder,X_encoder_indices_test,X_decoder_indices_test, X_encoder_words_test, X_decoder_words_test, \n",
    "            word_index_mapper, device)\n",
    "    print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_whole_val(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi):\n",
    "    model_encoder.eval()\n",
    "    model_decoder.eval()\n",
    "    accuracy_tuple_list = []  #[(jumbed_sent,unjumbled_sent,predicted_sent,hard,soft,word_count),...,]\n",
    "    with torch.no_grad():\n",
    "        for data_index in range(len(Xval_e)):\n",
    "            if data_index % 50 == 0: print(data_index,end = ' ')\n",
    "            Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xval_e[data_index])]).to(device)\n",
    "\n",
    "            init_ht_for_encoder = model_encoder.init_hidden().to(device)\n",
    "            op_from_encoder,ht,ht_for_decoder = model_encoder(Xe_b,init_ht_for_encoder)\n",
    "            sos_word = torch.tensor([[decoder_wtoi[\"<sos>\"]]]).to(device)\n",
    "            op,ht,softmax_op = model_decoder(sos_word,ht_for_decoder,op_from_encoder)\n",
    "            unjumbled_sentence = []\n",
    "            for i in range(25):\n",
    "                predicted_word = torch.argmax(op,axis=1).tolist()\n",
    "                unjumbled_sentence.append(decoder_itow[predicted_word[0]])\n",
    "                if predicted_word[0] == decoder_wtoi[\"<eos>\"]: break\n",
    "                op,ht,softmax_op = model_decoder(torch.tensor([predicted_word]).to(device),ht,op_from_encoder)\n",
    "                \n",
    "            hard_accuracy = 1 if \" \".join(unjumbled_sentence) == Yval[data_index] else 0\n",
    "            word_count = len(set(Yval[data_index].split()))\n",
    "            soft_accuracy = len(set(unjumbled_sentence).intersection(set(Yval[data_index].split())))/word_count\n",
    "            accuracy_tuple_list.append(\n",
    "                (Xval_e[data_index],Yval[data_index],\" \".join(unjumbled_sentence),hard_accuracy,soft_accuracy,word_count)\n",
    "            )\n",
    "    return accuracy_tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tuple_list = predict_on_whole_val(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "df = pd.DataFrame(accuracy_tuple_list)\n",
    "df.columns = [\"jumbled_sent\",\"unjumbled_sent\",\"prediction\",\"hard_accuracy\",\"soft_accuracy\",\"word_count\"]\n",
    "print(df.shape,df['hard_accuracy'].sum())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum((df['soft_accuracy']*df['word_count']).tolist())\n",
    "b = df['word_count'].sum()\n",
    "a,b,a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['soft_accuracy']==1].shape, df[df['soft_accuracy']==1].shape[0]/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(model_encoder_new,model_decoder_new,encoder_encode_decode,decoder_itow,decoder_wtoi):\n",
    "    model_encoder_new.eval()\n",
    "    model_decoder_new.eval()\n",
    "    accuracy_tuple_list = []  #[(jumbed_sent,unjumbled_sent,predicted_sent,hard,soft,word_count),...,]\n",
    "    with torch.no_grad():\n",
    "        for data_index in range(len(Xtest_e)):\n",
    "            if data_index % 50 == 0: print(data_index,end = ' ')\n",
    "            Xe_b = torch.tensor([encoder_encode_decode.get_encoding(Xtest_e[data_index])]).to(device)\n",
    "            init_ht_for_encoder = model_encoder_new.init_hidden().to(device)\n",
    "            op_from_encoder,ht,ht_for_decoder = model_encoder_new(Xe_b,init_ht_for_encoder)\n",
    "            sos_word = torch.tensor([[decoder_wtoi[\"<sos>\"]]]).to(device)\n",
    "            op,ht,softmax_op = model_decoder_new(sos_word,ht_for_decoder,op_from_encoder)\n",
    "            unjumbled_sentence = []\n",
    "            for i in range(25):\n",
    "                predicted_word = torch.argmax(op,axis=1).tolist()\n",
    "                unjumbled_sentence.append(decoder_itow[predicted_word[0]])\n",
    "                if predicted_word[0] == decoder_wtoi[\"<eos>\"]: break\n",
    "                op,ht,softmax_op = model_decoder_new(torch.tensor([predicted_word]).to(device),ht,op_from_encoder)\n",
    "            hard_accuracy = 1 if \" \".join(unjumbled_sentence) == Ytest[data_index] else 0\n",
    "            word_count = len(set(Ytest[data_index].split()))\n",
    "            soft_accuracy = len(set(unjumbled_sentence).intersection(set(Ytest[data_index].split())))/word_count\n",
    "            accuracy_tuple_list.append(\n",
    "                (Xtest_e[data_index],Ytest[data_index],\" \".join(unjumbled_sentence),hard_accuracy,soft_accuracy,word_count)\n",
    "            )\n",
    "    return accuracy_tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_e = [\n",
    "    \"is eating . Nitish apple <eos>\",\n",
    "    \"is city my favorite New York. <eos>\",\n",
    "    \"a a and dog are man woods walking through the . <eos>\"\n",
    "]\n",
    "Ytest = [\n",
    "    \"Nitish is eating apple . <eos>\",\n",
    "    \"New York is my faorite city. <eos>\",\n",
    "    \"a man and a dog are walking through the woods . <eos>\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tuple_list_test = predict_on_test(model_encoder,model_decoder,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "df_test = pd.DataFrame(accuracy_tuple_list_test)\n",
    "df_test.columns = [\"jumbled_sent\",\"unjumbled_sent\",\"prediction\",\"hard_accuracy\",\"soft_accuracy\",\"word_count\"]\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Model was not required to be saved explicitly as it is a part of the decoder model only.\n",
    "#### Additionally the word_to_index and index_to_word dictionary and get_encoding funtion will be required for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_encoder.state_dict(), \"SavedModels/Jumble/encoder_model.pt\")\n",
    "torch.save(model_attention.state_dict(), \"SavedModels/Jumble/attention_model.pt\")\n",
    "torch.save(model_decoder.state_dict(), \"SavedModels/Jumble/decoder_model.pt\")\n",
    "\n",
    "# torch.save(model_encoder,\"SavedModels/Jumble/encoder_model.pt\")\n",
    "# torch.save(model_attention,\"SavedModels/Jumble/attention_model.pt\")\n",
    "# torch.save(model_decoder,\"SavedModels/Jumble/decoder_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "hidden_size_encoder = 400\n",
    "encoder_encode_decode = EncodeDecode(encoder_wtoi,encoder_itow,pad_token,unknown_token)\n",
    "decoder_encode_decode = EncodeDecode(decoder_wtoi,decoder_itow,pad_token,unknown_token)\n",
    "loaded_encoder_model = UnjumbleEncoderModel(\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=300,num_lstm_layers=2,\n",
    "    hidden_size=hidden_size_encoder,make_bidirectional=True,debug=False\n",
    ").to(device)\n",
    "loaded_encoder_model.load_state_dict(torch.load(\"SavedModels/Jumble/encoder_model.pt\"))\n",
    "loaded_encoder_model.eval()\n",
    "\n",
    "if loaded_encoder_model.bidirectional: hidden_size_decoder = 2*hidden_size_encoder\n",
    "loaded_attention_model = UnjumbleBahadnauAttention(hidden_size_decoder,debug=False).to(device)\n",
    "loaded_attention_model.load_state_dict(torch.load(\"SavedModels/Jumble/attention_model.pt\"))\n",
    "loaded_attention_model.eval()\n",
    "\n",
    "loaded_decoder_model = UnjumbleDecoderModel(model_attention = loaded_attention_model,\n",
    "    vocab_size=len(encoder_wtoi),embedding_dim=300,num_lstm_layers=1,\n",
    "    hidden_size=hidden_size_decoder,make_bidirectional=False,debug=False\n",
    ").to(device)\n",
    "loaded_decoder_model.load_state_dict(torch.load(\"SavedModels/Jumble/decoder_model.pt\"))\n",
    "loaded_decoder_model.eval()\n",
    "\n",
    "\n",
    "# loaded_encoder_model = torch.load(\"SavedModels/Jumble/encoder_model.pt\")\n",
    "# loaded_decoder_model = torch.load(\"SavedModels/Jumble/decoder_model.pt\")\n",
    "# model_attention = torch.load(\"SavedModels/Jumble/attention_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tuple_list_test = predict_on_test(loaded_encoder_model,loaded_decoder_model,encoder_encode_decode,decoder_itow,decoder_wtoi)\n",
    "df_test = pd.DataFrame(accuracy_tuple_list_test)\n",
    "df_test.columns = [\"jumbled_sent\",\"unjumbled_sent\",\"prediction\",\"hard_accuracy\",\"soft_accuracy\",\"word_count\"]\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
