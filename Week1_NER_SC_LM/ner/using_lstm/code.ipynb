{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self,filename):\n",
    "        self.filename = filename\n",
    "        self.X, self.Y = self.get_data_from_file()\n",
    "            \n",
    "    def check_sanity(self,X,Y):\n",
    "        if len(X) != len(Y): return \"No. of sentences and targets must be same.\"\n",
    "        flag = False\n",
    "        for index,(el1,el2) in enumerate(zip(X,Y)):\n",
    "            if len(el1) != len(el2):\n",
    "                flag = True\n",
    "                error_index = index\n",
    "                break\n",
    "        if flag:\n",
    "            return \"Length of sentence \" + str(error_index) + \" does not match with target size.\"\n",
    "        else: return \"all fine\"\n",
    "    \n",
    "    def get_data_from_file(self):\n",
    "        with open(self.filename) as f:\n",
    "            X, Y = [],[]\n",
    "            sentence, target = [],[]\n",
    "            for index,line in enumerate(f):\n",
    "                if index < 2: continue\n",
    "                if line.strip() == \". . O O\":\n",
    "                    X.append(sentence)\n",
    "                    Y.append(target)\n",
    "                    sentence, target = [], []\n",
    "                elif line.strip().split(\" \")[0] != \"\":\n",
    "                    sentence.append(line.strip().split(\" \")[0])\n",
    "                    target.append(line.strip().split(\" \")[-1])\n",
    "        msg = self.check_sanity(X,Y) \n",
    "        if msg == \"all fine\": return X,Y\n",
    "        else: raise ValueError(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabBuilder:\n",
    "    def __init__(self,X,padding_token, unknown_token):\n",
    "        self.X = X\n",
    "        self.padding_token = padding_token\n",
    "        self.unknown_token = unknown_token\n",
    "        self.word_to_index, self.index_to_word = self.generate_vocab()\n",
    "    \n",
    "    def generate_vocab(self):\n",
    "        vocab_set = set()\n",
    "        for sentence in self.X:\n",
    "            for word in sentence: vocab_set.add(word)\n",
    "        vocab_set.add(self.padding_token)\n",
    "        vocab_set.add(self.unknown_token)\n",
    "        vocab_set = list(vocab_set)\n",
    "        word_to_index = {k:v for v,k in enumerate(vocab_set)}\n",
    "        index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "        return word_to_index,index_to_word\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelBuilder:\n",
    "    def __init__(self,Y,padding_token):\n",
    "        self.Y = Y\n",
    "        self.padding_token = padding_token\n",
    "        self.token_to_int, self.int_to_token = self.generate_labels()\n",
    "        \n",
    "    def generate_labels(self):\n",
    "        token_set = set()\n",
    "        for target in self.Y:\n",
    "            for token in target: token_set.add(token)\n",
    "        token_set.add(self.padding_token)\n",
    "        token_set = list(token_set)\n",
    "        token_to_int = {k:v for v,k in enumerate(token_set)}\n",
    "        int_to_token = {v:k for k,v in token_to_int.items()}\n",
    "        return token_to_int, int_to_token\n",
    "    \n",
    "    def get_label_count(self):\n",
    "        return len(self.token_to_int)\n",
    "    \n",
    "    def get_token_distribution(self):\n",
    "        target_tokens_count_dict = {}\n",
    "        for target in self.Y:\n",
    "            for token in target:\n",
    "                if token in target_tokens_count_dict: target_tokens_count_dict[token] += 1\n",
    "                else: target_tokens_count_dict[token]  = 1\n",
    "        target_tokens_count_dict[self.padding_token] = -1\n",
    "        return target_tokens_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeXY:\n",
    "    def __init__(self, word_to_index, token_to_int,unknown_token):\n",
    "        self.word_to_index = word_to_index\n",
    "        self.token_to_int = token_to_int\n",
    "        self.unknown_token = unknown_token\n",
    "    \n",
    "    def encode_X(self,data):\n",
    "        X_encoded = []\n",
    "        for sentence in data:\n",
    "            encoded_sentence = []\n",
    "            for word in sentence:\n",
    "                if word in self.word_to_index: encoded_sentence.append(self.word_to_index[word])\n",
    "                else: encoded_sentence.append(self.word_to_index[self.unknown_token])\n",
    "            X_encoded.append(encoded_sentence)\n",
    "        return X_encoded\n",
    "    \n",
    "    def encode_Y(self,data):\n",
    "        Y_encoded = []\n",
    "        for target in data:\n",
    "            encoded_target = []\n",
    "            for token in target: encoded_target.append(self.token_to_int[token])\n",
    "            Y_encoded.append(encoded_target)\n",
    "        return Y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeXY:\n",
    "    def __init__(self, index_to_word, int_to_token):\n",
    "        self.index_to_word = index_to_word\n",
    "        self.int_to_token = int_to_token\n",
    "    \n",
    "    def decode_test_input(self,test_op):\n",
    "        X_decoded = []\n",
    "        for encoded_list in test_op:\n",
    "            decoded_list = []\n",
    "            for word_index in encoded_list: decoded_list.append(self.index_to_word[int(word_index)])\n",
    "            X_decoded.append(decoded_list)\n",
    "        return X_decoded\n",
    "    \n",
    "    def decode_test_op(self, predicted_op):\n",
    "        Y_decoded = []\n",
    "        for encoded_list in predicted_op:\n",
    "            decoded_list = []\n",
    "            for label in encoded_list: decoded_list.append(self.int_to_token[int(label)])\n",
    "            Y_decoded.append(decoded_list)\n",
    "        return Y_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, X,Y,batch_size,padding_token,enocoder_XY):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.batch_size = batch_size\n",
    "        self.padding_token = padding_token\n",
    "        self.enocoder_XY = enocoder_XY\n",
    "    \n",
    "    def get_num_batches(self):\n",
    "        return len(self.X)//self.batch_size\n",
    "    \n",
    "    def pad_batch(self,batch_data, max_length):\n",
    "        padded_batch_data= []\n",
    "        for el in batch_data:\n",
    "            if len(el) > max_length: padded_el = el[:max_length]\n",
    "            else: padded_el = el + [self.padding_token]*(max_length - len(el))\n",
    "            padded_batch_data.append(padded_el)\n",
    "        return padded_batch_data\n",
    "    \n",
    "    def get_batch(self,batch_index, return_encoded = True, sentence_length=-1):\n",
    "        Xb = self.X[batch_index*self.batch_size:(batch_index+1)*self.batch_size]\n",
    "        Yb = self.Y[batch_index*self.batch_size:(batch_index+1)*self.batch_size]\n",
    "        \n",
    "        max_length = sentence_length\n",
    "        if sentence_length == -1:\n",
    "            for sentence in Xb:\n",
    "                if len(sentence) > max_length: max_length = len(sentence)\n",
    "                    \n",
    "        Xb = self.pad_batch(Xb,max_length)\n",
    "        Yb = self.pad_batch(Yb,max_length)\n",
    "        \n",
    "        Xb_encoded = self.enocoder_XY.encode_X(Xb)\n",
    "        Yb_encoded = self.enocoder_XY.encode_Y(Yb)\n",
    "        if not return_encoded: return Xb, Yb\n",
    "        else: return torch.tensor(Xb_encoded), torch.tensor(Yb_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5620 5620 1621 1621\n",
      "20333 20333 20333\n",
      "Pad index 4\n",
      "10 {'B-PER': 0, 'B-MISC': 1, 'B-LOC': 2, 'I-MISC': 3, 'PAD': 4, 'I-LOC': 5, 'I-PER': 6, 'B-ORG': 7, 'I-ORG': 8, 'O': 9} 10 {'B-ORG': 5231, 'O': 126007, 'B-MISC': 2711, 'B-PER': 5509, 'I-PER': 3842, 'B-LOC': 5638, 'I-ORG': 2853, 'I-MISC': 958, 'I-LOC': 897, 'PAD': -1}\n",
      "562 162\n"
     ]
    }
   ],
   "source": [
    "## Read Files\n",
    "train_file = \"train.txt\"\n",
    "test_file = \"test.txt\"\n",
    "train_data_loader = DataLoader(train_file)\n",
    "test_data_loader = DataLoader(test_file)\n",
    "X_tr, Y_tr = train_data_loader.X, train_data_loader.Y\n",
    "X_test, Y_test = test_data_loader.X, test_data_loader.Y\n",
    "print(len(X_tr),len(Y_tr),len(X_test),len(Y_test))\n",
    "\n",
    "## Create Vocabulary and ClassLabel from Training Data Only\n",
    "## pad tokens will be added in the vocabulary and class labels\n",
    "## unknown token will be added in vocabulary only, for handling the unseen words in TestSet\n",
    "pad_token = \"PAD\"\n",
    "unknown_token = \"UNK\"\n",
    "vocab_builder = VocabBuilder(X_tr, padding_token=pad_token, unknown_token=unknown_token)\n",
    "word_to_index_tr, index_to_word_tr = vocab_builder.word_to_index, vocab_builder.index_to_word\n",
    "### vocab_size required later\n",
    "vocab_size = vocab_builder.get_vocab_size()\n",
    "print(vocab_size,len(word_to_index_tr),len(index_to_word_tr))\n",
    "\n",
    "label_builder = LabelBuilder(Y_tr, padding_token=pad_token)\n",
    "token_to_int_tr, int_to_token_tr = label_builder.token_to_int, label_builder.int_to_token\n",
    "### pad_index, num_classes required later\n",
    "pad_index = token_to_int_tr['PAD']\n",
    "print(\"Pad index\",pad_index)\n",
    "num_classes = label_builder.get_label_count()\n",
    "print(num_classes,token_to_int_tr, len(int_to_token_tr),label_builder.get_token_distribution())\n",
    "\n",
    "## Use only Training Vocabulary to convert text to integer indices, and tokens to integer class labels\n",
    "enocoder_XY = EncodeXY(word_to_index_tr, token_to_int_tr, unknown_token=unknown_token)\n",
    "decoder_XY = DecodeXY(index_to_word_tr, int_to_token_tr)\n",
    "\n",
    "## Generate Training and Test Batches\n",
    "## It is better to encode Batches, rather than the whole dataset at once.\n",
    "## So the encoder_XY is passed as an argument to the BatchGenerator.\n",
    "batch_size = 10\n",
    "max_sentence_length = 40\n",
    "batch_generator_tr = BatchGenerator(X_tr,Y_tr, batch_size, pad_token, enocoder_XY)\n",
    "batch_generator_test = BatchGenerator(X_test,Y_test, batch_size, pad_token, enocoder_XY)\n",
    "### num_batches_tr, num_batches_test  required later\n",
    "num_batches_tr = batch_generator_tr.get_num_batches()\n",
    "num_batches_test = batch_generator_test.get_num_batches()\n",
    "print(num_batches_tr, num_batches_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNERModel(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, lstm_neurons, num_lstm_layers, num_classes,\n",
    "                 make_birectional=False, debug_mode=False):\n",
    "        super().__init__()\n",
    "        self.debug_mode = debug_mode\n",
    "        self.bidirectional = make_birectional\n",
    "        self.lstm_neurons = lstm_neurons\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_neurons, \n",
    "                            num_layers=num_lstm_layers, bidirectional=make_birectional, batch_first=True)\n",
    "        if self.bidirectional:\n",
    "            self.linear1 = nn.Linear(in_features=2*lstm_neurons, out_features=100)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(in_features=lstm_neurons, out_features=100)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.linear2 = nn.Linear(in_features=100, out_features=num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self,x,ht,ct):\n",
    "        if self.debug_mode: print(\"Before embedding layer:\",x.shape)\n",
    "            \n",
    "        x = self.embedding(x)\n",
    "        if self.debug_mode: print(\"After embedding layer:\",x.shape)\n",
    "            \n",
    "        x, (ht, ct) = self.lstm(x,(ht,ct))\n",
    "        if self.debug_mode: print(\"After lstm layer:\",x.shape,ht.shape,ct.shape)\n",
    "        \n",
    "        x = x.reshape(-1, x.shape[2])\n",
    "        if self.debug_mode: print(\"After reshaping:\",x.shape)\n",
    "            \n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        if self.debug_mode: print(\"After 1st linear layer:\",x.shape)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.log_softmax(x)\n",
    "        if self.debug_mode: print(\"After 2nd linear layer:\",x.shape)\n",
    "        \n",
    "        return x, ht,ct\n",
    "    \n",
    "    def init_state_of_lstm(self,batch_size):\n",
    "        if self.bidirectional: first_param = 2*self.num_lstm_layers\n",
    "        else: first_param = self.num_lstm_layers\n",
    "        return (\n",
    "            torch.randn(first_param, batch_size, self.lstm_neurons),\n",
    "            torch.randn(first_param, batch_size, self.lstm_neurons),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,sentence_list,max_length):\n",
    "    model.eval()\n",
    "    init_states = model.init_state_of_lstm(len(sentence_list))\n",
    "    padded_data = batch_generator_tr.pad_batch(sentence_list,max_length=max_length)\n",
    "    encoded_sentences = enocoder_XY.encode_X(padded_data)\n",
    "    with torch.no_grad():\n",
    "        op, _, _ = model(torch.tensor(encoded_sentences), init_states[0],init_states[1])\n",
    "        op = op.reshape(len(sentence_list),max_length,-1)\n",
    "        predictions = torch.argmax(op, axis=2)\n",
    "        predicted_entities = decoder_XY.decode_test_op(predictions)\n",
    "        return predicted_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(model):\n",
    "    model.eval()\n",
    "    init_states = model.init_state_of_lstm(batch_size)\n",
    "    Y_actual = []\n",
    "    Y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches_test):\n",
    "            Xb, Yb = batch_generator_test.get_batch(i)\n",
    "            Yb = Yb.reshape(-1)\n",
    "            op, _, _ = model(Xb,init_states[0],init_states[1])\n",
    "            Y_pred += [int(el) for el in torch.argmax(op,axis=1)]\n",
    "            Y_actual += [int(el) for el in Yb]\n",
    "            \n",
    "    Y_actual_without_pad, Y_pred_without_pad = [],[]\n",
    "    for el1, el2 in zip(Y_actual, Y_pred):\n",
    "        if el1 == pad_index: continue\n",
    "        else: \n",
    "            Y_actual_without_pad.append(el1)\n",
    "            Y_pred_without_pad.append(el2)\n",
    "    return f1_score(Y_actual_without_pad, Y_pred_without_pad, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debugging Network and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "embedding_dim = 60\n",
    "lstm_neurons = 100\n",
    "num_lstm_layers = 2\n",
    "make_bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before embedding layer: torch.Size([10, 45])\n",
      "After embedding layer: torch.Size([10, 45, 60])\n",
      "After lstm layer: torch.Size([10, 45, 200]) torch.Size([4, 10, 100]) torch.Size([4, 10, 100])\n",
      "After reshaping: torch.Size([450, 200])\n",
      "After 1st linear layer: torch.Size([450, 100])\n",
      "After 2nd linear layer: torch.Size([450, 10])\n",
      "torch.Size([450, 10])\n",
      "tensor([-2.3367, -2.3451, -2.4246, -2.2385, -2.2527, -2.1447, -2.2918, -2.4276,\n",
      "        -2.2657, -2.3324], grad_fn=<SelectBackward>)\n",
      "torch.Size([450, 10]) torch.Size([450])\n",
      "tensor(2.3479, grad_fn=<NllLossBackward>)\n",
      "Before embedding layer: torch.Size([10, 45])\n",
      "After embedding layer: torch.Size([10, 45, 60])\n",
      "After lstm layer: torch.Size([10, 45, 200]) torch.Size([4, 10, 100]) torch.Size([4, 10, 100])\n",
      "After reshaping: torch.Size([450, 200])\n",
      "After 1st linear layer: torch.Size([450, 100])\n",
      "After 2nd linear layer: torch.Size([450, 10])\n",
      "torch.Size([450, 10])\n",
      "tensor([-4.6573e+01, -4.7208e+01, -4.8693e+01, -4.6250e+01, -1.3938e-03,\n",
      "        -4.3250e+01, -4.5483e+01, -4.9426e+01, -4.4327e+01, -6.5765e+00],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([450, 10]) torch.Size([450])\n",
      "tensor(7.4895, grad_fn=<NllLossBackward>)\n",
      "0.17777777777777778\n"
     ]
    }
   ],
   "source": [
    "model = MyNERModel(vocab_size=vocab_size, embedding_dim=embedding_dim, lstm_neurons=lstm_neurons, \n",
    "                   num_lstm_layers=num_lstm_layers, num_classes = num_classes,\n",
    "                   make_birectional=make_bidirectional, debug_mode=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.1)\n",
    "loss_function = nn.NLLLoss()\n",
    "(ht,ct) = model.init_state_of_lstm(batch_size)\n",
    "Y_actual, Y_pred = [], []\n",
    "\n",
    "optimizer.zero_grad()\n",
    "Xb, Yb = batch_generator_tr.get_batch(2,sentence_length=-1)\n",
    "op, ht,ct = model(Xb,ht,ct)\n",
    "print(op.shape)\n",
    "print(op[0])\n",
    "Yb = Yb.reshape(-1)\n",
    "print(op.shape, Yb.shape)\n",
    "loss = loss_function(op, Yb)\n",
    "print(loss)\n",
    "ht = ht.detach()\n",
    "ct = ct.detach()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "Y_pred += [int(el) for el in torch.argmax(op,axis=1)]\n",
    "Y_actual += [int(el) for el in Yb]\n",
    "\n",
    "optimizer.zero_grad()\n",
    "Xb, Yb = batch_generator_tr.get_batch(2,sentence_length=-1)\n",
    "op, ht,ct = model(Xb,ht,ct)\n",
    "print(op.shape)\n",
    "print(op[0])\n",
    "Yb = Yb.reshape(-1)\n",
    "print(op.shape, Yb.shape)\n",
    "loss = loss_function(op, Yb)\n",
    "print(loss)\n",
    "ht = ht.detach()\n",
    "ct = ct.detach()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "Y_pred += [int(el) for el in torch.argmax(op,axis=1)]\n",
    "Y_actual += [int(el) for el in Yb]\n",
    "\n",
    "print(f1_score(Y_actual,Y_pred,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_for_predictions = [\n",
    "    \"Paris is a beautiful city in France\".split(\" \"),\n",
    "    \"Jim Courier is eating apples at New York\".split(\" \"),\n",
    "    \"EU rejects German call to boycott British lamb\".split(\" \")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Paris', 'is', 'a', 'beautiful', 'city', 'in', 'France'], ['Jim', 'Courier', 'is', 'eating', 'apples', 'at', 'New', 'York'], ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']]\n",
      "Before embedding layer: torch.Size([3, 8])\n",
      "After embedding layer: torch.Size([3, 8, 60])\n",
      "After lstm layer: torch.Size([3, 8, 200]) torch.Size([4, 3, 100]) torch.Size([4, 3, 100])\n",
      "After reshaping: torch.Size([24, 200])\n",
      "After 1st linear layer: torch.Size([24, 100])\n",
      "After 2nd linear layer: torch.Size([24, 10])\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_for_predictions)\n",
    "print(predict(model,sentences_for_predictions,max_length=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 40]) torch.Size([4, 10, 100]) torch.Size([4, 10, 100])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1               [-1, 40, 60]       1,219,980\n",
      "              LSTM-2  [[-1, 40, 200], [-1, 10, 100], [-1, 10, 100]]               0\n",
      "            Linear-3                  [-1, 100]          20,100\n",
      "         LeakyReLU-4                  [-1, 100]               0\n",
      "            Linear-5                   [-1, 10]           1,010\n",
      "        LogSoftmax-6                   [-1, 10]               0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\othersoftwares\\python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-996b8f9e862d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                    make_birectional=make_bidirectional, debug_mode=False)\n\u001b[0;32m      5\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mct\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_state_of_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_network\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\othersoftwares\\python38\\lib\\site-packages\\torchsummary\\torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device, seq_network)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;31m# assume 4 bytes/number (float on cuda).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mtotal_input_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[0mtotal_output_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtotal_output\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# x2 for gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mtotal_params_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32md:\\othersoftwares\\python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2997\u001b[0m     \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2998\u001b[0m     \"\"\"\n\u001b[1;32m-> 2999\u001b[1;33m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[0;32m   3000\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0;32m   3001\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\othersoftwares\\python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'tuple'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = MyNERModel(vocab_size=vocab_size, embedding_dim=embedding_dim, lstm_neurons=lstm_neurons, \n",
    "                   num_lstm_layers=num_lstm_layers, num_classes = num_classes,\n",
    "                   make_birectional=make_bidirectional, debug_mode=False)\n",
    "(ht,ct) = model.init_state_of_lstm(batch_size)\n",
    "summary(model,input_size=[(10,40),(4,10,100),(4,10,100)], device=\"cpu\", seq_network=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "embedding_dim = 60\n",
    "lstm_neurons = 100\n",
    "num_lstm_layers = 3\n",
    "make_bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNERModel(vocab_size=vocab_size, embedding_dim=embedding_dim, lstm_neurons=lstm_neurons, \n",
    "                   num_lstm_layers=num_lstm_layers, num_classes = num_classes,\n",
    "                   make_birectional=make_bidirectional, debug_mode=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400 420 440 460 480 500 520 540 560 \n",
      "Epoch: 1, Loss: 156.33182615041733, F1-Score: 0.8314892675370658\n",
      "[['Paris', 'is', 'a', 'beautiful', 'city', 'in', 'France'], ['Jim', 'Courier', 'is', 'eating', 'apples', 'at', 'New', 'York'], ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']]\n",
      "[['B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'PAD'], ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-MISC', 'PAD'], ['O', 'O', 'I-PER', 'O', 'O', 'O', 'B-MISC', 'O']]\n",
      "0.8404364056528307\n",
      "0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400 420 440 460 480 500 520 540 560 \n",
      "Epoch: 2, Loss: 74.60997171700001, F1-Score: 0.897172721710946\n",
      "[['Paris', 'is', 'a', 'beautiful', 'city', 'in', 'France'], ['Jim', 'Courier', 'is', 'eating', 'apples', 'at', 'New', 'York'], ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']]\n",
      "[['B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'PAD'], ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC'], ['I-ORG', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'B-MISC', 'O']]\n",
      "0.8623011287885523\n",
      "0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400 420 440 460 480 500 520 540 560 \n",
      "Epoch: 3, Loss: 53.500302996020764, F1-Score: 0.9299884149278211\n",
      "[['Paris', 'is', 'a', 'beautiful', 'city', 'in', 'France'], ['Jim', 'Courier', 'is', 'eating', 'apples', 'at', 'New', 'York'], ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']]\n",
      "[['B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'PAD'], ['I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-MISC', 'I-LOC'], ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']]\n",
      "0.8966758510354634\n",
      "0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400 420 440 460 480 500 520 540 560 \n",
      "Epoch: 4, Loss: 43.14531632885337, F1-Score: 0.9452768051234657\n",
      "[['Paris', 'is', 'a', 'beautiful', 'city', 'in', 'France'], ['Jim', 'Courier', 'is', 'eating', 'apples', 'at', 'New', 'York'], ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']]\n",
      "[['B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'PAD'], ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC'], ['I-PER', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']]\n",
      "0.9085859034752466\n",
      "0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400 420 440 460 480 500 520 540 560 \n",
      "Epoch: 5, Loss: 31.400609680917114, F1-Score: 0.96350702263645\n",
      "[['Paris', 'is', 'a', 'beautiful', 'city', 'in', 'France'], ['Jim', 'Courier', 'is', 'eating', 'apples', 'at', 'New', 'York'], ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']]\n",
      "[['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'PAD'], ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC'], ['I-PER', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']]\n",
      "0.9171184783574793\n",
      "0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400 420 440 460 480 500 520 540 560 \n",
      "Epoch: 6, Loss: 23.824700598081108, F1-Score: 0.9733413170534866\n",
      "[['Paris', 'is', 'a', 'beautiful', 'city', 'in', 'France'], ['Jim', 'Courier', 'is', 'eating', 'apples', 'at', 'New', 'York'], ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']]\n",
      "[['B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'PAD'], ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC'], ['I-LOC', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']]\n",
      "0.9172295795929251\n",
      "0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400 420 440 460 480 500 520 540 560 \n",
      "Epoch: 7, Loss: 18.254761511227116, F1-Score: 0.979244497090715\n",
      "[['Paris', 'is', 'a', 'beautiful', 'city', 'in', 'France'], ['Jim', 'Courier', 'is', 'eating', 'apples', 'at', 'New', 'York'], ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']]\n",
      "[['I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'PAD'], ['B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC'], ['B-MISC', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']]\n",
      "0.9112745533730335\n",
      "0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400 420 440 460 480 500 520 540 560 \n",
      "Epoch: 8, Loss: 14.496884882595623, F1-Score: 0.9835205602488838\n",
      "[['Paris', 'is', 'a', 'beautiful', 'city', 'in', 'France'], ['Jim', 'Courier', 'is', 'eating', 'apples', 'at', 'New', 'York'], ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']]\n",
      "[['I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'PAD'], ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC'], ['B-MISC', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']]\n",
      "0.9086970047106924\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    (ht,ct) = model.init_state_of_lstm(batch_size)\n",
    "    epoch_loss = 0\n",
    "    Y_actual, Y_pred = [], []\n",
    "    for i in range(num_batches_tr):\n",
    "        if i%20 == 0: print(i, end=' ')\n",
    "        optimizer.zero_grad()\n",
    "        Xb, Yb = batch_generator_tr.get_batch(i,sentence_length=-1)\n",
    "        op, ht,ct = model(Xb,ht,ct)\n",
    "        Yb = Yb.reshape(-1)\n",
    "        loss = loss_function(op, Yb)\n",
    "        epoch_loss += loss.item()\n",
    "        ht = ht.detach()\n",
    "        ct = ct.detach()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        Y_pred += [int(el) for el in torch.argmax(op,axis=1)]\n",
    "        Y_actual += [int(el) for el in Yb]\n",
    "    Y_actual_without_pad, Y_pred_without_pad = [],[]\n",
    "    for el1, el2 in zip(Y_actual, Y_pred):\n",
    "        if el1 == pad_index: continue\n",
    "        else: \n",
    "            Y_actual_without_pad.append(el1)\n",
    "            Y_pred_without_pad.append(el2)\n",
    "    print(\"\\nEpoch: {}, Loss: {}, F1-Score: {}\".format(e+1,epoch_loss,f1_score(Y_actual_without_pad,Y_pred_without_pad,average=\"micro\")))\n",
    "    print(sentences_for_predictions)\n",
    "    print(predict(model,sentences_for_predictions, max_length=8))\n",
    "    print(predict_on_test(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Jim', 'Prakash', 'is', 'talking', 'at', 'Delhi']]\n",
      "[['B-PER', 'I-PER', 'O', 'O', 'O', 'B-LOC', 'PAD', 'PAD', 'PAD', 'PAD']]\n"
     ]
    }
   ],
   "source": [
    "sentences_for_predictions_1 = [\n",
    "    \"Jim Prakash is talking at Delhi\".split(\" \")\n",
    "]\n",
    "print(sentences_for_predictions_1)\n",
    "print(predict(model,sentences_for_predictions_1, max_length=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
