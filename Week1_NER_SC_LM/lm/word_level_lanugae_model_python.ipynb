{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextReader:\n",
    "    def __init__(self,filename,code_file=False,lower_case=False):\n",
    "        self.filename = filename\n",
    "        self.code_file = code_file\n",
    "        self.lower_case = lower_case\n",
    "        \n",
    "    def get_all_words(self,regex_params=None):\n",
    "        '''\n",
    "        regex_params = {\n",
    "            \"uppercase\": True,\n",
    "            \"digits\":False,\n",
    "            \"punctuation_list\":None\n",
    "        }\n",
    "        '''\n",
    "        if regex_params:\n",
    "            regex_string = \"a-z\"\n",
    "            if regex_params[\"uppercase\"]: regex_string += \"A-Z\"\n",
    "            if regex_params[\"digits\"]: regex_string += \"0-9\"\n",
    "            if regex_params[\"punctuation_list\"]: regex_string += \"\".join(regex_params[\"punctuation_list\"])\n",
    "            regex_string = \"[\" + regex_string +\"]\"\n",
    "            with open(self.filename,\"r\") as f: text = f.read()\n",
    "            words = re.findall(regex_string,text)\n",
    "            return words\n",
    "        else:\n",
    "            with open(self.filename,\"r\") as f: words = f.read().split(\" \")\n",
    "            return words + [\" \"]\n",
    "        \n",
    "    def get_unique_words(self,regex_params=None, distinguish_casing=False):\n",
    "        '''\n",
    "        regex_params = {\n",
    "            \"uppercase\": True,\n",
    "            \"digits\":False,\n",
    "            \"punctuation_list\":None\n",
    "        }\n",
    "        '''\n",
    "        all_words = self.get_all_words(regex_params)\n",
    "        if not distinguish_casing: return list(set([word.lower() for word in all_words]))\n",
    "        else: return list(set(all_words))\n",
    "    \n",
    "    \n",
    "    def get_X_and_Y(self,window_size=10):\n",
    "        if self.code_file:\n",
    "            X,Y = [],[]\n",
    "            with open(self.filename,\"r\",errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    words = line.strip().split() + [\"\\n\"]\n",
    "                    if len(words) <= 2: continue\n",
    "                    x_w, y_w = [],[]\n",
    "                    if len(words) > window_size:\n",
    "                        for i in range(0,len(words)-window_size):\n",
    "                            x_w.append(words[i:i+window_size])\n",
    "                            y_w.append(words[i+1:i+1+window_size])\n",
    "                    else:\n",
    "                        x_w.append(words[:-1])\n",
    "                        y_w.append(words[1:])\n",
    "                    X += x_w\n",
    "                    Y += y_w\n",
    "                return X,Y\n",
    "        else:\n",
    "            with open(self.filename,\"r\",errors=\"ignore\") as f:\n",
    "                if self.lower_case: text = f.read().lower().split()\n",
    "                else: text = f.read().split()\n",
    "                X,Y = [],[]\n",
    "                for i in range(len(text)-window_size):\n",
    "                    X.append(text[i:i+window_size])\n",
    "                    Y.append(text[i+1:window_size+i+1])\n",
    "                return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437 437 [['import', 'torch'], ['import', 'torch.nn', 'as', 'nn'], ['from', 'sklearn.metrics', 'import', 'f1_score,', 'classification_report'], ['class', 'DataLoader:']] [['torch', '\\n'], ['torch.nn', 'as', 'nn', '\\n'], ['sklearn.metrics', 'import', 'f1_score,', 'classification_report', '\\n'], ['DataLoader:', '\\n']] [['\"Jim', 'Prakash', 'is', 'talking', 'at'], ['Prakash', 'is', 'talking', 'at', 'Delhi\".split(\"'], ['is', 'talking', 'at', 'Delhi\".split(\"', '\")'], ['print(predict(model,sentences_for_predictions_1,', 'max_length=10))']] [['Prakash', 'is', 'talking', 'at', 'Delhi\".split(\"'], ['is', 'talking', 'at', 'Delhi\".split(\"', '\")'], ['talking', 'at', 'Delhi\".split(\"', '\")', '\\n'], ['max_length=10))', '\\n']]\n"
     ]
    }
   ],
   "source": [
    "text_reader = TextReader(\"data1.txt\",code_file=True,lower_case=False)\n",
    "X,Y = text_reader.get_X_and_Y(window_size=5)\n",
    "print(len(X),len(Y),X[:4],Y[:4],X[-4:],Y[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabBuilder:\n",
    "    def __init__(self, X,Y, unknown_token=\"<UNK>\",pad_token=\"<PAD>\"):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.unknown_token = unknown_token\n",
    "        self.pad_token = pad_token\n",
    "    \n",
    "    def get_word_vocab(self,for_X=False, for_Y=False, for_both=False):\n",
    "        word_to_index, index_to_word = {},{}\n",
    "        if for_X: all_words = list(set([word for el in self.X for word in el]))\n",
    "        if for_Y: all_words = list(set([word for el in self.Y for word in el]))\n",
    "        if for_both: all_words = list(set([word for el in self.Y for word in el] + [word for el in self.X for word in el]))\n",
    "        for i,word in enumerate(all_words):\n",
    "            word_to_index[word] = i\n",
    "            index_to_word[i] = word\n",
    "        len_vocab = len(word_to_index)\n",
    "        word_to_index[self.unknown_token] = len_vocab\n",
    "        index_to_word[len_vocab] = self.unknown_token\n",
    "        word_to_index[self.pad_token] = len_vocab+1\n",
    "        index_to_word[len_vocab+1] = self.pad_token\n",
    "        return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548 452 549\n",
      "548\n"
     ]
    }
   ],
   "source": [
    "vocab_builder = VocabBuilder(X,Y)\n",
    "X_w, X_i = vocab_builder.get_word_vocab(for_X=True)\n",
    "Y_w, Y_i = vocab_builder.get_word_vocab(for_Y=True)\n",
    "A_w, A_i = vocab_builder.get_word_vocab(for_both=True)\n",
    "print(len(X_w),len(Y_w),len(A_w))\n",
    "print(A_w['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateEncoding:\n",
    "    def __init__(self,data_x,vocab_x,data_y,vocab_y,unknown_token):\n",
    "        self.data_x = data_x\n",
    "        self.vocab_x = vocab_x\n",
    "        self.data_y = data_y\n",
    "        self.vocab_y = vocab_y\n",
    "        self.unknown_token = unknown_token\n",
    "        self.pure_vocab_x = self.remove_unknown_token_from_voab(vocab_x)\n",
    "        self.pure_vocab_y = self.remove_unknown_token_from_voab(vocab_y)\n",
    "    \n",
    "    def remove_unknown_token_from_voab(self,vocab):\n",
    "        return {k:v for k,v in vocab.items() if k != self.unknown_token}\n",
    "        \n",
    "    def get_encoding_X(self,raw_text=None):\n",
    "        if raw_text: data_to_encode = raw_text\n",
    "        else: data_to_encode = self.data_x\n",
    "        encoded_X = []\n",
    "        for word_list in data_to_encode:\n",
    "            word_encoding = []\n",
    "            for word in word_list: \n",
    "                if word not in self.pure_vocab_x: word_encoding.append(self.vocab_x[self.unknown_token])\n",
    "                else: word_encoding.append(self.vocab_x[word])\n",
    "            encoded_X.append(word_encoding)\n",
    "        return encoded_X\n",
    "    \n",
    "    def get_encoding_Y(self,raw_text=None):\n",
    "        if raw_text: data_to_encode = raw_text\n",
    "        else: data_to_encode = self.data_y\n",
    "        encoded_Y = []\n",
    "        for word_list in data_to_encode:\n",
    "            word_encoding = []\n",
    "            for word in word_list: \n",
    "                if word not in self.pure_vocab_y: word_encoding.append(self.vocab_y[self.unknown_token])\n",
    "                else: word_encoding.append(self.vocab_y[word])\n",
    "            encoded_Y.append(word_encoding)\n",
    "        return encoded_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437 437 [[357, 262], [357, 166, 217, 36], [322, 474, 357, 511, 276], [359, 29]] [[262, 397], [166, 217, 36, 397], [474, 357, 511, 276, 397], [29, 397]] [[285, 508, 139, 400, 52], [508, 139, 400, 52, 152], [139, 400, 52, 152, 501], [96, 148]] [[508, 139, 400, 52, 152], [139, 400, 52, 152, 501], [400, 52, 152, 501, 397], [148, 397]]\n"
     ]
    }
   ],
   "source": [
    "encoding_generator = GenerateEncoding(data_x=X,data_y=Y,vocab_x=A_w,vocab_y=A_w, unknown_token=\"<UNK>\")\n",
    "X_enc = encoding_generator.get_encoding_X()\n",
    "Y_enc = encoding_generator.get_encoding_Y()\n",
    "print(len(X_enc),len(Y_enc),X_enc[:4],Y_enc[:4],X_enc[-4:],Y_enc[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self,X,Y,batch_size):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def get_batch(self,batch_index,make_tensor=False):\n",
    "        Xb = self.X[batch_index*self.batch_size:(batch_index+1)*self.batch_size]\n",
    "        Yb = self.Y[batch_index*self.batch_size:(batch_index+1)*self.batch_size]\n",
    "        if make_tensor: return torch.tensor(Xb),torch.tensor(Yb)\n",
    "        return Xb,Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWordLevelRNNModel(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, lstm_neurons, num_lstm_layers, num_classes,\n",
    "                 make_birectional=False, debug_mode=False):\n",
    "        super().__init__()\n",
    "        self.debug_mode = debug_mode\n",
    "        self.bidirectional = make_birectional\n",
    "        self.lstm_neurons = lstm_neurons\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_neurons, \n",
    "                            num_layers=num_lstm_layers, bidirectional=make_birectional, batch_first=True)\n",
    "        \n",
    "        in_features = lstm_neurons\n",
    "        if self.bidirectional: in_features = 2*lstm_neurons\n",
    "        self.linear1 = nn.Linear(in_features=in_features, out_features=100)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.linear2 = nn.Linear(in_features=100, out_features=num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self,x,ht,ct):\n",
    "        if self.debug_mode: print(\"Before embedding layer:\",x.shape)\n",
    "        x = self.embedding(x)\n",
    "        if self.debug_mode: print(\"After embedding layer:\",x.shape)\n",
    "        x, (ht, ct) = self.lstm(x,(ht,ct))\n",
    "        if self.debug_mode: print(\"After lstm layer:\",x.shape,ht.shape,ct.shape)\n",
    "        x = x.reshape(-1, x.shape[2])\n",
    "        if self.debug_mode: print(\"After reshaping:\",x.shape)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        if self.debug_mode: print(\"After 1st linear layer:\",x.shape)\n",
    "        x = self.linear2(x)\n",
    "        x = self.log_softmax(x)\n",
    "        if self.debug_mode: print(\"After 2nd linear layer:\",x.shape)\n",
    "        return x, ht,ct\n",
    "    \n",
    "    def init_state_of_lstm(self,batch_size):\n",
    "        if self.bidirectional: first_param = 2*self.num_lstm_layers\n",
    "        else: first_param = self.num_lstm_layers\n",
    "        return (\n",
    "            torch.randn(first_param, batch_size, self.lstm_neurons),\n",
    "            torch.randn(first_param, batch_size, self.lstm_neurons),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_reader = TextReader(\"data1.txt\")\n",
    "text_reader = TextReader(\"data2.txt\",code_file=False,lower_case=True)\n",
    "window_size = 10\n",
    "X,Y = text_reader.get_X_and_Y(window_size=window_size)\n",
    "unknown_token = \"<UNK>\"\n",
    "pad_token = \"<PAD>\"\n",
    "vocab_builder = VocabBuilder(X,Y,unknown_token=unknown_token,pad_token=pad_token)\n",
    "word_to_index, index_to_word = vocab_builder.get_word_vocab(for_both=True)\n",
    "print(len(word_to_index),len(index_to_word))\n",
    "\n",
    "encoding_generator = GenerateEncoding(\n",
    "    data_x=X,data_y=Y,vocab_x=word_to_index,vocab_y=word_to_index, unknown_token=unknown_token\n",
    ")\n",
    "X_enc = encoding_generator.get_encoding_X()\n",
    "Y_enc = encoding_generator.get_encoding_Y()\n",
    "print(len(X_enc),len(Y_enc),X_enc[:4],Y_enc[:4],X_enc[-4:],Y_enc[-4:])\n",
    "\n",
    "batch_size = 5\n",
    "batch_generator = BatchGenerator(X_enc,Y_enc,batch_size)\n",
    "Xb,Yb = batch_generator.get_batch(batch_index=1)\n",
    "print(len(Xb),len(Xb[0]),len(Yb),len(Yb[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 8\n",
    "batch_generator = BatchGenerator(X_enc,Y_enc,batch_size)\n",
    "num_batches = len(X_enc)//batch_size\n",
    "embedding_dim = 50\n",
    "vocab_size = len(index_to_word)\n",
    "num_classes = len(index_to_word)\n",
    "num_lstm_layers = 4\n",
    "lstm_neurons = 100\n",
    "make_bidirectional = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = MyWordLevelRNNModel(vocab_size=vocab_size, embedding_dim=embedding_dim, lstm_neurons=lstm_neurons, \n",
    "                   num_lstm_layers=num_lstm_layers, num_classes = num_classes,\n",
    "                   make_birectional=make_bidirectional, debug_mode=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.1)\n",
    "loss_function = nn.NLLLoss()\n",
    "(ht,ct) = model.init_state_of_lstm(batch_size)\n",
    "Y_actual, Y_pred = [], []\n",
    "\n",
    "optimizer.zero_grad()\n",
    "Xb, Yb = batch_generator.get_batch(2,make_tensor=True)\n",
    "\n",
    "op, ht,ct = model(Xb,ht,ct)\n",
    "print(op.shape)\n",
    "# print(op[0])\n",
    "Yb = Yb.reshape(-1)\n",
    "print(op.shape, Yb.shape)\n",
    "loss = loss_function(op, Yb)\n",
    "print(loss)\n",
    "ht = ht.detach()\n",
    "ct = ct.detach()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "Y_pred += [int(el) for el in torch.argmax(op,axis=1)]\n",
    "Y_actual += [int(el) for el in Yb]\n",
    "\n",
    "optimizer.zero_grad()\n",
    "Xb, Yb = batch_generator.get_batch(3,make_tensor=True)\n",
    "op, ht,ct = model(Xb,ht,ct)\n",
    "print(op.shape)\n",
    "# print(op[0])\n",
    "Yb = Yb.reshape(-1)\n",
    "print(op.shape, Yb.shape)\n",
    "loss = loss_function(op, Yb)\n",
    "print(loss)\n",
    "ht = ht.detach()\n",
    "ct = ct.detach()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "Y_pred += [int(el) for el in torch.argmax(op,axis=1)]\n",
    "Y_actual += [int(el) for el in Yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyWordLevelRNNModel(vocab_size=vocab_size, embedding_dim=embedding_dim, lstm_neurons=lstm_neurons, \n",
    "                   num_lstm_layers=num_lstm_layers, num_classes = num_classes,\n",
    "                   make_birectional=make_bidirectional, debug_mode=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    (ht,ct) = model.init_state_of_lstm(batch_size)\n",
    "    epoch_loss = 0\n",
    "    Y_actual, Y_pred = [], []\n",
    "    for i in range(num_batches):\n",
    "        if i%20 == 0: print(i, end=' ')\n",
    "        optimizer.zero_grad()\n",
    "        Xb, Yb = batch_generator.get_batch(i,make_tensor=True)\n",
    "        op, ht,ct = model(Xb,ht,ct)\n",
    "        Yb = Yb.reshape(-1)\n",
    "        loss = loss_function(op, Yb)\n",
    "        epoch_loss += loss.item()\n",
    "        ht = ht.detach()\n",
    "        ct = ct.detach()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"\\nEpoch: {}, Loss: {}\".format(e+1,epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = [\"we believe in the power of books\".split(\" \")]\n",
    "test_string_enc = encoding_generator.get_encoding_X(raw_text=test_string)\n",
    "pred_op = deepcopy(test_string_enc)\n",
    "print(pred_op)\n",
    "\n",
    "model.eval()\n",
    "if make_bidirectional: first_param = 2*num_lstm_layers\n",
    "else: first_param = num_lstm_layers\n",
    "ht_pred = torch.randn(first_param, 1, lstm_neurons)\n",
    "ct_pred = torch.randn(first_param, 1, lstm_neurons)\n",
    "\n",
    "unigram = True # unigram will work, becuase it is a statefulRNN (ht and ct is getting updated for every character)\n",
    "window_size = 5\n",
    "num_chars = len(test_string[0])+500\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_chars):\n",
    "        input_vec = torch.tensor([pred_op[0][i:i+1]])\n",
    "        op,ht_pred,ct_pred = model(input_vec,ht_pred,ct_pred)\n",
    "        op = torch.argmax(op,axis=1).tolist()\n",
    "        if i >= len(test_string_enc[0])-1: pred_op[0].append(op[0])\n",
    "    pred_word = \" \".join([index_to_word[el] for el in pred_op[0]])\n",
    "    print(pred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = [\"reading about ai from a book should be good way to learn\".split(\" \")]\n",
    "test_string_enc = encoding_generator.get_encoding_X(raw_text=test_string)\n",
    "pred_op = deepcopy(test_string_enc)\n",
    "print(pred_op)\n",
    "\n",
    "model.eval()\n",
    "if make_bidirectional: first_param = 2*num_lstm_layers\n",
    "else: first_param = num_lstm_layers\n",
    "ht_pred = torch.randn(first_param, 1, lstm_neurons)\n",
    "ct_pred = torch.randn(first_param, 1, lstm_neurons)\n",
    "\n",
    "unigram = True # unigram will work, becuase it is a statefulRNN (ht and ct is getting updated for every character)\n",
    "window_size = 5\n",
    "num_chars = len(test_string[0])+500\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_chars):\n",
    "        input_vec = torch.tensor([pred_op[0][i:i+1]])\n",
    "        op,ht_pred,ct_pred = model(input_vec,ht_pred,ct_pred)\n",
    "        op = torch.argmax(op,axis=1).tolist()\n",
    "        if i >= len(test_string_enc[0])-1: pred_op[0].append(op[0])\n",
    "    pred_word = \" \".join([index_to_word[el] for el in pred_op[0]])\n",
    "    print(pred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitf0b0a3d2859f4904a6dd3c0263fd37ec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
